{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DogFaceNet version 7: Dev version\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import skimage as sk\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm_notebook\n",
    "import tensorflow.keras.backend as K\n",
    "from triplets_processing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = '../data/dogfacenet/aligned/after_2/'\n",
    "PATH_SAVE = '../output/history/'\n",
    "PATH_MODEL = '../output/model/'\n",
    "SIZE = (104,104,3)\n",
    "VALID_SPLIT = 0.1\n",
    "TEST_SPLIT = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data pre-processing\n",
    "- Load image and labels\n",
    "- Training set, validation set (close-set) and testing (open-set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = np.empty(0)\n",
    "labels = np.empty(0)\n",
    "idx = 0\n",
    "for root,dirs,files in os.walk(PATH):\n",
    "    if len(files)>1:\n",
    "        for i in range(len(files)):\n",
    "            files[i] = root + '/' + files[i]\n",
    "        filenames = np.append(filenames,files)\n",
    "        labels = np.append(labels,np.ones(len(files))*idx)\n",
    "        idx += 1\n",
    "print(len(labels))\n",
    "h,w,c = SIZE\n",
    "images = np.empty((len(filenames),h,w,c))\n",
    "for i,f in enumerate(filenames):\n",
    "    images[i] = sk.io.imread(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization\n",
    "images /= 255.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbof_classes = len(np.unique(labels))\n",
    "print(nbof_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rotation_range=8,\n",
    "    zoom_range=0.1,\n",
    "    fill_mode='nearest',\n",
    "    channel_shift_range = 0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_transform(images, datagen):\n",
    "    \"\"\"\n",
    "    Apply a data preprocessing transformation to images\n",
    "    Args:\n",
    "        -images\n",
    "        -ImageDataGenerator\n",
    "    Return:\n",
    "        -a serie of images of the same shape of the input but transformed\n",
    "    \"\"\"\n",
    "    for x in datagen.flow(images, batch_size=len(images)):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_apply_transform(image, datagen):\n",
    "    \"\"\"\n",
    "    Apply a data preprocessing transformation to a single image\n",
    "    Args:\n",
    "        -image\n",
    "        -ImageDataGenerator\n",
    "    Return:\n",
    "        -an image of the same shape of the input but transformed\n",
    "    \"\"\"\n",
    "    image_exp = np.expand_dims(image,0)\n",
    "    for x in datagen.flow(image_exp, batch_size=1):\n",
    "        return x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test of apply_transform\n",
    "out = apply_transform(images[:5], datagen)\n",
    "for i in range(len(out)):\n",
    "    plt.subplot(1,len(out),i+1)\n",
    "    plt.imshow(out[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test of single_apply_transform\n",
    "out = single_apply_transform(images[2], datagen)\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(out)\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(images[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbof_repetition = 10\n",
    "\n",
    "classes = np.unique(labels)\n",
    "images_aug = np.empty((len(images)*nbof_repetition,h,w,c))\n",
    "labels_aug = np.empty(len(labels)*nbof_repetition)\n",
    "\n",
    "count = 0\n",
    "\n",
    "for i in tqdm_notebook(range(nbof_classes)):\n",
    "    length = len(labels[np.equal(labels,classes[i])])\n",
    "    \n",
    "    for k in range(nbof_repetition):\n",
    "        for x,y in datagen.flow(images[np.equal(labels,classes[i])],labels[np.equal(labels,classes[i])],batch_size=length,shuffle=False):           \n",
    "            images_aug[count:count+length] = x\n",
    "            labels_aug[count:count+length] = y\n",
    "            break\n",
    "        count += length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "for i in range(25):\n",
    "    plt.subplot(5,5,i+1)\n",
    "    plt.imshow(images_aug[i]/255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open-set: test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbof_test = int(TEST_SPLIT*nbof_classes)\n",
    "\n",
    "keep_test = np.less(labels,nbof_test)\n",
    "keep_train = np.logical_not(keep_test)\n",
    "\n",
    "images_test = images[keep_test]\n",
    "labels_test = labels[keep_test]\n",
    "\n",
    "images_train = images[keep_train]\n",
    "labels_train = labels[keep_train]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Triplet definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def define_triplets(images,labels,nbof_triplet = 10000 * 3, datagen=datagen):\n",
    "    triplet_train = np.empty((nbof_triplet,h,w,c))\n",
    "    y_triplet = np.empty(nbof_triplet)\n",
    "    classes = np.unique(labels)\n",
    "    for i in tqdm_notebook(range(0,nbof_triplet,3)):\n",
    "        # Pick a class and chose two pictures from this class\n",
    "        classAP = classes[np.random.randint(len(classes))]\n",
    "        keep = np.equal(labels,classAP)\n",
    "        keep_classAP = images[keep]\n",
    "        keep_classAP_idx = labels[keep]\n",
    "        idx_image1 = np.random.randint(len(keep_classAP))\n",
    "        idx_image2 = np.random.randint(len(keep_classAP))\n",
    "        while idx_image1 == idx_image2:\n",
    "            idx_image2 = np.random.randint(len(keep_classAP))\n",
    "\n",
    "        triplet_train[i] = single_apply_transform(keep_classAP[idx_image1],datagen)\n",
    "        triplet_train[i+1] = single_apply_transform(keep_classAP[idx_image2],datagen)\n",
    "        y_triplet[i] = keep_classAP_idx[idx_image1]\n",
    "        y_triplet[i+1] = keep_classAP_idx[idx_image2]\n",
    "        # Pick a class for the negative picture\n",
    "        classN = classes[np.random.randint(len(classes))]\n",
    "        while classN==classAP:\n",
    "            classN = classes[np.random.randint(len(classes))]\n",
    "        keep = np.equal(labels,classN)\n",
    "        keep_classN = images[keep]\n",
    "        keep_classN_idx = labels[keep]\n",
    "        idx_image3 = np.random.randint(len(keep_classN))\n",
    "        triplet_train[i+2] = single_apply_transform(keep_classN[idx_image3],datagen)\n",
    "        y_triplet[i+2] = keep_classN_idx[idx_image3]\n",
    "        \n",
    "    return triplet_train, y_triplet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#triplet_train, y_triplet_train = define_triplets(images_train,labels_train)\n",
    "triplet_test, y_triplet_test = define_triplets(images_test,labels_test,1000*3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "# Check the triplet\n",
    "start = 10*3\n",
    "print(y_triplet_train[start:start+24])\n",
    "plt.figure(figsize=(6,18))\n",
    "for i in range(0,24):\n",
    "    plt.subplot(8,3,i+1)\n",
    "    plt.imshow(triplet_train[i+start])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "# Check the triplet\n",
    "print(y_triplet_test[:24])\n",
    "plt.figure(figsize=(6,18))\n",
    "for i in range(0,24):\n",
    "    plt.subplot(8,3,i+1)\n",
    "    plt.imshow(triplet_test[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 1.\n",
    "def cos_triplet(y_true,y_pred):\n",
    "    \n",
    "    a = y_pred[0::3]\n",
    "    p = y_pred[1::3]\n",
    "    n = y_pred[2::3]\n",
    "    \n",
    "    ap = K.sum(a*p,-1) * 0.5 + 0.5\n",
    "    an = K.sum(a*n,-1) * 0.5 + 0.5\n",
    "\n",
    "    return tf.nn.relu(ap - an + alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.3\n",
    "def triplet(y_true,y_pred):\n",
    "    \n",
    "    a = y_pred[0::3]\n",
    "    p = y_pred[1::3]\n",
    "    n = y_pred[2::3]\n",
    "    \n",
    "    ap = K.sum(K.square(a-p),-1)\n",
    "    an = K.sum(K.square(a-n),-1)\n",
    "\n",
    "    return K.sum(tf.nn.relu(ap - an + alpha))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following version was found in a blog post\n",
    "from triplet_loss import batch_all_triplet_loss, batch_hard_triplet_loss\n",
    "def triplet(y_true,y_pred):\n",
    "    y_true = y_true[:,0]\n",
    "    y_true = tf.reshape(y_true,[-1])\n",
    "#     loss,_ = batch_all_triplet_loss(y_true,y_pred,1.)\n",
    "    loss = batch_hard_triplet_loss(y_true,y_pred,1.)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metric definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_triplet_acc(y_true,y_pred):\n",
    "    a = y_pred[0::3]\n",
    "    p = y_pred[1::3]\n",
    "    n = y_pred[2::3]\n",
    "    \n",
    "    ap = K.sum(a*p,-1) * 0.5 + 0.5\n",
    "    an = K.sum(a*n,-1) * 0.5 + 0.5\n",
    "    \n",
    "    return K.less(ap,an)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def triplet_acc(y_true,y_pred):\n",
    "    a = y_pred[0::3]\n",
    "    p = y_pred[1::3]\n",
    "    n = y_pred[2::3]\n",
    "    \n",
    "    ap = K.sum(K.square(a-p),-1)\n",
    "    an = K.sum(K.square(a-n),-1)\n",
    "    \n",
    "    return K.less(ap+alpha,an)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# dogfacenet_v6\n",
    "emb_size = 18\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.layers import Activation, Dropout, Flatten, Dense, Lambda\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(104, 104, 3)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(256, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(emb_size))\n",
    "model.add(Lambda(lambda x: tf.nn.l2_normalize(x,axis=-1)))\n",
    "\n",
    "model.compile(loss=triplet,\n",
    "              optimizer='rmsprop',\n",
    "              metrics=[triplet_acc])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# dogfacenet_v9\n",
    "emb_size = 20\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.layers import Activation, Dropout, Flatten, Dense, Lambda\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=(104, 104, 3)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
    "model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(emb_size))\n",
    "model.add(Lambda(lambda x: tf.nn.l2_normalize(x,axis=-1)))\n",
    "\n",
    "model.compile(loss=triplet,\n",
    "              optimizer='rmsprop',\n",
    "              metrics=[triplet_acc])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# dogfacenet_v11\n",
    "emb_size = 16\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.layers import Activation, Dropout, Flatten, Dense, Lambda, BatchNormalization\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=(104, 104, 3)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
    "model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))\n",
    "model.add(Conv2D(1024, (3, 3), activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(emb_size))\n",
    "model.add(Lambda(lambda x: tf.nn.l2_normalize(x,axis=-1)))\n",
    "\n",
    "model.compile(loss=triplet,\n",
    "              optimizer='adam',\n",
    "              metrics=[triplet_acc])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Transfer learning\n",
    "base = tf.keras.Model(model.layers[0].input, model.layers[20].output)\n",
    "x = Dense(512, activation='relu')(base.output)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(16)(x)\n",
    "x = Lambda(lambda x: tf.nn.l2_normalize(x,axis=-1))(x)\n",
    "\n",
    "model2 = tf.keras.Model(base.layers[0].input, x)\n",
    "\n",
    "\n",
    "for layer in model2.layers[:21]:\n",
    "    layer.trainable=True\n",
    "\n",
    "model2.compile(loss=triplet,\n",
    "              optimizer='adam',\n",
    "              metrics=[triplet_acc])\n",
    "model2.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(PATH_MODEL + '2019.02.14.dogfacenet_v11.hard_triplet_trained.data_aug.20.h5', custom_objects={'triplet':triplet,'triplet_acc':triplet_acc})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(PATH_MODEL + '2019.02.27.dogfacenet_v11.hard_triplet_trained.data_aug_2.16.h5', custom_objects={'triplet':triplet,'triplet_acc':triplet_acc})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(triplet_test,y_triplet_test, batch_size=63)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import FileLink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FileLink(PATH_MODEL + '2019.02.27.dogfacenet_v11.hard_triplet_trained.data_aug_2.16.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FileLink(PATH_SAVE + '2018.02.27.dogfacenet_v11.hard_triplet.data_aug.10.a_0.3.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(PATH_MODEL + '2019.02.12.hard_triplet_trained.data_aug.0.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "triplet_train, y_triplet_train = define_triplets(images_train,labels_train)\n",
    "triplet_test, y_triplet_test = define_triplets(images_test,labels_test,1000*3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histories = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    history = model.fit(\n",
    "        triplet_train,\n",
    "        y_triplet_train,\n",
    "        batch_size = 21*3,\n",
    "        epochs = 1,\n",
    "        validation_data=(triplet_test,y_triplet_test),\n",
    "        shuffle=False\n",
    "    )\n",
    "    histories += [history]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(PATH_MODEL + '2019.02.27.dogfacenet_v11.hard_triplet_trained.data_aug.'+str(16)+'.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histories2 = [histories[i] for i in range(29)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(PATH_MODEL + '2019.02.27.dogfacenet_v11.hard_triplet_trained.data_aug.6.h5', custom_objects={'triplet':triplet,'triplet_acc':triplet_acc})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(histories2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new_images_train, new_labels_train = shuffle_classes(images_train,labels_train)\n",
    "predict_train=model.predict(new_images_train)\n",
    "triplet_train, y_triplet_train = define_hard_triplets(new_images_train,new_labels_train,predict_train,20,900)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2):\n",
    "    history = model.fit(\n",
    "        triplet_train,\n",
    "        y_triplet_train,\n",
    "        batch_size = 21*3,\n",
    "        epochs = 1,\n",
    "        validation_data=(triplet_test,y_triplet_test),\n",
    "        shuffle=False\n",
    "    )\n",
    "    histories2 += [history]\n",
    "model.save(PATH_MODEL + '2019.02.27.dogfacenet_v11.hard_triplet_trained.data_aug.'+str(16)+'.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histories = histories[:-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for k in range(1):\n",
    "\n",
    "    new_images_train, new_labels_train = shuffle_classes(images_train,labels_train)\n",
    "    predict_train=model.predict(new_images_train)\n",
    "    triplet_train, y_triplet_train = define_hard_triplets(new_images_train,new_labels_train,predict_train, 10, 0)\n",
    "\n",
    "    for i in range(1):\n",
    "        history = model.fit(\n",
    "            triplet_train,\n",
    "            y_triplet_train,\n",
    "            batch_size = 21*3,\n",
    "            epochs = 1,\n",
    "            validation_data=(triplet_test,y_triplet_test),\n",
    "            shuffle=False\n",
    "        )\n",
    "        histories += [history]\n",
    "\n",
    "model.save(PATH_MODEL + '2019.02.27.dogfacenet_v11.hard_triplet_trained.data_aug_2.'+str(17)+'.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "loss = np.empty(0)\n",
    "val_loss = np.empty(0)\n",
    "acc = np.empty(0)\n",
    "val_acc = np.empty(0)\n",
    "\n",
    "for history in histories:\n",
    "    loss = np.append(loss,history.history['loss'])\n",
    "    val_loss = np.append(val_loss,history.history['val_loss'])\n",
    "    acc = np.append(acc,history.history['triplet_acc'])\n",
    "    val_acc = np.append(val_acc,history.history['val_triplet_acc'])\n",
    "\n",
    "    \n",
    "history_ = np.array([loss,val_loss,acc,val_acc])\n",
    "np.save(PATH_SAVE+'2018.02.27.dogfacenet_v11.hard_triplet.data_aug.10.a_0.3.npy',history_)\n",
    "np.savetxt(PATH_SAVE+'2018.02.27.dogfacenet_v11.hard_triplet.data_aug.10.a_0.3.txt',history_)\n",
    "\n",
    "epochs = np.arange(len(loss))\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(epochs,loss, '-o', label=\"loss\")\n",
    "plt.plot(epochs,val_loss, '-o', label=\"val_loss\")\n",
    "plt.xlabel(\"Number of epochs\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(epochs,acc, '-o', label=\"acc\")\n",
    "plt.plot(epochs,val_acc, '-o', label=\"val_acc\")\n",
    "plt.xlabel(\"Number of epochs\")\n",
    "plt.yticks(np.arange(0,1.1,0.1))\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    new_images_train, new_labels_train = shuffle_classes(images_train,labels_train)\n",
    "    predict_train=model.predict(new_images_train)\n",
    "    triplet_train, y_triplet_train = define_hard_triplets(new_images_train,new_labels_train,predict_train)\n",
    "\n",
    "    model.fit(\n",
    "        triplet_train,\n",
    "        y_triplet_train,\n",
    "        batch_size = 21*3,\n",
    "        epochs = 1,\n",
    "        validation_data=(triplet_test,y_triplet_test),\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    model.save(PATH_MODEL + '2019.02.14.dogfacenet_v11.hard_triplet_trained.data_aug.'+str(i+16)+'.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new_images_train, new_labels_train = shuffle_classes(images_train,labels_train)\n",
    "predict_train=model.predict(new_images_train)\n",
    "triplet_train, y_triplet_train = define_hard_triplets(new_images_train,new_labels_train,predict_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history=model.fit(\n",
    "    triplet_train,\n",
    "    y_triplet_train,\n",
    "    batch_size = 21*3,\n",
    "    epochs = 1,\n",
    "    validation_data=(triplet_test,y_triplet_test),\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(PATH_MODEL + '2019.02.14.dogfacenet_v11.hard_triplet_trained.data_aug.6.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = np.empty(0)\n",
    "val_loss = np.empty(0)\n",
    "acc = np.empty(0)\n",
    "val_acc = np.empty(0)\n",
    "\n",
    "loss = np.append(loss,history.history['loss'])\n",
    "val_loss = np.append(val_loss,history.history['val_loss'])\n",
    "acc = np.append(acc,history.history['triplet_acc'])\n",
    "val_acc = np.append(val_acc,history.history['val_triplet_acc'])\n",
    "    \n",
    "    \n",
    "history_ = np.array([loss,val_loss,acc,val_acc])\n",
    "np.save(PATH_SAVE+'2018.02.12.dogfacenet_v6.hard_triplet.data_aug.0.a_0.3.npy',history_)\n",
    "np.savetxt(PATH_SAVE+'2018.02.12.dogfacenet_v6.hard_triplet.data_aug.0.a_0.3.txt',history_)\n",
    "\n",
    "epochs = np.arange(len(loss))\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(epochs,loss, '-o', label=\"loss\")\n",
    "plt.plot(epochs,val_loss, '-o', label=\"val_loss\")\n",
    "plt.xlabel(\"Number of epochs\")\n",
    "plt.legend()\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(epochs,acc, '-o', label=\"acc\")\n",
    "plt.plot(epochs,val_acc, '-o', label=\"val_acc\")\n",
    "plt.xlabel(\"Number of epochs\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "histories = []\n",
    "nbof_cycles = 9\n",
    "for i in range(0,nbof_cycles,1):\n",
    "\n",
    "    history=model.fit(\n",
    "        triplet_train,\n",
    "        y_triplet_train,\n",
    "        batch_size = 21*3,\n",
    "        epochs = nbof_cycles-i,\n",
    "        validation_data=(triplet_test,y_triplet_test),\n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    histories += [history]\n",
    "    model.save(PATH_MODEL + '2019.02.14.dogfacenet_v10.hard_triplet_trained.data_aug.' + str(i+1) + '.h5')\n",
    "    \n",
    "    new_images_train, new_labels_train = shuffle_classes(images_train,labels_train)\n",
    "    predict_train=model.predict(new_images_train)\n",
    "    triplet_train, y_triplet_train = define_hard_triplets(new_images_train,new_labels_train,predict_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = np.empty(0)\n",
    "val_loss = np.empty(0)\n",
    "acc = np.empty(0)\n",
    "val_acc = np.empty(0)\n",
    "\n",
    "for i in range(len(histories)):\n",
    "    history = histories[i]\n",
    "    \n",
    "    loss = np.append(loss,history.history['loss'])\n",
    "    val_loss = np.append(val_loss,history.history['val_loss'])\n",
    "    acc = np.append(acc,history.history['triplet_acc'])\n",
    "    val_acc = np.append(val_acc,history.history['val_triplet_acc'])\n",
    "    \n",
    "    \n",
    "history_ = np.array([loss,val_loss,acc,val_acc])\n",
    "np.save(PATH_SAVE+'2018.02.13.dogfacenet_v9.hard_triplet.2.a_0.3.npy',history_)\n",
    "np.savetxt(PATH_SAVE+'2018.02.13.dogfacenet_v9.hard_triplet.2.a_0.3.txt',history_)\n",
    "\n",
    "epochs = np.arange(len(loss))\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(epochs,loss, '-o', label=\"loss\")\n",
    "plt.plot(epochs,val_loss, '-o', label=\"val_loss\")\n",
    "plt.xlabel(\"Number of epochs\")\n",
    "plt.legend()\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(epochs,acc, '-o', label=\"acc\")\n",
    "plt.plot(epochs,val_acc, '-o', label=\"val_acc\")\n",
    "plt.xlabel(\"Number of epochs\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on new triplet integrity\n",
    "print(new_y_triplet_train[-30:])\n",
    "t = np.equal(new_y_triplet_train[0::3],new_y_triplet_train[1::3])\n",
    "print(t)\n",
    "np.sum(t.astype(np.float32))/len(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_classes(images,labels):\n",
    "    \"\"\"\n",
    "    Shuffles the classes\n",
    "    \"\"\"\n",
    "    classes = np.unique(labels)\n",
    "    np.random.shuffle(classes)\n",
    "    \n",
    "    shuffled_images = np.empty(images.shape)\n",
    "    shuffled_labels = np.empty(labels.shape)\n",
    "    idx = 0\n",
    "    for i in range(len(classes)):\n",
    "        keep_classes = np.equal(labels,classes[i])\n",
    "        length = np.sum(keep_classes.astype(int))\n",
    "        shuffled_labels[idx:idx+length] = labels[keep_classes]\n",
    "        shuffled_images[idx:idx+length] = images[keep_classes]\n",
    "        idx += length\n",
    "    return shuffled_images, shuffled_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def global_define_hard_triplets(images,labels,predict,datagen=datagen):\n",
    "    \"\"\"\n",
    "    Generates hard triplet for offline selection. It will consider the whole dataset.\n",
    "    \n",
    "    Args:\n",
    "        -images: images from which the triplets will be created\n",
    "        -labels: labels of the images\n",
    "        -predict: predicted embeddings for the images by the trained model\n",
    "        -alpha: threshold of the triplet loss\n",
    "    Returns:\n",
    "        -triplet\n",
    "        -y_triplet: labels of the triplets\n",
    "    \"\"\"\n",
    "    _,idx_classes = np.unique(labels,return_index=True)\n",
    "    classes = labels[np.sort(idx_classes)]\n",
    "    nbof_classes = len(classes)\n",
    "    triplets = np.empty((3*len(predict),h,w,c))\n",
    "    y_triplets = np.empty(3*len(predict))\n",
    "    \n",
    "    idx_triplets = 0\n",
    "    idx_images = 0\n",
    "    \n",
    "    for i in range(nbof_classes):\n",
    "        keep_class = np.equal(labels,classes[i])\n",
    "        mask_class = np.identity(len(predict)) * keep_class.astype(np.float32)\n",
    "        \n",
    "        #predict_class = mask_class.dot(predict)\n",
    "        predict_other = np.copy(predict)\n",
    "        for j in range(len(predict)):\n",
    "            if keep_class[j]:\n",
    "                predict_other[j] += np.inf\n",
    "        \n",
    "        keep_predict_class = predict[keep_class]\n",
    "        \n",
    "        for j in range(len(keep_predict_class)):\n",
    "            # Computes the distance between the current vector and the vectors in the class\n",
    "            dist_class = np.sum(np.square(keep_predict_class-keep_predict_class[j]),axis=-1)\n",
    "            \n",
    "            # Add the anchor\n",
    "            triplets[idx_triplets] = single_apply_transform(images[idx_images+j],datagen)\n",
    "            y_triplets[idx_triplets] = labels[idx_images+j]\n",
    "            \n",
    "            # Add the hard positive\n",
    "            triplets[idx_triplets+1] = single_apply_transform(images[idx_images+np.argmax(dist_class)],datagen)\n",
    "            y_triplets[idx_triplets+1] = labels[idx_images+np.argmax(dist_class)]\n",
    "            \n",
    "            # Computes the distance between the current vector and the vectors of the others classes\n",
    "            dist_other = np.sum(np.square(predict_other-keep_predict_class[j]),axis=-1)\n",
    "            \n",
    "            # Add the hard negative\n",
    "            triplets[idx_triplets+2] = single_apply_transform(images[np.argmin(dist_other)],datagen)\n",
    "            y_triplets[idx_triplets+2] = labels[np.argmin(dist_other)]\n",
    "            \n",
    "            idx_triplets += 3\n",
    "        \n",
    "        idx_images += len(keep_predict_class)\n",
    "        \n",
    "    return triplets, y_triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Offline triplet selection:\n",
    "# Generate hard triplets\n",
    "\n",
    "def define_hard_triplets(images,labels,predict,class_subset_size=10, add=100*3):\n",
    "    \"\"\"\n",
    "    Generates hard triplet for offline selection\n",
    "    \n",
    "    Args:\n",
    "        -images: images from which the triplets will be created\n",
    "        -labels: labels of the images\n",
    "        -predict: predicted embeddings for the images by the trained model\n",
    "        -alpha: threshold of the triplet loss\n",
    "        -class_subset_class: number of classes in a subset\n",
    "        -\n",
    "    Returns:\n",
    "        -triplet\n",
    "        -y_triplet: labels of the triplets\n",
    "    \"\"\"\n",
    "    _,idx_classes = np.unique(labels,return_index=True)\n",
    "    classes = labels[np.sort(idx_classes)]\n",
    "    nbof_classes = len(classes)\n",
    "    triplets = np.empty((3*len(predict)+add*(nbof_classes//class_subset_size + 1),h,w,c))\n",
    "    y_triplets = np.empty(3*len(predict)+add*(nbof_classes//class_subset_size + 1))\n",
    "    idx = 0\n",
    "    for i in tqdm_notebook(range(0,len(classes),class_subset_size)):\n",
    "        selected_classes = classes[i:i+class_subset_size]\n",
    "        keep_classes = np.array([labels[j] in selected_classes for j in range(len(labels))])\n",
    "        \n",
    "        selected_predict = predict[keep_classes]\n",
    "        length = len(selected_predict)*3\n",
    "        \n",
    "        triplets_tmp,y_triplets_tmp = global_define_hard_triplets(\n",
    "                                                                images[keep_classes],\n",
    "                                                                labels[keep_classes],\n",
    "                                                                selected_predict\n",
    "                                                            )\n",
    "        print(len(triplets_tmp))\n",
    "        \n",
    "        triplets[idx:idx+length] = triplets_tmp\n",
    "        y_triplets[idx:idx+length] = y_triplets_tmp\n",
    "        \n",
    "        triplets[idx+length:idx+length+add], y_triplets[idx+length:idx+length+add] = define_triplets(images, labels, add)\n",
    "        \n",
    "        idx += len(triplets_tmp) + add\n",
    "    return triplets, y_triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lez = 30\n",
    "n_images_train, n_labels_train = shuffle_classes(images_train[:lez],labels_train[:lez])\n",
    "\n",
    "# Test\n",
    "predict = model.predict(n_images_train)\n",
    "# Takes a subset of classes:\n",
    "_,idx_classes = np.unique(n_labels_train,return_index=True)\n",
    "classes = n_labels_train[idx_classes]\n",
    "\n",
    "# step1\n",
    "selected_classes = classes[0:10]\n",
    "keep_classes = np.array([n_labels_train[i] in selected_classes for i in range(len(n_labels_train))])\n",
    "\n",
    "selected_predict = predict[keep_classes]\n",
    "length = len(selected_predict)\n",
    "\n",
    "im,lab1 = global_define_hard_triplets(n_images_train[keep_classes],n_labels_train[keep_classes],selected_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_labels_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(predict[keep_classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lab1[-30:])\n",
    "t = np.equal(lab1[0::3],lab1[1::3])\n",
    "print(t)\n",
    "np.sum(t.astype(np.float32))/len(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "histories = []\n",
    "n = 4\n",
    "for i in range(n):\n",
    "    history=model.fit(\n",
    "        triplet_train,\n",
    "        y_triplet_train,\n",
    "        batch_size = 24*3,\n",
    "        epochs = n-i,\n",
    "        validation_data=(triplet_test,y_triplet_test),\n",
    "        shuffle=False\n",
    "    )\n",
    "    histories += [history]\n",
    "    triplet_train, y_triplet_train = define_triplets(images_train,labels_train)\n",
    "    triplet_test, y_triplet_test = define_triplets(images_test,labels_test,1000*3)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(PATH_MODEL+'2019.02.08.dogfacenet_v9.emb_16.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = histories[0]\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "acc = history.history['triplet_acc']\n",
    "val_acc = history.history['val_triplet_acc']\n",
    "history_ = np.array([loss,val_loss,acc,val_acc])\n",
    "# np.save(PATH_SAVE+'2018.02.08.dogfacenet_v6.triplet.a_0.3.npy',history_)\n",
    "# np.savetxt(PATH_SAVE+'2018.02.08.dogfacenet_v6.triplet.a_0.3.txt',history_)\n",
    "\n",
    "epochs = np.arange(len(loss))\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(epochs,loss, '-o', label=\"loss\")\n",
    "plt.plot(epochs,val_loss, '-o', label=\"val_loss\")\n",
    "plt.xlabel(\"Number of epochs\")\n",
    "plt.legend()\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(epochs,acc, '-o', label=\"acc\")\n",
    "plt.plot(epochs,val_acc, '-o', label=\"val_acc\")\n",
    "plt.xlabel(\"Number of epochs\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate on verification task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NBOF_PAIRS = 5000\n",
    "#NBOF_PAIRS = len(images_test)\n",
    "\n",
    "# Create pairs\n",
    "h,w,c = SIZE\n",
    "pairs = np.empty((NBOF_PAIRS*2,h,w,c))\n",
    "issame = np.empty(NBOF_PAIRS)\n",
    "class_test = np.unique(labels_test)\n",
    "for i in range(NBOF_PAIRS):\n",
    "    alea = np.random.rand()\n",
    "    # Pair of different dogs\n",
    "    if alea < 0.5:\n",
    "        # Chose the classes:\n",
    "        class1 = np.random.randint(len(class_test))\n",
    "        class2 = np.random.randint(len(class_test))\n",
    "        while class1==class2:\n",
    "            class2 = np.random.randint(len(class_test))\n",
    "            \n",
    "        # Extract images of this class:\n",
    "        images_class1 = images_test[np.equal(labels_test,class1)]\n",
    "        images_class2 = images_test[np.equal(labels_test,class2)]\n",
    "        \n",
    "        # Chose an image amoung these selected images\n",
    "        pairs[i*2] = images_class1[np.random.randint(len(images_class1))]\n",
    "        pairs[i*2+1] = images_class2[np.random.randint(len(images_class2))]\n",
    "        issame[i] = 0\n",
    "    # Pair of same dogs\n",
    "    else:\n",
    "        # Chose a class\n",
    "        clas = np.random.randint(len(class_test))\n",
    "        images_class = images_test[np.equal(labels_test,clas)]\n",
    "        \n",
    "        # Select two images from this class\n",
    "        idx_image1 = np.random.randint(len(images_class))\n",
    "        idx_image2 = np.random.randint(len(images_class))\n",
    "        while idx_image1 == idx_image2:\n",
    "            idx_image2 = np.random.randint(len(images_class))\n",
    "        \n",
    "        pairs[i*2] = images_class[idx_image1]\n",
    "        pairs[i*2+1] = images_class[idx_image2]\n",
    "        issame[i] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Test: Check the pairs\n",
    "s = 10\n",
    "n = 5\n",
    "print(issame[s:(n+s)])\n",
    "fig = plt.figure(figsize=(5,3*n))\n",
    "for i in range(s,s+n):\n",
    "    plt.subplot(n,2,2*(i-s)+1)\n",
    "    plt.imshow(pairs[2*i]*0.5+0.5)\n",
    "    plt.subplot(n,2,2*(i-s)+2)\n",
    "    plt.imshow(pairs[2*i+1]*0.5+0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = tf.keras.Model(model.layers[0].input, model.layers[-1].output)\n",
    "predict=mod.predict(pairs)\n",
    "# Separates the pairs\n",
    "emb1 = predict[0::2]\n",
    "emb2 = predict[1::2]\n",
    "\n",
    "# Computes distance between pairs\n",
    "diff = np.square(emb1-emb2)\n",
    "dist = np.sum(diff,1)\n",
    "\n",
    "\n",
    "best = 0\n",
    "best_t = 0\n",
    "thresholds = np.arange(0.0001,4,0.001)\n",
    "for i in range(len(thresholds)):\n",
    "    less = np.less(dist, thresholds[i])\n",
    "    acc = np.logical_not(np.logical_xor(less, issame))\n",
    "    acc = acc.astype(float)\n",
    "    out = np.sum(acc)\n",
    "    out = out/len(acc)\n",
    "    if out > best:\n",
    "        best_t = thresholds[i]\n",
    "        best = out\n",
    "\n",
    "print(\"Best threshold: \" + str(best_t))\n",
    "print(\"Best accuracy: \" + str(best))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = tf.keras.Model(model.layers[0].input, model.layers[-1].output)\n",
    "predict=mod.predict(pairs)\n",
    "# Separates the pairs\n",
    "emb1 = predict[0::2]\n",
    "emb2 = predict[1::2]\n",
    "\n",
    "# Computes distance between pairs\n",
    "diff = np.square(emb1-emb2)\n",
    "dist = np.sum(diff,1)\n",
    "\n",
    "# Computes the ROC depending on different thresholds\n",
    "\n",
    "thresholds = np.arange(0.0001,4,0.001)\n",
    "tprs = np.empty(len(thresholds))\n",
    "fprs = np.empty(len(thresholds))\n",
    "\n",
    "p = np.sum(issame.astype(float))\n",
    "n = np.sum(np.logical_not(issame).astype(float))\n",
    "\n",
    "for i in range(len(thresholds)):\n",
    "    logical_pred = np.less(dist, thresholds[i])\n",
    "    tp = np.sum(np.logical_and(logical_pred,issame).astype(float))\n",
    "    fp = np.sum(np.logical_and(logical_pred,np.logical_not(issame)).astype(float))\n",
    "    tprs[i] = tp/p\n",
    "    fprs[i] = fp/n\n",
    "    \n",
    "plt.figure(figsize=(7,7))\n",
    "plt.plot(fprs,tprs)\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC curve\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Face clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = tf.keras.Model(model.layers[0].input, model.layers[-1].output)\n",
    "predict=mod.predict(images_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=len(np.unique(labels_test)),max_iter=2000, random_state=0,tol=1.0).fit(predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_cluster = [images_test[np.equal(kmeans.labels_,i)] for i in range(len(labels_test))]\n",
    "labels_cluster = [labels_test[np.equal(kmeans.labels_,i)] for i in range(len(labels_test))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(images_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(images_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(images_cluster)):\n",
    "    length = len(images_cluster[i])\n",
    "    if length > 0:\n",
    "        print(labels_cluster[i])\n",
    "        fig=plt.figure(figsize=(length*2,2))\n",
    "        for j in range(length):\n",
    "            plt.subplot(1,length,j+1)\n",
    "            plt.imshow(images_cluster[i][j])\n",
    "            plt.xticks([])\n",
    "            plt.yticks([])\n",
    "        plt.show()\n",
    "        #fig.savefig('D:/CREATIONS/PAPERS/DogFaceNet/clustering/dfn11.clustering.'+str(i)+'.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recognition: One-shot learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = model.predict(images_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_acc = 0\n",
    "nbof_try = 1000 # We will do many try on the test dataset to obtain a more precize accuracy\n",
    "\n",
    "for k in tqdm_notebook(range(nbof_try)):\n",
    "    nbof_kimages=int(0.8*len(np.unique(labels_test)))\n",
    "    kpred=np.empty((nbof_kimages,pred_test.shape[-1]))\n",
    "    y_kimages = np.unique(labels_test)[:nbof_kimages]\n",
    "\n",
    "    others_pred = np.copy(pred_test)\n",
    "    y_others = np.copy(labels_test)\n",
    "\n",
    "    for i in range(nbof_kimages):\n",
    "        keep_classes_images = np.arange(len(y_others))[np.equal(y_kimages[i],y_others)]\n",
    "        choice = np.random.randint(len(keep_classes_images))\n",
    "\n",
    "        kpred[i] = others_pred[keep_classes_images[choice]]\n",
    "\n",
    "        others_pred = np.delete(others_pred,keep_classes_images[choice],0)\n",
    "        y_others = np.delete(y_others,keep_classes_images[choice],0)\n",
    "    \n",
    "    threshold = 0.3\n",
    "\n",
    "    acc = 0\n",
    "\n",
    "    class_pred = np.empty(len(others_pred))\n",
    "\n",
    "    for i in range(len(others_pred)):\n",
    "        # computes distance with the key dataset\n",
    "        dist = np.sum(np.square(kpred-others_pred[i]),1)\n",
    "        if np.min(dist) < threshold:\n",
    "            class_pred[i] = np.argmin(dist)\n",
    "            if np.argmin(dist)==y_others[i]:\n",
    "                acc += 1\n",
    "        else:\n",
    "            class_pred[i] = -1\n",
    "            if not y_others[i] in y_kimages:\n",
    "                acc += 1\n",
    "\n",
    "    acc /= len(others_pred)\n",
    "    \n",
    "    mean_acc += acc\n",
    "    \n",
    "mean_acc /= nbof_try\n",
    "print(\"Mean accuracy for a one shot learner: \" + str(mean_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test for one-shot learning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To train a one-shot learner on the test dataset:\n",
    "#  -we randomly select key images: one picture per classes among 80% of the classes\n",
    "#  -we take a new picture\n",
    "#  -we check if the dog if the dog is known or not:\n",
    "#   computes the distance between the embedding vector and every embeddings saved in\n",
    "#   the dataset and compares the given distance with a threshold\n",
    "#  -if not, return -1\n",
    "#  -if yes, return the corresponding class\n",
    "\n",
    "# Note: for the test we could have compute the prediction for every pictures and\n",
    "# then separate key frames from the others but we played RP here :)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# We randomly select key images: one picture per classes among 80% of the classes\n",
    "nbof_kimages=int(0.8*len(np.unique(labels_test)))\n",
    "kimages=np.empty((nbof_kimages,h,w,c))\n",
    "y_kimages = np.unique(labels_test)[:nbof_kimages]\n",
    "\n",
    "others = np.copy(images_test)\n",
    "y_others = np.copy(labels_test)\n",
    "\n",
    "for i in range(nbof_kimages):\n",
    "    keep_classes_images = np.arange(len(y_others))[np.equal(y_kimages[i],y_others)]\n",
    "    choice = np.random.randint(len(keep_classes_images))\n",
    "    \n",
    "    kimages[i] = others[keep_classes_images[choice]]\n",
    "    \n",
    "    others = np.delete(others,keep_classes_images[choice],0)\n",
    "    y_others = np.delete(y_others,keep_classes_images[choice],0)\n",
    "\n",
    "kpred = model.predict(kimages)\n",
    "\n",
    "# Prediction for every other pictures\n",
    "others_pred = model.predict(others)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = model.predict(images_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To train a one-shot learner on the test dataset:\n",
    "#  -we randomly select key images: one picture per classes among 80% of the classes\n",
    "#  -we take a new picture\n",
    "#  -we check if the dog if the dog is known or not:\n",
    "#   computes the distance between the embedding vector and every embeddings saved in\n",
    "#   the dataset and compares the given distance with a threshold\n",
    "#  -if not, return -1\n",
    "#  -if yes, return the corresponding class\n",
    "\n",
    "# Note: for the test we could have compute the prediction for every pictures and\n",
    "# then separate key frames from the others but we played RP here :)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# We randomly select key images: one picture per classes among 80% of the classes\n",
    "nbof_kimages=int(0.8*len(np.unique(labels_test)))\n",
    "kpred=np.empty((nbof_kimages,pred_test.shape[-1]))\n",
    "y_kimages = np.unique(labels_test)[:nbof_kimages]\n",
    "\n",
    "others_pred = np.copy(pred_test)\n",
    "y_others = np.copy(labels_test)\n",
    "\n",
    "for i in range(nbof_kimages):\n",
    "    keep_classes_images = np.arange(len(y_others))[np.equal(y_kimages[i],y_others)]\n",
    "    choice = np.random.randint(len(keep_classes_images))\n",
    "    \n",
    "    kpred[i] = others_pred[keep_classes_images[choice]]\n",
    "    \n",
    "    others_pred = np.delete(others_pred,keep_classes_images[choice],0)\n",
    "    y_others = np.delete(y_others,keep_classes_images[choice],0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will use the best threshold find on verification task\n",
    "threshold = 0.3\n",
    "\n",
    "# for i in range(len(others_pred)):\n",
    "\n",
    "acc = 0\n",
    "\n",
    "class_pred = np.empty(len(others_pred))\n",
    "\n",
    "for i in range(len(others_pred)):\n",
    "    # computes distance with the key dataset\n",
    "    dist = np.sum(np.square(kpred-others_pred[i]),1)\n",
    "    if np.min(dist) < threshold:\n",
    "        class_pred[i] = np.argmin(dist)\n",
    "        if np.argmin(dist)==y_others[i]:\n",
    "            acc += 1\n",
    "    else:\n",
    "        class_pred[i] = -1\n",
    "        if not y_others[i] in y_kimages:\n",
    "            acc += 1\n",
    "        \n",
    "acc /= len(others_pred)\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will use the best threshold find on verification task\n",
    "best_acc = 0\n",
    "best_t = 0.001\n",
    "for threshold in np.arange(0.001,1,0.001):\n",
    "\n",
    "    acc = 0\n",
    "\n",
    "    class_pred = np.empty(len(others_pred))\n",
    "\n",
    "    for i in range(len(others_pred)):\n",
    "        # computes distance with the key dataset\n",
    "        dist = np.sum(np.square(kpred-others_pred[i]),1)\n",
    "        if np.min(dist) < threshold:\n",
    "            class_pred[i] = np.argmin(dist)\n",
    "            if np.argmin(dist)==y_others[i]:\n",
    "                acc += 1\n",
    "        else:\n",
    "            class_pred[i] = -1\n",
    "            if not y_others[i] in y_kimages:\n",
    "                acc += 1\n",
    "    acc /= len(others_pred)\n",
    "    \n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        best_t = threshold\n",
    "\n",
    "\n",
    "print(best_acc)\n",
    "print(best_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "for i in range(38):\n",
    "    plt.subplot(5,8,i+1)\n",
    "    plt.imshow(kimages[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(13,18))\n",
    "for i in range(13*18):\n",
    "    plt.subplot(18,13,i+1)\n",
    "    plt.imshow(others[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recognition: K-NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_n_mins(t,n):\n",
    "    idx_mins = []\n",
    "    mins = []\n",
    "    for i in range(n):\n",
    "        idx_crt_min = 0\n",
    "        crt_min = t[0]\n",
    "        for j in range(1,len(t)):\n",
    "            if t[j] < crt_min:\n",
    "                crt_min = t[j]\n",
    "                idx_crt_min = j\n",
    "        idx_mins += [idx_crt_min]\n",
    "        mins += [crt_min]\n",
    "        t = t[:idx_crt_min] + t[idx_crt_min+1:]\n",
    "    return idx_mins, mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 2\n",
    "\n",
    "nbof_kimages=int(0.8*len(np.unique(labels_test)))\n",
    "kpred=np.empty((nbof_kimages*k,pred_test.shape[-1]))\n",
    "y_kimages = np.unique(labels_test)[:nbof_kimages]\n",
    "\n",
    "others_pred = np.copy(pred_test)\n",
    "y_others = np.copy(labels_test)\n",
    "\n",
    "for i in range(nbof_kimages):\n",
    "    keep_classes_images = np.arange(len(y_others))[np.equal(y_kimages[i],y_others)]\n",
    "    choices = np.empty(k)\n",
    "    for j in range(k):\n",
    "        choice = np.random.randint(len(keep_classes_images))\n",
    "        if k > len(keep_classes_images):\n",
    "            choices[j] = choice\n",
    "        else:\n",
    "            while choice in choices:\n",
    "                choice  = np.random.randint(len(keep_classes_images))\n",
    "            choices[j] = choice\n",
    "    \n",
    "    \n",
    "    for choice in choices:\n",
    "        kpred[i] = others_pred[keep_classes_images[choice]]\n",
    "\n",
    "        others_pred = np.delete(others_pred,keep_classes_images[choice],0)\n",
    "        y_others = np.delete(y_others,keep_classes_images[choice],0)\n",
    "\n",
    "threshold = 0.3\n",
    "\n",
    "acc = 0\n",
    "\n",
    "class_pred = np.empty(len(others_pred))\n",
    "\n",
    "for i in range(len(others_pred)):\n",
    "    # computes distance with the key dataset\n",
    "    dist = np.sum(np.square(kpred-others_pred[i]),1)\n",
    "    if np.min(dist) < threshold:\n",
    "        class_pred[i] = np.argmin(dist)\n",
    "        \n",
    "        idx_mins,_ = find_n_mins(dist,k+1)\n",
    "        if len(np.unique(idx_mins)) == k+1:\n",
    "            if np.argmin(dist)==y_others[i]:\n",
    "                acc += 1\n",
    "        else:\n",
    "            # Find the most probable class\n",
    "            ccount\n",
    "            for \n",
    "    else:\n",
    "        class_pred[i] = -1\n",
    "        if not y_others[i] in y_kimages:\n",
    "            acc += 1\n",
    "\n",
    "acc /= len(others_pred)\n",
    "\n",
    "mean_acc += acc\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recognition: Re-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 30.\n",
    "m = 0.3\n",
    "def cosine(y_true,y_pred):\n",
    "    \n",
    "    exp_s = K.exp(s * y_pred)\n",
    "    exp_s_m = K.exp(s * (y_pred - m))\n",
    "    \n",
    "    masked_exp_s_m = exp_s_m * y_true\n",
    "    \n",
    "    inv_mask = 1. - y_true\n",
    "    masked_exp_s = exp_s * inv_mask\n",
    "    \n",
    "    den = K.sum(masked_exp_s + masked_exp_s_m, axis=-1, keepdims=True)\n",
    "    out = masked_exp_s_m / den\n",
    "    out = K.sum(out,axis=-1)\n",
    "    ret = - K.log(out)\n",
    "    ret = K.sum(ret)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cosine(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, output_dim, **kwargs):\n",
    "        self.output_dim = output_dim\n",
    "        super(Cosine, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        shape = tf.TensorShape((input_shape[-1],self.output_dim))\n",
    "\n",
    "        self.kernel = self.add_weight(name='kernel', \n",
    "                                      shape=shape,\n",
    "                                      initializer='uniform',\n",
    "                                      trainable=True)\n",
    "        super(Cosine, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = tf.math.l2_normalize(x, axis=-1)\n",
    "        w = tf.math.l2_normalize(self.kernel, axis=0)\n",
    "        \n",
    "        return K.dot(x, w)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_test_exp = tf.keras.utils.to_categorical(labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#out = tf.keras.layers.Dense(128, activation='relu')(model.output)\n",
    "out = Cosine(len(labels_test_exp[0]))(model.output)\n",
    "#out = tf.keras.layers.Dense(24, activation='softmax')(out)\n",
    "recog = tf.keras.Model(model.input,out)\n",
    "for layer in model.layers: layer.trainable = False\n",
    "recog.compile(tf.keras.optimizers.Adam(),loss=cosine,metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "recog.fit(images_test,labels_test_exp,batch_size=64,epochs=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation on the heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod3 = tf.keras.Model(model.layers[0].input, model.layers[9].output)\n",
    "predict3 = mod3.predict(images_train[0:100:10])\n",
    "\n",
    "plt.figure(figsize=(15,15))\n",
    "for i in range(10):\n",
    "    plt.subplot(10,10,i*10+1)\n",
    "    sk.io.imshow(images_train[i*10])\n",
    "    \n",
    "    for j in range(9):\n",
    "        pred3 = np.mean(predict3[i][:,:,j*25:j*25+3],axis=-1)\n",
    "        plt.subplot(10,10,i*10+2+j)\n",
    "        sk.io.imshow(images_train[i*10])\n",
    "        plt.imshow(pred3,cmap='plasma')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "s = 100\n",
    "mod1 = tf.keras.Model(model.layers[0].input, model.layers[1].output)\n",
    "predict1 = mod1.predict(images_train[0+s:100+s:10])\n",
    "\n",
    "mod2 = tf.keras.Model(model.layers[0].input, model.layers[9].output)\n",
    "predict2 = mod2.predict(images_train[0+s:100+s:10])\n",
    "\n",
    "mod3 = tf.keras.Model(model.layers[0].input, model.layers[14].output)\n",
    "predict3 = mod3.predict(images_train[0+s:100+s:10])\n",
    "\n",
    "plt.figure(figsize=(9,20))\n",
    "for i in range(10):\n",
    "    pred1 = np.mean(predict1[i],axis=-1)\n",
    "    pred2 = np.mean(predict2[i],axis=-1)\n",
    "    pred3 = np.mean(predict3[i],axis=-1)\n",
    "\n",
    "    \n",
    "    plt.subplot(10,4,i*4+1)\n",
    "    sk.io.imshow(images_train[i*10 + s])\n",
    "    plt.subplot(10,4,i*4+2)\n",
    "    plt.imshow(pred1,cmap='plasma')\n",
    "    plt.subplot(10,4,i*4+3)\n",
    "    plt.imshow(pred2,cmap='plasma')\n",
    "    plt.subplot(10,4,i*4+4)\n",
    "    plt.imshow(pred3,cmap='plasma')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Archives:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# dogfacenet_v7\n",
    "out_num = len(unique)\n",
    "\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dropout, Dense, Lambda\n",
    "\n",
    "base_model = tf.keras.applications.VGG16(weights='imagenet', include_top=False, input_shape=(104, 104, 3))\n",
    "x = base_model.output\n",
    "# x = base_model.layers[8].output\n",
    "# x = MaxPooling2D((2,2))(x)\n",
    "# x = Dropout(0.25)(x)\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = tf.keras.layers.Flatten()(x)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(18)(x)\n",
    "x = Lambda(lambda x: tf.math.l2_normalize(x,axis=-1))(x)\n",
    "\n",
    "model = tf.keras.Model(inputs=base_model.input, outputs=x)\n",
    "\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "model.compile(loss=triplet,\n",
    "              optimizer='rmsprop',\n",
    "              metrics=[triplet_acc])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "triplet_train_pre = tf.keras.applications.vgg16.preprocess_input(triplet_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history=model.fit(\n",
    "    triplet_train_pre,\n",
    "    np.zeros((len(triplet_train),18)),\n",
    "    batch_size = 63,\n",
    "    epochs = 100,\n",
    "    validation_split=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# dogfacenet_v8 : inspired from FaceNet paper\n",
    "emb_size = 64\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.layers import Activation, Dropout, Flatten, Dense, Lambda, BatchNormalization\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(64, (5, 5), strides=(2,2), activation='relu', input_shape=(104, 104, 3)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(64, (1, 1), activation='relu', padding='same'))\n",
    "model.add(Conv2D(192, (3, 3), activation='relu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(192, (1, 1), activation='relu', padding='same'))\n",
    "model.add(Conv2D(384, (3, 3), activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(384, (1, 1), activation='relu', padding='same'))\n",
    "model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
    "\n",
    "model.add(Conv2D(256, (1, 1), activation='relu', padding='same'))\n",
    "model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
    "\n",
    "model.add(Conv2D(256, (1, 1), activation='relu', padding='same'))\n",
    "model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(2048, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(2048, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(emb_size))\n",
    "model.add(Lambda(lambda x: tf.nn.l2_normalize(x)))\n",
    "\n",
    "model.compile(loss=triplet,\n",
    "              optimizer='adagrad',\n",
    "              metrics=[triplet_acc])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# dogfacenet_v4\n",
    "out_num = len(unique)\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.layers import Activation, Dropout, Flatten, Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(64, (3, 3), input_shape=(104, 104, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(256, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(512, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(512, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(32))\n",
    "model.add(Cosine(out_num))\n",
    "\n",
    "model.compile(loss=cosine,\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = model.fit_generator(\n",
    "    train_datagen.flow(images_train,labels_train,batch_size = 64),\n",
    "    epochs = 12,\n",
    "    validation_data=(images_valid,labels_valid)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('../output/model/dogfacenet_v6_cosine.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
