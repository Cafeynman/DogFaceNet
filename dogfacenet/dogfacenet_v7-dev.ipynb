{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DogFaceNet version 7: Dev version\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import skimage as sk\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm_notebook\n",
    "import tensorflow.keras.backend as K\n",
    "from triplets_processing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = '../data/dogfacenet/aligned/after_2/'\n",
    "PATH_SAVE = '../output/history/'\n",
    "PATH_MODEL = '../output/model/'\n",
    "SIZE = (104,104,3)\n",
    "VALID_SPLIT = 0.1\n",
    "TEST_SPLIT = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data pre-processing\n",
    "- Load image and labels\n",
    "- Training set, validation set (close-set) and testing (open-set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = np.empty(0)\n",
    "labels = np.empty(0)\n",
    "idx = 0\n",
    "for root,dirs,files in os.walk(PATH):\n",
    "    if len(files)>1:\n",
    "        for i in range(len(files)):\n",
    "            files[i] = root + '/' + files[i]\n",
    "        filenames = np.append(filenames,files)\n",
    "        labels = np.append(labels,np.ones(len(files))*idx)\n",
    "        idx += 1\n",
    "print(len(labels))\n",
    "h,w,c = SIZE\n",
    "images = np.empty((len(filenames),h,w,c))\n",
    "for i,f in enumerate(filenames):\n",
    "    images[i] = sk.io.imread(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('filenames.npy',filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization\n",
    "images /= 255.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbof_classes = len(np.unique(labels))\n",
    "print(nbof_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rotation_range=8,\n",
    "    zoom_range=0.1,\n",
    "    fill_mode='nearest',\n",
    "    channel_shift_range = 0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open-set: test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbof_test = int(TEST_SPLIT*nbof_classes)\n",
    "\n",
    "keep_test = np.less(labels,nbof_test)\n",
    "keep_train = np.logical_not(keep_test)\n",
    "\n",
    "images_test = images[keep_test]\n",
    "labels_test = labels[keep_test]\n",
    "\n",
    "images_train = images[keep_train]\n",
    "labels_train = labels[keep_train]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Triplet definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#triplet_train, y_triplet_train = define_triplets(images_train,labels_train)\n",
    "triplet_test, y_triplet_test = define_triplets(images_test,labels_test,1000*3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.3\n",
    "def triplet(y_true,y_pred):\n",
    "    \n",
    "    a = y_pred[0::3]\n",
    "    p = y_pred[1::3]\n",
    "    n = y_pred[2::3]\n",
    "    \n",
    "    ap = K.sum(K.square(a-p),-1)\n",
    "    an = K.sum(K.square(a-n),-1)\n",
    "\n",
    "    return K.sum(tf.nn.relu(ap - an + alpha))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metric definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def triplet_acc(y_true,y_pred):\n",
    "    a = y_pred[0::3]\n",
    "    p = y_pred[1::3]\n",
    "    n = y_pred[2::3]\n",
    "    \n",
    "    ap = K.sum(K.square(a-p),-1)\n",
    "    an = K.sum(K.square(a-n),-1)\n",
    "    \n",
    "    return K.less(ap+alpha,an)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# dogfacenet_v11\n",
    "emb_size = 16\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.layers import Activation, Dropout, Flatten, Dense, Lambda, BatchNormalization\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=(104, 104, 3)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
    "model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))\n",
    "model.add(Conv2D(1024, (3, 3), activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(emb_size))\n",
    "model.add(Lambda(lambda x: tf.nn.l2_normalize(x,axis=-1)))\n",
    "\n",
    "model.compile(loss=triplet,\n",
    "              optimizer='adam',\n",
    "              metrics=[triplet_acc])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# dogfacenet_v12\n",
    "emb_size = 32\n",
    "\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Add, GlobalAveragePooling2D\n",
    "from tensorflow.keras.layers import Activation, Dropout, Flatten, Dense, Lambda, BatchNormalization\n",
    "\n",
    "inputs = Input(shape=(104, 104, 3))\n",
    "\n",
    "x = Conv2D(16, (7, 7), use_bias=False, activation='relu', padding='same')(inputs)\n",
    "x = BatchNormalization()(x)\n",
    "\n",
    "for layer in [16,32,64,128,512]:\n",
    "    # Batch normalization layer\n",
    "    x = Conv2D(layer, (3, 3), strides=(2,2), use_bias=False, activation='relu', padding='same')(x)\n",
    "    r = BatchNormalization()(x)\n",
    "    r = Dropout(0.25)(r)\n",
    "    \n",
    "    x = Conv2D(layer, (3, 3), use_bias=False, activation='relu', padding='same')(r)\n",
    "    x = BatchNormalization()(x)\n",
    "    r = Add()([r,x])\n",
    "    r = Dropout(0.25)(r)\n",
    "    \n",
    "    x = Conv2D(layer, (3, 3), use_bias=False, activation='relu', padding='same')(r)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Add()([r,x])\n",
    "    x = Dropout(0.25)(x)\n",
    "    \n",
    "    #model.add(Dropout(0.25))\n",
    "\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Flatten()(x)\n",
    "#model.add(Dense(1024, activation='relu'))\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(emb_size, use_bias=False)(x)\n",
    "outputs = Lambda(lambda x: tf.nn.l2_normalize(x,axis=-1))(x)\n",
    "\n",
    "model = tf.keras.Model(inputs,outputs)\n",
    "\n",
    "model.compile(loss=triplet,\n",
    "              optimizer='adam',\n",
    "              metrics=[triplet_acc])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# dogfacenet_v24\n",
    "emb_size = 32\n",
    "\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Add, GlobalAveragePooling2D\n",
    "from tensorflow.keras.layers import Activation, Dropout, Flatten, Dense, Lambda, BatchNormalization\n",
    "\n",
    "inputs = Input(shape=(104, 104, 3))\n",
    "\n",
    "x = Conv2D(64, (7, 7), use_bias=False, activation='relu', padding='same')(inputs)\n",
    "x = BatchNormalization()(x)\n",
    "\n",
    "for layer in [64,64,128,256,512]:\n",
    "    # Batch normalization layer\n",
    "    x = Conv2D(layer, (3, 3), strides=(2,2), use_bias=False, activation='relu', padding='same')(x)\n",
    "    r = BatchNormalization()(x)\n",
    "    r = Dropout(0.25)(r)\n",
    "    \n",
    "    x = Conv2D(layer, (3, 3), use_bias=False, activation='relu', padding='same')(r)\n",
    "    x = BatchNormalization()(x)\n",
    "    r = Add()([r,x])\n",
    "    r = Dropout(0.25)(r)\n",
    "    \n",
    "    x = Conv2D(layer, (3, 3), use_bias=False, activation='relu', padding='same')(r)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Add()([r,x])\n",
    "    x = Dropout(0.25)(x)\n",
    "    \n",
    "    #model.add(Dropout(0.25))\n",
    "\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Flatten()(x)\n",
    "#model.add(Dense(1024, activation='relu'))\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(emb_size, use_bias=False)(x)\n",
    "outputs = Lambda(lambda x: tf.nn.l2_normalize(x,axis=-1))(x)\n",
    "\n",
    "model = tf.keras.Model(inputs,outputs)\n",
    "\n",
    "model.compile(loss=triplet,\n",
    "              optimizer='adam',\n",
    "              metrics=[triplet_acc])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# dogfacenet_v23\n",
    "emb_size = 32\n",
    "\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Add, Concatenate, GlobalAveragePooling2D\n",
    "from tensorflow.keras.layers import Activation, Dropout, Flatten, Dense, Lambda, BatchNormalization\n",
    "\n",
    "inputs = Input(shape=(104, 104, 3))\n",
    "\n",
    "x = Conv2D(64, (7, 7), use_bias=False, activation='relu', padding='same')(inputs)\n",
    "x = BatchNormalization()(x)\n",
    "\n",
    "for layer in [64,128,512,1024]:\n",
    "    # Batch normalization layer\n",
    "    x = Conv2D(layer, (3, 3), strides=(2,2), use_bias=False, activation='relu', padding='same')(x)\n",
    "    r = BatchNormalization()(x)\n",
    "    r = Dropout(0.25)(r)\n",
    "    \n",
    "    for i in range(2):\n",
    "        gsize = layer//32\n",
    "        branch = []\n",
    "        for group in range(32):\n",
    "            x = Conv2D(gsize, (1, 1), use_bias=False, padding='same')(r)\n",
    "            x = BatchNormalization()(x)\n",
    "            \n",
    "            x = Conv2D(gsize, (3, 3), use_bias=False, activation='relu', padding='same')(x)\n",
    "            x = BatchNormalization()(x)\n",
    "            x = Dropout(0.5)(x)\n",
    "            \n",
    "            branch += [x]\n",
    "        \n",
    "        x = Concatenate()(branch)\n",
    "        x = Conv2D(layer, (1, 1), use_bias=False, activation='relu', padding='same')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        \n",
    "        x = Add()([r,x])\n",
    "        r = Dropout(0.25)(x)\n",
    "    \n",
    "\n",
    "x = GlobalAveragePooling2D()(r)\n",
    "x = Flatten()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(emb_size, use_bias=False)(x)\n",
    "outputs = Lambda(lambda x: tf.nn.l2_normalize(x,axis=-1))(x)\n",
    "\n",
    "model = tf.keras.Model(inputs,outputs)\n",
    "\n",
    "model.compile(loss=triplet,\n",
    "              optimizer='adam',\n",
    "              metrics=[triplet_acc])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# dogfacenet_v18\n",
    "emb_size = 32\n",
    "\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Add, GlobalAveragePooling2D\n",
    "from tensorflow.keras.layers import Activation, Dropout, Flatten, Dense, Lambda, BatchNormalization\n",
    "\n",
    "inputs = Input(shape=(104, 104, 3))\n",
    "\n",
    "x = Conv2D(32, (7, 7), use_bias=False, activation='relu', padding='same')(inputs)\n",
    "x = BatchNormalization()(x)\n",
    "\n",
    "for layer in [32,32,64,128,512]:\n",
    "    \n",
    "    for channel in range(4):\n",
    "    x = Conv2D(layer, (3, 3), strides=(2,2), use_bias=False, activation='relu', padding='same')(x)\n",
    "    r = BatchNormalization()(x)\n",
    "    r = Dropout(0.25)(r)\n",
    "    \n",
    "    x = Conv2D(layer, (3, 3), use_bias=False, activation='relu', padding='same')(r)\n",
    "    x = BatchNormalization()(x)\n",
    "    r = Add()([r,x])\n",
    "    r = Dropout(0.25)(r)\n",
    "    \n",
    "    x = Conv2D(layer, (3, 3), use_bias=False, activation='relu', padding='same')(r)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Add()([r,x])\n",
    "    x = Dropout(0.25)(x)\n",
    "    \n",
    "    #model.add(Dropout(0.25))\n",
    "\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Flatten()(x)\n",
    "#model.add(Dense(1024, activation='relu'))\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(emb_size, use_bias=False)(x)\n",
    "outputs = Lambda(lambda x: tf.nn.l2_normalize(x,axis=-1))(x)\n",
    "\n",
    "model = tf.keras.Model(inputs,outputs)\n",
    "\n",
    "model.compile(loss=triplet,\n",
    "              optimizer='adam',\n",
    "              metrics=[triplet_acc])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# dogfacenet_v20\n",
    "# close to official resnet with identity blocks\n",
    "emb_size = 32\n",
    "\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Add, GlobalAveragePooling2D\n",
    "from tensorflow.keras.layers import Activation, Dropout, Flatten, Dense, Lambda, BatchNormalization\n",
    "\n",
    "inputs = Input(shape=(104, 104, 3))\n",
    "\n",
    "x = Conv2D(16, (7, 7), use_bias=False, activation='relu', padding='same')(inputs)\n",
    "x = BatchNormalization()(x)\n",
    "\n",
    "layers = [16,32,64,128,512,1024]\n",
    "\n",
    "for i in range(len(layers)-1):\n",
    "    layer = layers[i]\n",
    "    layer2 = layers[i+1]\n",
    "    \n",
    "    x = Conv2D(layer2, (3, 3), strides=(2,2), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    r = Activation('relu')(x)\n",
    "    \n",
    "    # Resnet blocks\n",
    "    for j in range(2):\n",
    "        x = Conv2D(layer, (1, 1), padding='same')(r)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation('relu')(x)\n",
    "\n",
    "        x = Conv2D(layer, (3, 3), padding='same')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation('relu')(x)\n",
    "\n",
    "        x = Conv2D(layer2, (1, 1), padding='same')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        \n",
    "        x = Add()([r,x])\n",
    "        x = Activation('relu')(x)\n",
    "        r = Dropout(0.25)(x)\n",
    "\n",
    "x = GlobalAveragePooling2D()(r)\n",
    "x = Flatten()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(emb_size, use_bias=False)(x)\n",
    "outputs = Lambda(lambda x: tf.nn.l2_normalize(x,axis=-1))(x)\n",
    "\n",
    "model = tf.keras.Model(inputs,outputs)\n",
    "\n",
    "model.compile(loss=triplet,\n",
    "              optimizer='adam',\n",
    "              metrics=[triplet_acc])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# dogfacenet_v21\n",
    "# close to official resneXt with identity blocks\n",
    "emb_size = 32\n",
    "\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Add, Concatenate, GlobalAveragePooling2D\n",
    "from tensorflow.keras.layers import Activation, Dropout, Flatten, Dense, Lambda, BatchNormalization\n",
    "\n",
    "inputs = Input(shape=(104, 104, 3))\n",
    "\n",
    "x = Conv2D(32, (7, 7), use_bias=False, activation='relu', padding='same')(inputs)\n",
    "x = BatchNormalization()(x)\n",
    "\n",
    "layers = [32,64,128,512,1024,2048]\n",
    "\n",
    "for i in range(len(layers)-1):\n",
    "    layer = layers[i]\n",
    "    layer2 = layers[i+1]\n",
    "    \n",
    "    x = Conv2D(layer2, (3, 3), strides=(2,2), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    r = Activation('relu')(x)\n",
    "    \n",
    "    # ResneXt blocks, group 16\n",
    "    for j in range(2):\n",
    "        ksize = layer//16\n",
    "        branch_res = []\n",
    "        \n",
    "        for k in range(16):\n",
    "            x = Conv2D(ksize, (1, 1), padding='same')(r)\n",
    "            x = BatchNormalization()(x)\n",
    "            x = Activation('relu')(x)\n",
    "\n",
    "            x = Conv2D(ksize, (3, 3), padding='same')(x)\n",
    "            x = BatchNormalization()(x)\n",
    "            x = Activation('relu')(x)\n",
    "            x = Dropout(0.5)(x)\n",
    "            \n",
    "            branch_res += [x]\n",
    "        \n",
    "        x = Concatenate()(branch_res)\n",
    "        x = Conv2D(layer2, (1, 1), padding='same')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        \n",
    "        x = Add()([r,x])\n",
    "        x = Activation('relu')(x)\n",
    "        r = Dropout(0.25)(x)\n",
    "\n",
    "x = GlobalAveragePooling2D()(r)\n",
    "x = Flatten()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(emb_size, use_bias=False)(x)\n",
    "outputs = Lambda(lambda x: tf.nn.l2_normalize(x,axis=-1))(x)\n",
    "\n",
    "model = tf.keras.Model(inputs,outputs)\n",
    "\n",
    "model.compile(loss=triplet,\n",
    "              optimizer='adam',\n",
    "              metrics=[triplet_acc])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(PATH_MODEL + '2019.02.14.dogfacenet_v11.hard_triplet_trained.data_aug.20.h5', custom_objects={'triplet':triplet,'triplet_acc':triplet_acc})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(PATH_MODEL + '2019.02.27.dogfacenet_v11.hard_triplet_trained.data_aug_2.16.h5', custom_objects={'triplet':triplet,'triplet_acc':triplet_acc})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(triplet_test,y_triplet_test, batch_size=63)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import FileLink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "FileLink(PATH_MODEL + '2019.02.28.dogfacenet_v12.hard_triplet.alpha.0.3.2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FileLink(PATH_MODEL + '2019.02.28.dogfacenet_v12.hard_triplet.alpha.0.50.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FileLink(PATH_SAVE + '2018.02.27.dogfacenet_v11.hard_triplet.data_aug.10.a_0.3.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(PATH_MODEL + '2019.02.12.hard_triplet_trained.data_aug.0.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "triplet_train, y_triplet_train = define_triplets(images_train,labels_train)\n",
    "triplet_test, y_triplet_test = define_triplets(images_test,labels_test,1000*3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new_images_train, new_labels_train = shuffle_classes(images_train,labels_train)\n",
    "predict_train=model.predict(new_images_train)\n",
    "triplet_train, y_triplet_train = define_hard_triplets(new_images_train,new_labels_train,predict_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histories = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(6):\n",
    "    history = model.fit(\n",
    "        triplet_train,\n",
    "        y_triplet_train,\n",
    "        batch_size = 21*3,\n",
    "        epochs = 1,\n",
    "        validation_data=(triplet_test,y_triplet_test),\n",
    "        shuffle=False\n",
    "    )\n",
    "    histories += [history]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(PATH_MODEL + '2019.02.27.dogfacenet_v11.hard_triplet_trained.data_aug.'+str(16)+'.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histories2 = [histories[i] for i in range(29)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(PATH_MODEL + '2019.02.27.dogfacenet_v11.hard_triplet_trained.data_aug.6.h5', custom_objects={'triplet':triplet,'triplet_acc':triplet_acc})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2):\n",
    "    history = model.fit(\n",
    "        triplet_train,\n",
    "        y_triplet_train,\n",
    "        batch_size = 21*3,\n",
    "        epochs = 1,\n",
    "        validation_data=(triplet_test,y_triplet_test),\n",
    "        shuffle=False\n",
    "    )\n",
    "    histories2 += [history]\n",
    "model.save(PATH_MODEL + '2019.02.27.dogfacenet_v11.hard_triplet_trained.data_aug.'+str(16)+'.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histories = histories[:-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=triplet,\n",
    "              optimizer=tf.keras.optimizers.Adam(0.0001),\n",
    "              metrics=[triplet_acc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 14*3 epochs with lr=0.001\n",
    "- 4*3 epochs with lr=0.0005\n",
    "- 4*3 epochs with lr=0.0003\n",
    "- 4*1 epochs with lr=0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " model.save(PATH_MODEL + '2019.02.28.dogfacenet_v22.hard_triplet.alpha.0.3.'+str(l)+'.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histories = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epochs = [14,4,4,4]\n",
    "lr = [0.001,0.0005,0.0003,0.0001]\n",
    "for l in range(1,len(lr)):\n",
    "    model.compile(loss=triplet,\n",
    "              optimizer=tf.keras.optimizers.Adam(lr[l]),\n",
    "              metrics=[triplet_acc])\n",
    "    for k in range(epochs[l]):\n",
    "        epoch_nb = k+sum(epochs[:l])\n",
    "        print(\"Beginning epoch number: \"+str(epoch_nb)+\", over \"+str(sum(epochs))+\" epochs. \\n\")\n",
    "        \n",
    "        new_images_train, new_labels_train = shuffle_classes(images_train,labels_train)\n",
    "        predict_train=model.predict(new_images_train)\n",
    "        triplet_train, y_triplet_train = define_hard_triplets(new_images_train,new_labels_train,predict_train)\n",
    "\n",
    "        for i in range(3):\n",
    "            history = model.fit(\n",
    "                triplet_train,\n",
    "                y_triplet_train,\n",
    "                batch_size = 21*3,\n",
    "                epochs = 1,\n",
    "                validation_data=(triplet_test,y_triplet_test),\n",
    "                shuffle=False\n",
    "            )\n",
    "            histories += [history]\n",
    "\n",
    "    model.save(PATH_MODEL + '2019.03.02.dogfacenet_v24.hard_triplet.alpha.0.3.'+str(l)+'.h5')\n",
    "\n",
    "loss = np.empty(0)\n",
    "val_loss = np.empty(0)\n",
    "acc = np.empty(0)\n",
    "val_acc = np.empty(0)\n",
    "\n",
    "for history in histories:\n",
    "    loss = np.append(loss,history.history['loss'])\n",
    "    val_loss = np.append(val_loss,history.history['val_loss'])\n",
    "    acc = np.append(acc,history.history['triplet_acc'])\n",
    "    val_acc = np.append(val_acc,history.history['val_triplet_acc'])\n",
    "\n",
    "    \n",
    "history_ = np.array([loss,val_loss,acc,val_acc])\n",
    "np.save(PATH_SAVE+'2018.03.02.dogfacenet_v24.hard_triplet.data_aug.0.a_0.3.npy',history_)\n",
    "np.savetxt(PATH_SAVE+'2018.03.02.dogfacenet_v24.hard_triplet.data_aug.0.a_0.3.txt',history_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(PATH_MODEL + '2019.02.28.dogfacenet_v24.hard_triplet.alpha.0.3.'+str(0)+'.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_ = np.load(PATH_SAVE+'2018.02.28.dogfacenet_v20.hard_triplet.data_aug.0.a_0.3.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss,val_loss,acc,val_acc = history_\n",
    "\n",
    "epochs = np.arange(len(loss))\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(epochs,loss, '-o', label=\"loss\")\n",
    "plt.plot(epochs,val_loss, '-o', label=\"val_loss\")\n",
    "plt.xlabel(\"Number of epochs\")\n",
    "plt.yticks(np.arange(0,8,1))\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(epochs,acc, '-o', label=\"acc\")\n",
    "plt.plot(epochs,val_acc, '-o', label=\"val_acc\")\n",
    "plt.xlabel(\"Number of epochs\")\n",
    "plt.yticks(np.arange(0,1.1,0.1))\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "loss = np.empty(0)\n",
    "val_loss = np.empty(0)\n",
    "acc = np.empty(0)\n",
    "val_acc = np.empty(0)\n",
    "\n",
    "for history in histories:\n",
    "    loss = np.append(loss,history.history['loss'])\n",
    "    val_loss = np.append(val_loss,history.history['val_loss'])\n",
    "    acc = np.append(acc,history.history['triplet_acc'])\n",
    "    val_acc = np.append(val_acc,history.history['val_triplet_acc'])\n",
    "\n",
    "    \n",
    "history_ = np.array([loss,val_loss,acc,val_acc])\n",
    "np.save(PATH_SAVE+'2018.02.28.dogfacenet_v24.hard_triplet.data_aug.0.a_0.3.npy',history_)\n",
    "np.savetxt(PATH_SAVE+'2018.02.28.dogfacenet_v24.hard_triplet.data_aug.0.a_0.3.txt',history_)\n",
    "\n",
    "epochs = np.arange(len(loss))\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(epochs,loss, '-o', label=\"loss\")\n",
    "plt.plot(epochs,val_loss, '-o', label=\"val_loss\")\n",
    "plt.xlabel(\"Number of epochs\")\n",
    "plt.yticks(np.arange(0,8,1))\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(epochs,acc, '-o', label=\"acc\")\n",
    "plt.plot(epochs,val_acc, '-o', label=\"val_acc\")\n",
    "plt.xlabel(\"Number of epochs\")\n",
    "plt.yticks(np.arange(0,1.1,0.1))\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    new_images_train, new_labels_train = shuffle_classes(images_train,labels_train)\n",
    "    predict_train=model.predict(new_images_train)\n",
    "    triplet_train, y_triplet_train = define_hard_triplets(new_images_train,new_labels_train,predict_train)\n",
    "\n",
    "    model.fit(\n",
    "        triplet_train,\n",
    "        y_triplet_train,\n",
    "        batch_size = 21*3,\n",
    "        epochs = 1,\n",
    "        validation_data=(triplet_test,y_triplet_test),\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    model.save(PATH_MODEL + '2019.02.14.dogfacenet_v11.hard_triplet_trained.data_aug.'+str(i+16)+'.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new_images_train, new_labels_train = shuffle_classes(images_train,labels_train)\n",
    "predict_train=model.predict(new_images_train)\n",
    "triplet_train, y_triplet_train = define_hard_triplets(new_images_train,new_labels_train,predict_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history=model.fit(\n",
    "    triplet_train,\n",
    "    y_triplet_train,\n",
    "    batch_size = 21*3,\n",
    "    epochs = 1,\n",
    "    validation_data=(triplet_test,y_triplet_test),\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(PATH_MODEL + '2019.02.14.dogfacenet_v11.hard_triplet_trained.data_aug.6.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = np.empty(0)\n",
    "val_loss = np.empty(0)\n",
    "acc = np.empty(0)\n",
    "val_acc = np.empty(0)\n",
    "\n",
    "loss = np.append(loss,history.history['loss'])\n",
    "val_loss = np.append(val_loss,history.history['val_loss'])\n",
    "acc = np.append(acc,history.history['triplet_acc'])\n",
    "val_acc = np.append(val_acc,history.history['val_triplet_acc'])\n",
    "    \n",
    "    \n",
    "history_ = np.array([loss,val_loss,acc,val_acc])\n",
    "np.save(PATH_SAVE+'2018.02.12.dogfacenet_v6.hard_triplet.data_aug.0.a_0.3.npy',history_)\n",
    "np.savetxt(PATH_SAVE+'2018.02.12.dogfacenet_v6.hard_triplet.data_aug.0.a_0.3.txt',history_)\n",
    "\n",
    "epochs = np.arange(len(loss))\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(epochs,loss, '-o', label=\"loss\")\n",
    "plt.plot(epochs,val_loss, '-o', label=\"val_loss\")\n",
    "plt.xlabel(\"Number of epochs\")\n",
    "plt.legend()\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(epochs,acc, '-o', label=\"acc\")\n",
    "plt.plot(epochs,val_acc, '-o', label=\"val_acc\")\n",
    "plt.xlabel(\"Number of epochs\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "histories = []\n",
    "nbof_cycles = 9\n",
    "for i in range(0,nbof_cycles,1):\n",
    "\n",
    "    history=model.fit(\n",
    "        triplet_train,\n",
    "        y_triplet_train,\n",
    "        batch_size = 21*3,\n",
    "        epochs = nbof_cycles-i,\n",
    "        validation_data=(triplet_test,y_triplet_test),\n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    histories += [history]\n",
    "    model.save(PATH_MODEL + '2019.02.14.dogfacenet_v10.hard_triplet_trained.data_aug.' + str(i+1) + '.h5')\n",
    "    \n",
    "    new_images_train, new_labels_train = shuffle_classes(images_train,labels_train)\n",
    "    predict_train=model.predict(new_images_train)\n",
    "    triplet_train, y_triplet_train = define_hard_triplets(new_images_train,new_labels_train,predict_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = np.empty(0)\n",
    "val_loss = np.empty(0)\n",
    "acc = np.empty(0)\n",
    "val_acc = np.empty(0)\n",
    "\n",
    "for i in range(len(histories)):\n",
    "    history = histories[i]\n",
    "    \n",
    "    loss = np.append(loss,history.history['loss'])\n",
    "    val_loss = np.append(val_loss,history.history['val_loss'])\n",
    "    acc = np.append(acc,history.history['triplet_acc'])\n",
    "    val_acc = np.append(val_acc,history.history['val_triplet_acc'])\n",
    "    \n",
    "    \n",
    "history_ = np.array([loss,val_loss,acc,val_acc])\n",
    "np.save(PATH_SAVE+'2018.02.13.dogfacenet_v9.hard_triplet.2.a_0.3.npy',history_)\n",
    "np.savetxt(PATH_SAVE+'2018.02.13.dogfacenet_v9.hard_triplet.2.a_0.3.txt',history_)\n",
    "\n",
    "epochs = np.arange(len(loss))\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(epochs,loss, '-o', label=\"loss\")\n",
    "plt.plot(epochs,val_loss, '-o', label=\"val_loss\")\n",
    "plt.xlabel(\"Number of epochs\")\n",
    "plt.legend()\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(epochs,acc, '-o', label=\"acc\")\n",
    "plt.plot(epochs,val_acc, '-o', label=\"val_acc\")\n",
    "plt.xlabel(\"Number of epochs\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on new triplet integrity\n",
    "print(new_y_triplet_train[-30:])\n",
    "t = np.equal(new_y_triplet_train[0::3],new_y_triplet_train[1::3])\n",
    "print(t)\n",
    "np.sum(t.astype(np.float32))/len(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate on verification task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NBOF_PAIRS = 5000\n",
    "#NBOF_PAIRS = len(images_test)\n",
    "\n",
    "# Create pairs\n",
    "h,w,c = SIZE\n",
    "pairs = np.empty((NBOF_PAIRS*2,h,w,c))\n",
    "issame = np.empty(NBOF_PAIRS)\n",
    "class_test = np.unique(labels_test)\n",
    "for i in range(NBOF_PAIRS):\n",
    "    alea = np.random.rand()\n",
    "    # Pair of different dogs\n",
    "    if alea < 0.5:\n",
    "        # Chose the classes:\n",
    "        class1 = np.random.randint(len(class_test))\n",
    "        class2 = np.random.randint(len(class_test))\n",
    "        while class1==class2:\n",
    "            class2 = np.random.randint(len(class_test))\n",
    "            \n",
    "        # Extract images of this class:\n",
    "        images_class1 = images_test[np.equal(labels_test,class1)]\n",
    "        images_class2 = images_test[np.equal(labels_test,class2)]\n",
    "        \n",
    "        # Chose an image amoung these selected images\n",
    "        pairs[i*2] = images_class1[np.random.randint(len(images_class1))]\n",
    "        pairs[i*2+1] = images_class2[np.random.randint(len(images_class2))]\n",
    "        issame[i] = 0\n",
    "    # Pair of same dogs\n",
    "    else:\n",
    "        # Chose a class\n",
    "        clas = np.random.randint(len(class_test))\n",
    "        images_class = images_test[np.equal(labels_test,clas)]\n",
    "        \n",
    "        # Select two images from this class\n",
    "        idx_image1 = np.random.randint(len(images_class))\n",
    "        idx_image2 = np.random.randint(len(images_class))\n",
    "        while idx_image1 == idx_image2:\n",
    "            idx_image2 = np.random.randint(len(images_class))\n",
    "        \n",
    "        pairs[i*2] = images_class[idx_image1]\n",
    "        pairs[i*2+1] = images_class[idx_image2]\n",
    "        issame[i] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Test: Check the pairs\n",
    "s = 10\n",
    "n = 5\n",
    "print(issame[s:(n+s)])\n",
    "fig = plt.figure(figsize=(5,3*n))\n",
    "for i in range(s,s+n):\n",
    "    plt.subplot(n,2,2*(i-s)+1)\n",
    "    plt.imshow(pairs[2*i]*0.5+0.5)\n",
    "    plt.subplot(n,2,2*(i-s)+2)\n",
    "    plt.imshow(pairs[2*i+1]*0.5+0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = tf.keras.Model(model.layers[0].input, model.layers[-1].output)\n",
    "predict=mod.predict(pairs)\n",
    "# Separates the pairs\n",
    "emb1 = predict[0::2]\n",
    "emb2 = predict[1::2]\n",
    "\n",
    "# Computes distance between pairs\n",
    "diff = np.square(emb1-emb2)\n",
    "dist = np.sum(diff,1)\n",
    "\n",
    "\n",
    "best = 0\n",
    "best_t = 0\n",
    "thresholds = np.arange(0.0001,4,0.001)\n",
    "for i in range(len(thresholds)):\n",
    "    less = np.less(dist, thresholds[i])\n",
    "    acc = np.logical_not(np.logical_xor(less, issame))\n",
    "    acc = acc.astype(float)\n",
    "    out = np.sum(acc)\n",
    "    out = out/len(acc)\n",
    "    if out > best:\n",
    "        best_t = thresholds[i]\n",
    "        best = out\n",
    "\n",
    "print(\"Best threshold: \" + str(best_t))\n",
    "print(\"Best accuracy: \" + str(best))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = tf.keras.Model(model.layers[0].input, model.layers[-1].output)\n",
    "predict=mod.predict(pairs)\n",
    "# Separates the pairs\n",
    "emb1 = predict[0::2]\n",
    "emb2 = predict[1::2]\n",
    "\n",
    "# Computes distance between pairs\n",
    "diff = np.square(emb1-emb2)\n",
    "dist = np.sum(diff,1)\n",
    "\n",
    "# Computes the ROC depending on different thresholds\n",
    "\n",
    "thresholds = np.arange(0.0001,4,0.001)\n",
    "tprs = np.empty(len(thresholds))\n",
    "fprs = np.empty(len(thresholds))\n",
    "\n",
    "p = np.sum(issame.astype(float))\n",
    "n = np.sum(np.logical_not(issame).astype(float))\n",
    "\n",
    "for i in range(len(thresholds)):\n",
    "    logical_pred = np.less(dist, thresholds[i])\n",
    "    tp = np.sum(np.logical_and(logical_pred,issame).astype(float))\n",
    "    fp = np.sum(np.logical_and(logical_pred,np.logical_not(issame)).astype(float))\n",
    "    tprs[i] = tp/p\n",
    "    fprs[i] = fp/n\n",
    "    \n",
    "plt.figure(figsize=(7,7))\n",
    "plt.plot(fprs,tprs)\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC curve\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Face clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = tf.keras.Model(model.layers[0].input, model.layers[-1].output)\n",
    "predict=mod.predict(images_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=len(np.unique(labels_test)),max_iter=2000, random_state=0,tol=0.3).fit(predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_cluster = [images_test[np.equal(kmeans.labels_,i)] for i in range(len(labels_test))]\n",
    "labels_cluster = [labels_test[np.equal(kmeans.labels_,i)] for i in range(len(labels_test))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(images_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(images_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(images_cluster)):\n",
    "    length = len(images_cluster[i])\n",
    "    if length > 0:\n",
    "        print(labels_cluster[i])\n",
    "        fig=plt.figure(figsize=(length*2,2))\n",
    "        for j in range(length):\n",
    "            plt.subplot(1,length,j+1)\n",
    "            plt.imshow(images_cluster[i][j])\n",
    "            plt.xticks([])\n",
    "            plt.yticks([])\n",
    "        plt.show()\n",
    "        #fig.savefig('D:/CREATIONS/PAPERS/DogFaceNet/clustering/dfn11.clustering.'+str(i)+'.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recognition: One-shot learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = model.predict(images_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_acc = 0\n",
    "nbof_try = 1000 # We will do many try on the test dataset to obtain a more precize accuracy\n",
    "\n",
    "for k in tqdm_notebook(range(nbof_try)):\n",
    "    nbof_kimages=int(0.8*len(np.unique(labels_test)))\n",
    "    kpred=np.empty((nbof_kimages,pred_test.shape[-1]))\n",
    "    y_kimages = np.unique(labels_test)[:nbof_kimages]\n",
    "\n",
    "    others_pred = np.copy(pred_test)\n",
    "    y_others = np.copy(labels_test)\n",
    "\n",
    "    for i in range(nbof_kimages):\n",
    "        keep_classes_images = np.arange(len(y_others))[np.equal(y_kimages[i],y_others)]\n",
    "        choice = np.random.randint(len(keep_classes_images))\n",
    "\n",
    "        kpred[i] = others_pred[keep_classes_images[choice]]\n",
    "\n",
    "        others_pred = np.delete(others_pred,keep_classes_images[choice],0)\n",
    "        y_others = np.delete(y_others,keep_classes_images[choice],0)\n",
    "    \n",
    "    threshold = 0.3\n",
    "\n",
    "    acc = 0\n",
    "\n",
    "    class_pred = np.empty(len(others_pred))\n",
    "\n",
    "    for i in range(len(others_pred)):\n",
    "        # computes distance with the key dataset\n",
    "        dist = np.sum(np.square(kpred-others_pred[i]),1)\n",
    "        if np.min(dist) < threshold:\n",
    "            class_pred[i] = np.argmin(dist)\n",
    "            if np.argmin(dist)==y_others[i]:\n",
    "                acc += 1\n",
    "        else:\n",
    "            class_pred[i] = -1\n",
    "            if not y_others[i] in y_kimages:\n",
    "                acc += 1\n",
    "\n",
    "    acc /= len(others_pred)\n",
    "    \n",
    "    mean_acc += acc\n",
    "    \n",
    "mean_acc /= nbof_try\n",
    "print(\"Mean accuracy for a one shot learner: \" + str(mean_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test for one-shot learning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To train a one-shot learner on the test dataset:\n",
    "#  -we randomly select key images: one picture per classes among 80% of the classes\n",
    "#  -we take a new picture\n",
    "#  -we check if the dog if the dog is known or not:\n",
    "#   computes the distance between the embedding vector and every embeddings saved in\n",
    "#   the dataset and compares the given distance with a threshold\n",
    "#  -if not, return -1\n",
    "#  -if yes, return the corresponding class\n",
    "\n",
    "# Note: for the test we could have compute the prediction for every pictures and\n",
    "# then separate key frames from the others but we played RP here :)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# We randomly select key images: one picture per classes among 80% of the classes\n",
    "nbof_kimages=int(0.8*len(np.unique(labels_test)))\n",
    "kimages=np.empty((nbof_kimages,h,w,c))\n",
    "y_kimages = np.unique(labels_test)[:nbof_kimages]\n",
    "\n",
    "others = np.copy(images_test)\n",
    "y_others = np.copy(labels_test)\n",
    "\n",
    "for i in range(nbof_kimages):\n",
    "    keep_classes_images = np.arange(len(y_others))[np.equal(y_kimages[i],y_others)]\n",
    "    choice = np.random.randint(len(keep_classes_images))\n",
    "    \n",
    "    kimages[i] = others[keep_classes_images[choice]]\n",
    "    \n",
    "    others = np.delete(others,keep_classes_images[choice],0)\n",
    "    y_others = np.delete(y_others,keep_classes_images[choice],0)\n",
    "\n",
    "kpred = model.predict(kimages)\n",
    "\n",
    "# Prediction for every other pictures\n",
    "others_pred = model.predict(others)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = model.predict(images_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To train a one-shot learner on the test dataset:\n",
    "#  -we randomly select key images: one picture per classes among 80% of the classes\n",
    "#  -we take a new picture\n",
    "#  -we check if the dog if the dog is known or not:\n",
    "#   computes the distance between the embedding vector and every embeddings saved in\n",
    "#   the dataset and compares the given distance with a threshold\n",
    "#  -if not, return -1\n",
    "#  -if yes, return the corresponding class\n",
    "\n",
    "# Note: for the test we could have compute the prediction for every pictures and\n",
    "# then separate key frames from the others but we played RP here :)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# We randomly select key images: one picture per classes among 80% of the classes\n",
    "nbof_kimages=int(0.8*len(np.unique(labels_test)))\n",
    "kpred=np.empty((nbof_kimages,pred_test.shape[-1]))\n",
    "y_kimages = np.unique(labels_test)[:nbof_kimages]\n",
    "\n",
    "others_pred = np.copy(pred_test)\n",
    "y_others = np.copy(labels_test)\n",
    "\n",
    "for i in range(nbof_kimages):\n",
    "    keep_classes_images = np.arange(len(y_others))[np.equal(y_kimages[i],y_others)]\n",
    "    choice = np.random.randint(len(keep_classes_images))\n",
    "    \n",
    "    kpred[i] = others_pred[keep_classes_images[choice]]\n",
    "    \n",
    "    others_pred = np.delete(others_pred,keep_classes_images[choice],0)\n",
    "    y_others = np.delete(y_others,keep_classes_images[choice],0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will use the best threshold find on verification task\n",
    "threshold = 0.3\n",
    "\n",
    "# for i in range(len(others_pred)):\n",
    "\n",
    "acc = 0\n",
    "\n",
    "class_pred = np.empty(len(others_pred))\n",
    "\n",
    "for i in range(len(others_pred)):\n",
    "    # computes distance with the key dataset\n",
    "    dist = np.sum(np.square(kpred-others_pred[i]),1)\n",
    "    if np.min(dist) < threshold:\n",
    "        class_pred[i] = np.argmin(dist)\n",
    "        if np.argmin(dist)==y_others[i]:\n",
    "            acc += 1\n",
    "    else:\n",
    "        class_pred[i] = -1\n",
    "        if not y_others[i] in y_kimages:\n",
    "            acc += 1\n",
    "        \n",
    "acc /= len(others_pred)\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will use the best threshold find on verification task\n",
    "best_acc = 0\n",
    "best_t = 0.001\n",
    "for threshold in np.arange(0.001,1,0.001):\n",
    "\n",
    "    acc = 0\n",
    "\n",
    "    class_pred = np.empty(len(others_pred))\n",
    "\n",
    "    for i in range(len(others_pred)):\n",
    "        # computes distance with the key dataset\n",
    "        dist = np.sum(np.square(kpred-others_pred[i]),1)\n",
    "        if np.min(dist) < threshold:\n",
    "            class_pred[i] = np.argmin(dist)\n",
    "            if np.argmin(dist)==y_others[i]:\n",
    "                acc += 1\n",
    "        else:\n",
    "            class_pred[i] = -1\n",
    "            if not y_others[i] in y_kimages:\n",
    "                acc += 1\n",
    "    acc /= len(others_pred)\n",
    "    \n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        best_t = threshold\n",
    "\n",
    "\n",
    "print(best_acc)\n",
    "print(best_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "for i in range(38):\n",
    "    plt.subplot(5,8,i+1)\n",
    "    plt.imshow(kimages[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(13,18))\n",
    "for i in range(13*18):\n",
    "    plt.subplot(18,13,i+1)\n",
    "    plt.imshow(others[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recognition: K-NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_n_mins(t,n):\n",
    "    idx_mins = []\n",
    "    mins = []\n",
    "    for i in range(n):\n",
    "        idx_crt_min = 0\n",
    "        crt_min = t[0]\n",
    "        for j in range(1,len(t)):\n",
    "            if t[j] < crt_min:\n",
    "                crt_min = t[j]\n",
    "                idx_crt_min = j\n",
    "        idx_mins += [idx_crt_min]\n",
    "        mins += [crt_min]\n",
    "        t = t[:idx_crt_min] + t[idx_crt_min+1:]\n",
    "    return idx_mins, mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 2\n",
    "\n",
    "nbof_kimages=int(0.8*len(np.unique(labels_test)))\n",
    "kpred=np.empty((nbof_kimages*k,pred_test.shape[-1]))\n",
    "y_kimages = np.unique(labels_test)[:nbof_kimages]\n",
    "\n",
    "others_pred = np.copy(pred_test)\n",
    "y_others = np.copy(labels_test)\n",
    "\n",
    "for i in range(nbof_kimages):\n",
    "    keep_classes_images = np.arange(len(y_others))[np.equal(y_kimages[i],y_others)]\n",
    "    choices = np.empty(k)\n",
    "    for j in range(k):\n",
    "        choice = np.random.randint(len(keep_classes_images))\n",
    "        if k > len(keep_classes_images):\n",
    "            choices[j] = choice\n",
    "        else:\n",
    "            while choice in choices:\n",
    "                choice  = np.random.randint(len(keep_classes_images))\n",
    "            choices[j] = choice\n",
    "    \n",
    "    \n",
    "    for choice in choices:\n",
    "        kpred[i] = others_pred[keep_classes_images[choice]]\n",
    "\n",
    "        others_pred = np.delete(others_pred,keep_classes_images[choice],0)\n",
    "        y_others = np.delete(y_others,keep_classes_images[choice],0)\n",
    "\n",
    "threshold = 0.3\n",
    "\n",
    "acc = 0\n",
    "\n",
    "class_pred = np.empty(len(others_pred))\n",
    "\n",
    "for i in range(len(others_pred)):\n",
    "    # computes distance with the key dataset\n",
    "    dist = np.sum(np.square(kpred-others_pred[i]),1)\n",
    "    if np.min(dist) < threshold:\n",
    "        class_pred[i] = np.argmin(dist)\n",
    "        \n",
    "        idx_mins,_ = find_n_mins(dist,k+1)\n",
    "        if len(np.unique(idx_mins)) == k+1:\n",
    "            if np.argmin(dist)==y_others[i]:\n",
    "                acc += 1\n",
    "        else:\n",
    "            # Find the most probable class\n",
    "            ccount\n",
    "            for \n",
    "    else:\n",
    "        class_pred[i] = -1\n",
    "        if not y_others[i] in y_kimages:\n",
    "            acc += 1\n",
    "\n",
    "acc /= len(others_pred)\n",
    "\n",
    "mean_acc += acc\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recognition: Re-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 30.\n",
    "m = 0.3\n",
    "def cosine(y_true,y_pred):\n",
    "    \n",
    "    exp_s = K.exp(s * y_pred)\n",
    "    exp_s_m = K.exp(s * (y_pred - m))\n",
    "    \n",
    "    masked_exp_s_m = exp_s_m * y_true\n",
    "    \n",
    "    inv_mask = 1. - y_true\n",
    "    masked_exp_s = exp_s * inv_mask\n",
    "    \n",
    "    den = K.sum(masked_exp_s + masked_exp_s_m, axis=-1, keepdims=True)\n",
    "    out = masked_exp_s_m / den\n",
    "    out = K.sum(out,axis=-1)\n",
    "    ret = - K.log(out)\n",
    "    ret = K.sum(ret)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cosine(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, output_dim, **kwargs):\n",
    "        self.output_dim = output_dim\n",
    "        super(Cosine, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        shape = tf.TensorShape((input_shape[-1],self.output_dim))\n",
    "\n",
    "        self.kernel = self.add_weight(name='kernel', \n",
    "                                      shape=shape,\n",
    "                                      initializer='uniform',\n",
    "                                      trainable=True)\n",
    "        super(Cosine, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = tf.math.l2_normalize(x, axis=-1)\n",
    "        w = tf.math.l2_normalize(self.kernel, axis=0)\n",
    "        \n",
    "        return K.dot(x, w)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_test_exp = tf.keras.utils.to_categorical(labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#out = tf.keras.layers.Dense(128, activation='relu')(model.output)\n",
    "out = Cosine(len(labels_test_exp[0]))(model.output)\n",
    "#out = tf.keras.layers.Dense(24, activation='softmax')(out)\n",
    "recog = tf.keras.Model(model.input,out)\n",
    "for layer in model.layers: layer.trainable = False\n",
    "recog.compile(tf.keras.optimizers.Adam(),loss=cosine,metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "recog.fit(images_test,labels_test_exp,batch_size=64,epochs=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation on the heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod3 = tf.keras.Model(model.layers[0].input, model.layers[9].output)\n",
    "predict3 = mod3.predict(images_train[0:100:10])\n",
    "\n",
    "plt.figure(figsize=(15,15))\n",
    "for i in range(10):\n",
    "    plt.subplot(10,10,i*10+1)\n",
    "    sk.io.imshow(images_train[i*10])\n",
    "    \n",
    "    for j in range(9):\n",
    "        pred3 = np.mean(predict3[i][:,:,j*25:j*25+3],axis=-1)\n",
    "        plt.subplot(10,10,i*10+2+j)\n",
    "        sk.io.imshow(images_train[i*10])\n",
    "        plt.imshow(pred3,cmap='plasma')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "s = 100\n",
    "mod1 = tf.keras.Model(model.layers[0].input, model.layers[1].output)\n",
    "predict1 = mod1.predict(images_train[0+s:100+s:10])\n",
    "\n",
    "mod2 = tf.keras.Model(model.layers[0].input, model.layers[9].output)\n",
    "predict2 = mod2.predict(images_train[0+s:100+s:10])\n",
    "\n",
    "mod3 = tf.keras.Model(model.layers[0].input, model.layers[14].output)\n",
    "predict3 = mod3.predict(images_train[0+s:100+s:10])\n",
    "\n",
    "plt.figure(figsize=(9,20))\n",
    "for i in range(10):\n",
    "    pred1 = np.mean(predict1[i],axis=-1)\n",
    "    pred2 = np.mean(predict2[i],axis=-1)\n",
    "    pred3 = np.mean(predict3[i],axis=-1)\n",
    "\n",
    "    \n",
    "    plt.subplot(10,4,i*4+1)\n",
    "    sk.io.imshow(images_train[i*10 + s])\n",
    "    plt.subplot(10,4,i*4+2)\n",
    "    plt.imshow(pred1,cmap='plasma')\n",
    "    plt.subplot(10,4,i*4+3)\n",
    "    plt.imshow(pred2,cmap='plasma')\n",
    "    plt.subplot(10,4,i*4+4)\n",
    "    plt.imshow(pred3,cmap='plasma')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = model.fit_generator(\n",
    "    train_datagen.flow(images_train,labels_train,batch_size = 64),\n",
    "    epochs = 12,\n",
    "    validation_data=(images_valid,labels_valid)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('../output/model/dogfacenet_v6_cosine.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
