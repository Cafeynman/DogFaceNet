{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DogFaceNet: test file with MNIST dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import skimage as sk\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIZE = (28,28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset preprocessing\n",
    "- Get the dataset from folders\n",
    "- Associate the corresponding classes\n",
    "- Resized the dataset\n",
    "- Shuffle the dataset?\n",
    "- Divide the dataset into validation, training and testing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.astype(np.float32)\n",
    "x_test = x_test.astype(np.float32)\n",
    "# x_train -= 127.5\n",
    "# x_test -= 127.5\n",
    "# x_train *= 1./128\n",
    "# x_test *= 1./128\n",
    "x_train /= 255\n",
    "x_test /= 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### New method\n",
    "- We divide the validation and test set from the training set with the classic division method: 85 percent training, 10 validating, 5 testing.\n",
    "- We then computes pairs of images in the validation set and testing set:\n",
    " - Some of these pairs are images of the same dog and some are picture of different dogs\n",
    " - We create a two lists:\n",
    "  - A list a images containing the pairs: two successive images are a pair of images. For example, image 0 and is a pair, image 2 and 3 is another pair, etc...\n",
    "  - A list of boolean called 'issame' indicating if a pair is a pair of images of the same dog or a pair of different dogs. For example, if image 0 and image 1 are showing the same dog value 0 and 1 in the list will be True. On the other hand if the image 2 and 3 represent two different dogs the value 2 and 3 in the list will be at False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1f1fcb60240>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sk.io.imshow(x_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of validation images: 10000\n",
      "Number of training images: 60000\n",
      "Number of classes in the training set: 10\n",
      "Number of pairs: 10000\n",
      "Number of same images: 2559\n",
      "Number of validation images: 10000\n"
     ]
    }
   ],
   "source": [
    "w, h = SIZE\n",
    "\n",
    "images_valid = x_test\n",
    "labels_valid = y_test\n",
    "\n",
    "images_train = x_train\n",
    "labels_train = y_train\n",
    "\n",
    "print(\"Number of validation images: \" + str(len(labels_valid)))\n",
    "print(\"Number of training images: \" + str(len(labels_train)))\n",
    "print(\"Number of classes in the training set: \" + str(max(labels_train) - min(labels_train)+1))\n",
    "\n",
    "\n",
    "# Creates the pairs\n",
    "\n",
    "nbof_pairs = (len(images_valid)//2)*2 # it has to be multiple of 2\n",
    "\n",
    "print(\"Number of pairs: \" + str(nbof_pairs))\n",
    "\n",
    "pairs = np.empty((nbof_pairs,w,h))\n",
    "issame = np.empty(nbof_pairs, dtype=int)\n",
    "y_pairs = np.empty(nbof_pairs)\n",
    "\n",
    "nbof_same = 0\n",
    "\n",
    "for i in range(0,nbof_pairs,2):\n",
    "    ## alea_issame will decide if the new pair will be a pair of same dog images or a pair of different\n",
    "    alea_issame = np.random.rand()\n",
    "\n",
    "    if alea_issame < 0.5: # Then it will be a pair of same dogs\n",
    "        # we randomly choose a dog\n",
    "        choice = np.random.randint(len(labels_valid))\n",
    "        \n",
    "        # we extract the images of this class\n",
    "        chosen_images = list(images_valid[np.equal(labels_valid,labels_valid[choice])])\n",
    "        chosen_labels = list(labels_valid[np.equal(labels_valid,labels_valid[choice])])\n",
    "        \n",
    "        while len(labels_valid[np.equal(labels_valid,labels_valid[choice])]) < 2:\n",
    "            choice = np.random.randint(len(labels_valid))\n",
    "            chosen_images = list(images_valid[np.equal(labels_valid,labels_valid[choice])])\n",
    "            chosen_labels = list(labels_valid[np.equal(labels_valid,labels_valid[choice])])\n",
    "            \n",
    "        # we then randomly choose two pictures of this class\n",
    "        choice1 = np.random.randint(len(chosen_images))\n",
    "        pairs[i] = chosen_images[choice1]\n",
    "        y_pairs[i] = chosen_labels[choice1]\n",
    "        \n",
    "        save = np.copy(chosen_images)\n",
    "        chosen_images = chosen_images[:choice1] + chosen_images[choice1+1:]\n",
    "        if len(chosen_images) == 0:\n",
    "            print(\"Bug!\")\n",
    "            print(save)\n",
    "        choice2 = np.random.randint(len(chosen_images))\n",
    "        pairs[i+1] = chosen_images[choice2]\n",
    "        y_pairs[i+1] = chosen_labels[choice2]\n",
    "        \n",
    "        issame[i] = issame[i+1] = 1\n",
    "        \n",
    "        nbof_same += 1\n",
    "        \n",
    "    else: # Then it will be a pair of different dogs\n",
    "        # we randomly choose two dogs\n",
    "        choice1 = np.random.randint(len(labels_valid))\n",
    "        \n",
    "        # we extract the images of the class\n",
    "        chosen_images = list(images_valid[np.equal(labels_valid,labels_valid[choice1])])\n",
    "        \n",
    "        # we choose an image of this class\n",
    "        choice = np.random.randint(len(chosen_images))\n",
    "        #print(choice)\n",
    "        pairs[i] = images_valid[choice]\n",
    "        y_pairs[i] = labels_valid[choice]\n",
    "        \n",
    "        choice2 = np.random.randint(len(labels_valid))\n",
    "        \n",
    "        # check if we have two different classes\n",
    "        while labels_valid[choice2] == labels_valid[choice1]:\n",
    "            choice2 = np.random.randint(len(labels_valid))\n",
    "        \n",
    "        chosen_images = list(images_valid[np.equal(labels_valid,labels_valid[choice2])])\n",
    "        \n",
    "        # we choose an image of this class\n",
    "        choice = np.random.randint(len(chosen_images))\n",
    "        \n",
    "        pairs[i+1] = images_valid[choice]\n",
    "        y_pairs[i+1] = labels_valid[choice]\n",
    "        \n",
    "        issame[i] = issame[i+1] = 0\n",
    "\n",
    "print(\"Number of same images: \" + str(nbof_same))\n",
    "print(\"Number of validation images: \" + str(len(labels_valid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 0 0 0 0 0 0 1 1]\n",
      "[2. 2. 9. 1. 2. 4. 1. 9. 8. 8.]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x1080 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# check the pairs\n",
    "s = 0\n",
    "n = 5\n",
    "print(issame[2*s:(n+s)*2])\n",
    "print(y_pairs[2*s:(n+s)*2])\n",
    "fig = plt.figure(figsize=(5,3*n))\n",
    "for i in range(s,s+n):\n",
    "    plt.subplot(n,2,2*(i-s)+1)\n",
    "    plt.imshow(pairs[2*i])\n",
    "    plt.subplot(n,2,2*(i-s)+2)\n",
    "    plt.imshow(pairs[2*i+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28, 1)\n",
      "(10000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "x_train = np.expand_dims(x_train, -1)\n",
    "print(x_train.shape)\n",
    "pairs = np.expand_dims(pairs, -1)\n",
    "print(pairs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "y_train = tf.keras.utils.to_categorical(y_train)\n",
    "y_pairs = tf.keras.utils.to_categorical(y_pairs)\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "issame_train = np.ones(len(x_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train input shapes: \n",
      "(60000, 28, 28, 1)\n",
      "(60000,)\n",
      "(60000,)\n",
      "Train output shape: \n",
      "(60000,)\n",
      "(60000, 2)\n",
      "Valid input shape: \n",
      "(10000, 28, 28, 1)\n",
      "(10000,)\n",
      "(10000,)\n",
      "Valid output shape: \n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "# Verify shapes\n",
    "# Train inputs\n",
    "print(\"Train input shapes: \")\n",
    "print(images_train.shape)\n",
    "print(labels_train.shape)\n",
    "print(issame_train_in.shape)\n",
    "\n",
    "print(\"Train output shape: \")\n",
    "print(labels_train.shape)\n",
    "print(issame_train_out.shape)\n",
    "\n",
    "print(\"Valid input shape: \")\n",
    "print(pairs.shape)\n",
    "print(labels_valid.shape)\n",
    "print(issame_in.shape)\n",
    "\n",
    "print(\"Valid output shape: \")\n",
    "print(labels_valid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the model\n",
    "- Define the ArcFace layer\n",
    "- Define the dummy model first\n",
    "- Compile it with the softmax loss and and Adam optimizer\n",
    "- Then use transfer learning with a more complex model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the Arcface layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Layer\n",
    "import math\n",
    "\n",
    "# Arcface should only be used for training\n",
    "class Arcface(Layer):\n",
    "\n",
    "    def __init__(self, out_num, s = 64., m = 0.5, **kwargs):\n",
    "        self.out_num = out_num\n",
    "        self.s = s\n",
    "        self.m = m\n",
    "        super(Arcface, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape, initializer='uniform'):\n",
    "        assert isinstance(input_shape, list)\n",
    "        \n",
    "        shape = tf.TensorShape((input_shape[0][-1],self.out_num))\n",
    "        print(shape)\n",
    "        \n",
    "        # Create a trainable weight variable for this layer.\n",
    "        self.kernel = self.add_weight(name='kernel',\n",
    "                                                 shape=shape,\n",
    "                                                 initializer=initializer,\n",
    "                                                 dtype=tf.float32,\n",
    "                                                 trainable=True)\n",
    "        super(Arcface, self).build(input_shape)  # Be sure to call this at the end\n",
    "\n",
    "    def call(self, x, training=None):\n",
    "        assert isinstance(x, list)\n",
    "        emb, labels = x\n",
    "        #labels_sq = tf.squeeze(labels,1)\n",
    "        #labels_sq = tf.reshape(labels,[None])\n",
    "        #labels_int = tf.cast(labels_sq,tf.int32, name='labels_int')\n",
    "        #print(labels_int.shape)\n",
    "        #mask = tf.one_hot(labels_int, depth=self.out_num, name='one_hot_mask')\n",
    "        mask = labels\n",
    "        #mask = tf.squeeze(mask,1)\n",
    "        #print(mask.shape)\n",
    "        #mask_shape = mask.shape.as_list()\n",
    "        #mask = tf.reshape(mask, (None,mask_shape[-1]))\n",
    "        #print(mask.shape)\n",
    "        def train_output():\n",
    "            cos_m = math.cos(self.m)\n",
    "            sin_m = math.sin(self.m)\n",
    "            mm = sin_m * self.m  # issue 1\n",
    "            threshold = math.cos(math.pi - self.m)\n",
    "\n",
    "            # inputs and weights norm\n",
    "            embedding_norm = tf.norm(emb, axis=1, keepdims=True)\n",
    "            embedding = tf.div(emb, embedding_norm, name='norm_embedding')\n",
    "\n",
    "            weights_norm = tf.norm(self.kernel, axis=0, keepdims=True)\n",
    "            weights = tf.div(self.kernel, weights_norm, name='norm_weights')\n",
    "            # cos(theta+m)\n",
    "            cos_t = tf.matmul(embedding, weights, name='cos_t')\n",
    "            print(cos_t.shape)\n",
    "            cos_t2 = tf.square(cos_t, name='cos_2')\n",
    "            sin_t2 = tf.subtract(1., cos_t2, name='sin_2')\n",
    "            sin_t = tf.sqrt(sin_t2, name='sin_t')\n",
    "            cos_mt = self.s * tf.subtract(tf.multiply(cos_t, cos_m), tf.multiply(sin_t, sin_m), name='cos_mt')\n",
    "\n",
    "            # this condition controls the theta+m should be in range [0, pi]\n",
    "            #      0<=theta+m<=pi\n",
    "            #     -m<=theta<=pi-m\n",
    "            cond_v = cos_t - threshold\n",
    "            cond = tf.cast(tf.nn.relu(cond_v, name='if_else'), dtype=tf.bool)\n",
    "\n",
    "            keep_val = self.s*(cos_t - mm)\n",
    "            cos_mt_temp = tf.where(cond, cos_mt, keep_val)\n",
    "\n",
    "            \n",
    "            # mask = tf.squeeze(mask, 1)\n",
    "            inv_mask = tf.subtract(1., mask, name='inverse_mask')\n",
    "\n",
    "            s_cos_t = tf.multiply(self.s, cos_t, name='scalar_cos_t')\n",
    "            mul1 = tf.multiply(s_cos_t, inv_mask)\n",
    "            print(mul1.shape)\n",
    "            mul2 = tf.multiply(cos_mt_temp, mask)\n",
    "            print(mul2.shape)\n",
    "            output = tf.add(mul1, mul2, name='arcface_loss_output')\n",
    "            print(output.shape)\n",
    "            print(cos_mt_temp.shape)\n",
    "            \n",
    "            return output\n",
    "        \n",
    "        def valid_output():\n",
    "            return mask\n",
    "        \n",
    "        return K.in_train_phase(train_output,valid_output,training=training)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        assert isinstance(input_shape, list)\n",
    "        shape_emb, shape_lab = input_shape\n",
    "        shape_emb[-1] = self.out_num\n",
    "        return tf.TensorShape(shape_emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the validation layer:\n",
    "- The bigger the validation batch the better it is (no less than 64 pictures -> 32 pairs)\n",
    "- It computes the ROC curve\n",
    "- Finds the best threshold\n",
    "- Returns a list of 2D vectors [1,0] if the pair was the same dog, [0,1] if it was a different dog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Layer\n",
    "\n",
    "# Should only be used for validating\n",
    "class Validation(Layer):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Validation, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert isinstance(input_shape, list)\n",
    "        self.emb_shape = input_shape[0]\n",
    "        super(Validation, self).build(input_shape)\n",
    "\n",
    "    def call(self, x, training=None):\n",
    "        \"\"\"\n",
    "        Inputs: a tuple containing the embeddings and the issame list\n",
    "        - embeddings: shape=(batch_size, embedding_size), type=float\n",
    "        - issame: shape=(batch_size), type=bool\n",
    "        \n",
    "        Outputs: a tensor of shape=(batch_size,2), the ouput is either [1,0] (is same) or [0,1] (is different)\n",
    "        \"\"\"\n",
    "        assert isinstance(x, list)\n",
    "        \n",
    "        embeddings, iss = x\n",
    "        \n",
    "        \n",
    "        \n",
    "        def train_output():\n",
    "            return iss\n",
    "        \n",
    "        def valid_output():\n",
    "            issame = tf.squeeze(iss)\n",
    "            #self.emb_shape = embeddings.shape\n",
    "            emb = tf.math.l2_normalize(embeddings,0)\n",
    "            # emb contains a list of pictures\n",
    "            # pictures with an even index are first pictures of the pairs\n",
    "            # pictures with an odd index are second pictures of the pairs\n",
    "            emb1 = embeddings[0::2]\n",
    "            emb2 = embeddings[1::2]\n",
    "            #emb1, emb2 = tf.split(embeddings, [32,32],0)\n",
    "            \n",
    "          # Compute the distance for each pair of vector\n",
    "            dist = tf.reduce_sum(tf.squared_difference(emb1,emb2),1)\n",
    "            dist = tf.reshape(tf.stack([dist,dist], axis=-1), [-1])\n",
    "            actual_issame_bool = tf.cast(issame,dtype=tf.bool)\n",
    "\n",
    "            def fn(t):\n",
    "                less = tf.less(dist,t)\n",
    "                acc = tf.logical_not(tf.logical_xor(less,actual_issame_bool))\n",
    "                acc = tf.cast(acc,tf.float32)\n",
    "                out = tf.reshape(tf.reduce_sum(acc),[])\n",
    "                return out\n",
    "\n",
    "\n",
    "            thresholds = tf.range(0,1,0.001)\n",
    "            apply_t = tf.map_fn(fn, thresholds)\n",
    "            best_t = tf.argmax(apply_t)\n",
    "\n",
    "            best = thresholds[best_t]\n",
    "\n",
    "          # Redo the manipulation with the best threshold\n",
    "            less = tf.less(dist,best)\n",
    "            less = tf.cast(less,tf.float32)\n",
    "            less = tf.expand_dims(less,1) # <- bug fixed\n",
    "            return less\n",
    "\n",
    "            \n",
    "        return K.in_train_phase(train_output,valid_output,training=training)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        assert isinstance(input_shape, list)\n",
    "        emb_shape, _ = input_shape\n",
    "        return (emb_shape[0], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Layer\n",
    "\n",
    "# Should only be used for validating\n",
    "class Validation(Layer):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Validation, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert isinstance(input_shape, list)\n",
    "        super(Validation, self).build(input_shape)\n",
    "\n",
    "    def call(self, x, training=None):\n",
    "        \"\"\"\n",
    "        Inputs: a tuple containing the embeddings and the issame list\n",
    "        - embeddings: shape=(batch_size, embedding_size), type=float\n",
    "        - issame: shape=(batch_size), type=bool\n",
    "        \n",
    "        Outputs: a tensor of shape=(batch_size,2), the ouput is either [1,0] (is same) or [0,1] (is different)\n",
    "        \"\"\"\n",
    "        assert isinstance(x, list)\n",
    "        \n",
    "        embeddings, iss = x\n",
    "        \n",
    "        \n",
    "        \n",
    "        def train_output():\n",
    "            return iss\n",
    "        \n",
    "        def valid_output():\n",
    "            emb = tf.math.l2_normalize(embeddings,0)\n",
    "            # emb contains a list of pictures\n",
    "            # pictures with an even index are first pictures of the pairs\n",
    "            # pictures with an odd index are second pictures of the pairs\n",
    "            emb1 = embeddings[0::2]\n",
    "            emb2 = embeddings[1::2]\n",
    "            \n",
    "          # Compute the distance for each pair of vector\n",
    "            dist = tf.reduce_sum(tf.squared_difference(emb1,emb2),1)\n",
    "            dist = tf.reshape(tf.stack([dist,dist], axis=-1), [-1])\n",
    "            less = tf.less(dist,0.01)\n",
    "            less = tf.cast(less,tf.float32)\n",
    "            less = tf.expand_dims(less,1) # <- bug fixed\n",
    "            return less\n",
    "\n",
    "            \n",
    "        return K.in_train_phase(train_output,valid_output,training=training)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        assert isinstance(input_shape, list)\n",
    "        emb_shape, _ = input_shape\n",
    "        emb_shape[-1] = 1\n",
    "        return emb_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Try to use the functional API\n",
    "#tf.reset_default_graph()\n",
    "def net(inputs_shapes, emb_size=4):\n",
    "    images_shape, labels_shape = inputs_shapes\n",
    "    input_image = tf.keras.Input(images_shape,name='image_input')\n",
    "    x = tf.keras.layers.Conv2D(32, (3,3), activation='relu')(input_image)\n",
    "    x = tf.keras.layers.Conv2D(64, (3,3), activation='relu')(x)\n",
    "    x = tf.keras.layers.MaxPooling2D((2,2))(x)\n",
    "    x = tf.keras.layers.Dropout(0.25)(x)\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dropout(0.5)(x)\n",
    "    emb = tf.keras.layers.Dense(20, activity_regularizer='l2')(x)\n",
    "    out = tf.keras.layers.Dense(10, activation='softmax', name='output')(emb)\n",
    "    \n",
    "    #emb = tf.keras.layers.Dense(emb_size, activity_regularizer='l2')(x)\n",
    "    #out = tf.keras.layers.Dense(1301, name='arcface')(emb)\n",
    "    \n",
    "#     input_labels = tf.keras.Input(labels_shape,name='input_labels')\n",
    "#     out = Arcface(10)([emb,input_labels])\n",
    "#     out = tf.keras.layers.Activation('softmax', name='output')(out)\n",
    "                                         \n",
    "#     input_issame = tf.keras.Input(issame_shape,name='issame_input')\n",
    "#     valid = Validation(name='validation')([emb, input_issame])\n",
    "    \n",
    "    #model = tf.keras.Model(inputs=[input_image,input_labels,input_issame], outputs=[out,valid])\n",
    "    #model = tf.keras.Model(inputs=[input_image,input_labels], outputs=out)\n",
    "    model = tf.keras.Model(inputs=[input_image], outputs=out)\n",
    "    return model\n",
    "\n",
    "w, h = SIZE\n",
    "inputs_shapes = [(w, h, 1,),(10,)]\n",
    "model = net(inputs_shapes)\n",
    "model.compile(tf.keras.optimizers.Adam(),loss={'output':'categorical_crossentropy'},metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54400/60000 [==========================>...] - ETA: 10:56 - loss: 2.6843 - acc: 0.03 - ETA: 3:46 - loss: 2.5084 - acc: 0.0938 - ETA: 2:21 - loss: 2.4260 - acc: 0.120 - ETA: 1:44 - loss: 2.3883 - acc: 0.135 - ETA: 1:23 - loss: 2.3676 - acc: 0.144 - ETA: 1:10 - loss: 2.3533 - acc: 0.154 - ETA: 1:01 - loss: 2.3435 - acc: 0.163 - ETA: 54s - loss: 2.3362 - acc: 0.165 - ETA: 49s - loss: 2.3300 - acc: 0.17 - ETA: 45s - loss: 2.3248 - acc: 0.19 - ETA: 42s - loss: 2.3202 - acc: 0.20 - ETA: 39s - loss: 2.3168 - acc: 0.20 - ETA: 37s - loss: 2.3134 - acc: 0.21 - ETA: 35s - loss: 2.3107 - acc: 0.22 - ETA: 33s - loss: 2.3077 - acc: 0.22 - ETA: 31s - loss: 2.3052 - acc: 0.23 - ETA: 30s - loss: 2.3025 - acc: 0.23 - ETA: 29s - loss: 2.3001 - acc: 0.24 - ETA: 28s - loss: 2.2980 - acc: 0.25 - ETA: 27s - loss: 2.2959 - acc: 0.25 - ETA: 26s - loss: 2.2942 - acc: 0.25 - ETA: 25s - loss: 2.2921 - acc: 0.26 - ETA: 25s - loss: 2.2902 - acc: 0.26 - ETA: 24s - loss: 2.2882 - acc: 0.27 - ETA: 23s - loss: 2.2858 - acc: 0.27 - ETA: 23s - loss: 2.2839 - acc: 0.28 - ETA: 22s - loss: 2.2817 - acc: 0.28 - ETA: 22s - loss: 2.2798 - acc: 0.29 - ETA: 21s - loss: 2.2777 - acc: 0.30 - ETA: 21s - loss: 2.2763 - acc: 0.30 - ETA: 20s - loss: 2.2743 - acc: 0.31 - ETA: 20s - loss: 2.2723 - acc: 0.31 - ETA: 20s - loss: 2.2704 - acc: 0.32 - ETA: 19s - loss: 2.2686 - acc: 0.32 - ETA: 19s - loss: 2.2668 - acc: 0.33 - ETA: 19s - loss: 2.2652 - acc: 0.33 - ETA: 18s - loss: 2.2637 - acc: 0.34 - ETA: 18s - loss: 2.2619 - acc: 0.34 - ETA: 18s - loss: 2.2600 - acc: 0.35 - ETA: 17s - loss: 2.2582 - acc: 0.35 - ETA: 17s - loss: 2.2564 - acc: 0.36 - ETA: 17s - loss: 2.2547 - acc: 0.36 - ETA: 17s - loss: 2.2528 - acc: 0.37 - ETA: 17s - loss: 2.2512 - acc: 0.38 - ETA: 16s - loss: 2.2496 - acc: 0.38 - ETA: 16s - loss: 2.2479 - acc: 0.38 - ETA: 16s - loss: 2.2460 - acc: 0.39 - ETA: 16s - loss: 2.2446 - acc: 0.39 - ETA: 15s - loss: 2.2429 - acc: 0.40 - ETA: 15s - loss: 2.2412 - acc: 0.40 - ETA: 15s - loss: 2.2394 - acc: 0.41 - ETA: 15s - loss: 2.2376 - acc: 0.41 - ETA: 15s - loss: 2.2361 - acc: 0.42 - ETA: 15s - loss: 2.2343 - acc: 0.42 - ETA: 14s - loss: 2.2326 - acc: 0.42 - ETA: 14s - loss: 2.2311 - acc: 0.43 - ETA: 14s - loss: 2.2296 - acc: 0.43 - ETA: 14s - loss: 2.2281 - acc: 0.43 - ETA: 14s - loss: 2.2265 - acc: 0.44 - ETA: 14s - loss: 2.2249 - acc: 0.44 - ETA: 13s - loss: 2.2233 - acc: 0.44 - ETA: 13s - loss: 2.2217 - acc: 0.45 - ETA: 13s - loss: 2.2200 - acc: 0.45 - ETA: 13s - loss: 2.2186 - acc: 0.45 - ETA: 13s - loss: 2.2171 - acc: 0.45 - ETA: 13s - loss: 2.2154 - acc: 0.46 - ETA: 13s - loss: 2.2137 - acc: 0.46 - ETA: 12s - loss: 2.2121 - acc: 0.46 - ETA: 12s - loss: 2.2104 - acc: 0.47 - ETA: 12s - loss: 2.2087 - acc: 0.47 - ETA: 12s - loss: 2.2072 - acc: 0.47 - ETA: 12s - loss: 2.2056 - acc: 0.47 - ETA: 12s - loss: 2.2039 - acc: 0.48 - ETA: 12s - loss: 2.2024 - acc: 0.48 - ETA: 12s - loss: 2.2007 - acc: 0.48 - ETA: 12s - loss: 2.1992 - acc: 0.49 - ETA: 11s - loss: 2.1975 - acc: 0.49 - ETA: 11s - loss: 2.1958 - acc: 0.49 - ETA: 11s - loss: 2.1943 - acc: 0.49 - ETA: 11s - loss: 2.1926 - acc: 0.50 - ETA: 11s - loss: 2.1911 - acc: 0.50 - ETA: 11s - loss: 2.1893 - acc: 0.50 - ETA: 11s - loss: 2.1879 - acc: 0.50 - ETA: 11s - loss: 2.1863 - acc: 0.51 - ETA: 11s - loss: 2.1847 - acc: 0.51 - ETA: 10s - loss: 2.1829 - acc: 0.51 - ETA: 10s - loss: 2.1814 - acc: 0.51 - ETA: 10s - loss: 2.1797 - acc: 0.52 - ETA: 10s - loss: 2.1783 - acc: 0.52 - ETA: 10s - loss: 2.1768 - acc: 0.52 - ETA: 10s - loss: 2.1750 - acc: 0.52 - ETA: 10s - loss: 2.1734 - acc: 0.53 - ETA: 10s - loss: 2.1718 - acc: 0.53 - ETA: 10s - loss: 2.1700 - acc: 0.53 - ETA: 10s - loss: 2.1684 - acc: 0.53 - ETA: 10s - loss: 2.1668 - acc: 0.53 - ETA: 9s - loss: 2.1649 - acc: 0.5402 - ETA: 9s - loss: 2.1633 - acc: 0.542 - ETA: 9s - loss: 2.1618 - acc: 0.544 - ETA: 9s - loss: 2.1604 - acc: 0.546 - ETA: 9s - loss: 2.1587 - acc: 0.548 - ETA: 9s - loss: 2.1570 - acc: 0.550 - ETA: 9s - loss: 2.1554 - acc: 0.552 - ETA: 9s - loss: 2.1539 - acc: 0.553 - ETA: 9s - loss: 2.1524 - acc: 0.555 - ETA: 9s - loss: 2.1509 - acc: 0.557 - ETA: 9s - loss: 2.1493 - acc: 0.558 - ETA: 8s - loss: 2.1476 - acc: 0.560 - ETA: 8s - loss: 2.1459 - acc: 0.562 - ETA: 8s - loss: 2.1445 - acc: 0.563 - ETA: 8s - loss: 2.1428 - acc: 0.565 - ETA: 8s - loss: 2.1414 - acc: 0.567 - ETA: 8s - loss: 2.1398 - acc: 0.568 - ETA: 8s - loss: 2.1383 - acc: 0.570 - ETA: 8s - loss: 2.1367 - acc: 0.572 - ETA: 8s - loss: 2.1350 - acc: 0.574 - ETA: 8s - loss: 2.1333 - acc: 0.575 - ETA: 8s - loss: 2.1316 - acc: 0.577 - ETA: 8s - loss: 2.1301 - acc: 0.579 - ETA: 7s - loss: 2.1286 - acc: 0.580 - ETA: 7s - loss: 2.1270 - acc: 0.582 - ETA: 7s - loss: 2.1256 - acc: 0.582 - ETA: 7s - loss: 2.1241 - acc: 0.584 - ETA: 7s - loss: 2.1225 - acc: 0.586 - ETA: 7s - loss: 2.1211 - acc: 0.587 - ETA: 7s - loss: 2.1195 - acc: 0.589 - ETA: 7s - loss: 2.1179 - acc: 0.590 - ETA: 7s - loss: 2.1164 - acc: 0.592 - ETA: 7s - loss: 2.1149 - acc: 0.593 - ETA: 7s - loss: 2.1133 - acc: 0.595 - ETA: 7s - loss: 2.1119 - acc: 0.596 - ETA: 7s - loss: 2.1102 - acc: 0.598 - ETA: 6s - loss: 2.1087 - acc: 0.599 - ETA: 6s - loss: 2.1072 - acc: 0.600 - ETA: 6s - loss: 2.1057 - acc: 0.601 - ETA: 6s - loss: 2.1041 - acc: 0.602 - ETA: 6s - loss: 2.1026 - acc: 0.603 - ETA: 6s - loss: 2.1011 - acc: 0.604 - ETA: 6s - loss: 2.0996 - acc: 0.605 - ETA: 6s - loss: 2.0981 - acc: 0.607 - ETA: 6s - loss: 2.0965 - acc: 0.608 - ETA: 6s - loss: 2.0952 - acc: 0.609 - ETA: 6s - loss: 2.0937 - acc: 0.610 - ETA: 6s - loss: 2.0921 - acc: 0.611 - ETA: 6s - loss: 2.0906 - acc: 0.612 - ETA: 5s - loss: 2.0889 - acc: 0.614 - ETA: 5s - loss: 2.0873 - acc: 0.615 - ETA: 5s - loss: 2.0857 - acc: 0.616 - ETA: 5s - loss: 2.0843 - acc: 0.617 - ETA: 5s - loss: 2.0828 - acc: 0.618 - ETA: 5s - loss: 2.0813 - acc: 0.619 - ETA: 5s - loss: 2.0799 - acc: 0.620 - ETA: 5s - loss: 2.0784 - acc: 0.621 - ETA: 5s - loss: 2.0769 - acc: 0.622 - ETA: 5s - loss: 2.0753 - acc: 0.623 - ETA: 5s - loss: 2.0740 - acc: 0.624 - ETA: 5s - loss: 2.0724 - acc: 0.625 - ETA: 5s - loss: 2.0708 - acc: 0.626 - ETA: 5s - loss: 2.0693 - acc: 0.627 - ETA: 5s - loss: 2.0677 - acc: 0.628 - ETA: 4s - loss: 2.0662 - acc: 0.629 - ETA: 4s - loss: 2.0647 - acc: 0.630 - ETA: 4s - loss: 2.0630 - acc: 0.632 - ETA: 4s - loss: 2.0616 - acc: 0.633 - ETA: 4s - loss: 2.0603 - acc: 0.633 - ETA: 4s - loss: 2.0589 - acc: 0.634 - ETA: 4s - loss: 2.0573 - acc: 0.635 - ETA: 4s - loss: 2.0559 - acc: 0.636 - ETA: 4s - loss: 2.0542 - acc: 0.637 - ETA: 4s - loss: 2.0527 - acc: 0.638 - ETA: 4s - loss: 2.0511 - acc: 0.639 - ETA: 4s - loss: 2.0498 - acc: 0.640 - ETA: 4s - loss: 2.0483 - acc: 0.641 - ETA: 4s - loss: 2.0468 - acc: 0.642 - ETA: 3s - loss: 2.0452 - acc: 0.643 - ETA: 3s - loss: 2.0437 - acc: 0.643 - ETA: 3s - loss: 2.0422 - acc: 0.644 - ETA: 3s - loss: 2.0407 - acc: 0.645 - ETA: 3s - loss: 2.0392 - acc: 0.646 - ETA: 3s - loss: 2.0378 - acc: 0.647 - ETA: 3s - loss: 2.0363 - acc: 0.648 - ETA: 3s - loss: 2.0347 - acc: 0.649 - ETA: 3s - loss: 2.0334 - acc: 0.649 - ETA: 3s - loss: 2.0317 - acc: 0.650 - ETA: 3s - loss: 2.0303 - acc: 0.651 - ETA: 3s - loss: 2.0287 - acc: 0.652 - ETA: 3s - loss: 2.0271 - acc: 0.653 - ETA: 3s - loss: 2.0256 - acc: 0.653 - ETA: 3s - loss: 2.0242 - acc: 0.654 - ETA: 2s - loss: 2.0228 - acc: 0.655 - ETA: 2s - loss: 2.0214 - acc: 0.655 - ETA: 2s - loss: 2.0196 - acc: 0.656 - ETA: 2s - loss: 2.0184 - acc: 0.657 - ETA: 2s - loss: 2.0170 - acc: 0.657 - ETA: 2s - loss: 2.0155 - acc: 0.658 - ETA: 2s - loss: 2.0141 - acc: 0.659 - ETA: 2s - loss: 2.0127 - acc: 0.659 - ETA: 2s - loss: 2.0112 - acc: 0.660 - ETA: 2s - loss: 2.0097 - acc: 0.661 - ETA: 2s - loss: 2.0082 - acc: 0.661 - ETA: 2s - loss: 2.0067 - acc: 0.662 - ETA: 2s - loss: 2.0053 - acc: 0.663 - ETA: 2s - loss: 2.0039 - acc: 0.664 - ETA: 2s - loss: 2.0027 - acc: 0.664 - ETA: 1s - loss: 2.0011 - acc: 0.665 - ETA: 1s - loss: 1.9996 - acc: 0.665 - ETA: 1s - loss: 1.9981 - acc: 0.666 - ETA: 1s - loss: 1.9966 - acc: 0.667 - ETA: 1s - loss: 1.9952 - acc: 0.667 - ETA: 1s - loss: 1.9936 - acc: 0.668 - ETA: 1s - loss: 1.9923 - acc: 0.669 - ETA: 1s - loss: 1.9911 - acc: 0.669 - ETA: 1s - loss: 1.9897 - acc: 0.66960000/60000 [==============================] - ETA: 1s - loss: 1.9882 - acc: 0.670 - ETA: 1s - loss: 1.9868 - acc: 0.671 - ETA: 1s - loss: 1.9853 - acc: 0.671 - ETA: 1s - loss: 1.9839 - acc: 0.672 - ETA: 1s - loss: 1.9824 - acc: 0.673 - ETA: 1s - loss: 1.9810 - acc: 0.674 - ETA: 0s - loss: 1.9796 - acc: 0.674 - ETA: 0s - loss: 1.9781 - acc: 0.675 - ETA: 0s - loss: 1.9767 - acc: 0.675 - ETA: 0s - loss: 1.9754 - acc: 0.676 - ETA: 0s - loss: 1.9740 - acc: 0.676 - ETA: 0s - loss: 1.9726 - acc: 0.677 - ETA: 0s - loss: 1.9713 - acc: 0.677 - ETA: 0s - loss: 1.9700 - acc: 0.678 - ETA: 0s - loss: 1.9687 - acc: 0.679 - ETA: 0s - loss: 1.9673 - acc: 0.679 - ETA: 0s - loss: 1.9661 - acc: 0.679 - ETA: 0s - loss: 1.9648 - acc: 0.679 - ETA: 0s - loss: 1.9633 - acc: 0.680 - ETA: 0s - loss: 1.9619 - acc: 0.680 - ETA: 0s - loss: 1.9605 - acc: 0.681 - 16s 265us/step - loss: 1.9592 - acc: 0.6820 - val_loss: 1.4557 - val_acc: 0.9774\n",
      "Epoch 2/12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54912/60000 [==========================>...] - ETA: 14s - loss: 1.6768 - acc: 0.78 - ETA: 13s - loss: 1.6380 - acc: 0.82 - ETA: 13s - loss: 1.6276 - acc: 0.83 - ETA: 13s - loss: 1.6214 - acc: 0.83 - ETA: 13s - loss: 1.6180 - acc: 0.82 - ETA: 12s - loss: 1.6181 - acc: 0.81 - ETA: 12s - loss: 1.6166 - acc: 0.82 - ETA: 12s - loss: 1.6171 - acc: 0.81 - ETA: 12s - loss: 1.6179 - acc: 0.81 - ETA: 12s - loss: 1.6137 - acc: 0.81 - ETA: 12s - loss: 1.6150 - acc: 0.81 - ETA: 12s - loss: 1.6143 - acc: 0.81 - ETA: 12s - loss: 1.6157 - acc: 0.81 - ETA: 12s - loss: 1.6141 - acc: 0.81 - ETA: 12s - loss: 1.6109 - acc: 0.81 - ETA: 12s - loss: 1.6093 - acc: 0.81 - ETA: 12s - loss: 1.6093 - acc: 0.81 - ETA: 12s - loss: 1.6055 - acc: 0.81 - ETA: 12s - loss: 1.6048 - acc: 0.82 - ETA: 12s - loss: 1.6026 - acc: 0.82 - ETA: 12s - loss: 1.6023 - acc: 0.82 - ETA: 12s - loss: 1.6030 - acc: 0.81 - ETA: 12s - loss: 1.6017 - acc: 0.81 - ETA: 11s - loss: 1.6007 - acc: 0.81 - ETA: 11s - loss: 1.5992 - acc: 0.82 - ETA: 11s - loss: 1.5975 - acc: 0.82 - ETA: 11s - loss: 1.5964 - acc: 0.82 - ETA: 11s - loss: 1.5944 - acc: 0.82 - ETA: 11s - loss: 1.5941 - acc: 0.82 - ETA: 11s - loss: 1.5939 - acc: 0.82 - ETA: 11s - loss: 1.5932 - acc: 0.82 - ETA: 11s - loss: 1.5916 - acc: 0.82 - ETA: 11s - loss: 1.5921 - acc: 0.82 - ETA: 11s - loss: 1.5914 - acc: 0.82 - ETA: 11s - loss: 1.5901 - acc: 0.82 - ETA: 11s - loss: 1.5892 - acc: 0.82 - ETA: 11s - loss: 1.5879 - acc: 0.82 - ETA: 11s - loss: 1.5874 - acc: 0.82 - ETA: 11s - loss: 1.5865 - acc: 0.82 - ETA: 11s - loss: 1.5855 - acc: 0.82 - ETA: 10s - loss: 1.5838 - acc: 0.82 - ETA: 10s - loss: 1.5824 - acc: 0.82 - ETA: 10s - loss: 1.5806 - acc: 0.82 - ETA: 10s - loss: 1.5792 - acc: 0.82 - ETA: 10s - loss: 1.5784 - acc: 0.82 - ETA: 10s - loss: 1.5771 - acc: 0.82 - ETA: 10s - loss: 1.5764 - acc: 0.82 - ETA: 10s - loss: 1.5752 - acc: 0.82 - ETA: 10s - loss: 1.5747 - acc: 0.82 - ETA: 10s - loss: 1.5735 - acc: 0.82 - ETA: 10s - loss: 1.5720 - acc: 0.82 - ETA: 10s - loss: 1.5721 - acc: 0.82 - ETA: 10s - loss: 1.5714 - acc: 0.82 - ETA: 10s - loss: 1.5704 - acc: 0.82 - ETA: 10s - loss: 1.5698 - acc: 0.82 - ETA: 10s - loss: 1.5690 - acc: 0.82 - ETA: 10s - loss: 1.5679 - acc: 0.82 - ETA: 10s - loss: 1.5674 - acc: 0.82 - ETA: 9s - loss: 1.5671 - acc: 0.8219 - ETA: 9s - loss: 1.5665 - acc: 0.821 - ETA: 9s - loss: 1.5655 - acc: 0.821 - ETA: 9s - loss: 1.5639 - acc: 0.822 - ETA: 9s - loss: 1.5631 - acc: 0.822 - ETA: 9s - loss: 1.5620 - acc: 0.823 - ETA: 9s - loss: 1.5612 - acc: 0.823 - ETA: 9s - loss: 1.5598 - acc: 0.823 - ETA: 9s - loss: 1.5579 - acc: 0.824 - ETA: 9s - loss: 1.5563 - acc: 0.824 - ETA: 9s - loss: 1.5551 - acc: 0.825 - ETA: 9s - loss: 1.5540 - acc: 0.825 - ETA: 9s - loss: 1.5532 - acc: 0.825 - ETA: 9s - loss: 1.5520 - acc: 0.826 - ETA: 9s - loss: 1.5508 - acc: 0.826 - ETA: 9s - loss: 1.5496 - acc: 0.826 - ETA: 9s - loss: 1.5493 - acc: 0.826 - ETA: 9s - loss: 1.5484 - acc: 0.826 - ETA: 8s - loss: 1.5473 - acc: 0.826 - ETA: 8s - loss: 1.5463 - acc: 0.826 - ETA: 8s - loss: 1.5455 - acc: 0.826 - ETA: 8s - loss: 1.5446 - acc: 0.826 - ETA: 8s - loss: 1.5441 - acc: 0.827 - ETA: 8s - loss: 1.5432 - acc: 0.826 - ETA: 8s - loss: 1.5425 - acc: 0.826 - ETA: 8s - loss: 1.5413 - acc: 0.826 - ETA: 8s - loss: 1.5399 - acc: 0.826 - ETA: 8s - loss: 1.5386 - acc: 0.827 - ETA: 8s - loss: 1.5374 - acc: 0.827 - ETA: 8s - loss: 1.5367 - acc: 0.827 - ETA: 8s - loss: 1.5354 - acc: 0.827 - ETA: 8s - loss: 1.5342 - acc: 0.828 - ETA: 8s - loss: 1.5336 - acc: 0.827 - ETA: 8s - loss: 1.5324 - acc: 0.828 - ETA: 8s - loss: 1.5315 - acc: 0.828 - ETA: 8s - loss: 1.5306 - acc: 0.828 - ETA: 8s - loss: 1.5297 - acc: 0.828 - ETA: 7s - loss: 1.5286 - acc: 0.828 - ETA: 7s - loss: 1.5279 - acc: 0.828 - ETA: 7s - loss: 1.5274 - acc: 0.828 - ETA: 7s - loss: 1.5265 - acc: 0.829 - ETA: 7s - loss: 1.5254 - acc: 0.829 - ETA: 7s - loss: 1.5245 - acc: 0.829 - ETA: 7s - loss: 1.5234 - acc: 0.829 - ETA: 7s - loss: 1.5222 - acc: 0.830 - ETA: 7s - loss: 1.5209 - acc: 0.830 - ETA: 7s - loss: 1.5201 - acc: 0.830 - ETA: 7s - loss: 1.5188 - acc: 0.831 - ETA: 7s - loss: 1.5178 - acc: 0.831 - ETA: 7s - loss: 1.5168 - acc: 0.831 - ETA: 7s - loss: 1.5154 - acc: 0.831 - ETA: 7s - loss: 1.5143 - acc: 0.832 - ETA: 7s - loss: 1.5131 - acc: 0.832 - ETA: 7s - loss: 1.5124 - acc: 0.832 - ETA: 7s - loss: 1.5113 - acc: 0.832 - ETA: 6s - loss: 1.5104 - acc: 0.832 - ETA: 6s - loss: 1.5095 - acc: 0.832 - ETA: 6s - loss: 1.5090 - acc: 0.832 - ETA: 6s - loss: 1.5080 - acc: 0.832 - ETA: 6s - loss: 1.5071 - acc: 0.832 - ETA: 6s - loss: 1.5065 - acc: 0.832 - ETA: 6s - loss: 1.5053 - acc: 0.833 - ETA: 6s - loss: 1.5048 - acc: 0.832 - ETA: 6s - loss: 1.5038 - acc: 0.832 - ETA: 6s - loss: 1.5030 - acc: 0.832 - ETA: 6s - loss: 1.5021 - acc: 0.832 - ETA: 6s - loss: 1.5012 - acc: 0.833 - ETA: 6s - loss: 1.5007 - acc: 0.832 - ETA: 6s - loss: 1.4995 - acc: 0.833 - ETA: 6s - loss: 1.4986 - acc: 0.833 - ETA: 6s - loss: 1.4975 - acc: 0.833 - ETA: 6s - loss: 1.4968 - acc: 0.833 - ETA: 5s - loss: 1.4962 - acc: 0.833 - ETA: 5s - loss: 1.4952 - acc: 0.834 - ETA: 5s - loss: 1.4944 - acc: 0.834 - ETA: 5s - loss: 1.4934 - acc: 0.834 - ETA: 5s - loss: 1.4929 - acc: 0.834 - ETA: 5s - loss: 1.4926 - acc: 0.833 - ETA: 5s - loss: 1.4918 - acc: 0.834 - ETA: 5s - loss: 1.4910 - acc: 0.834 - ETA: 5s - loss: 1.4902 - acc: 0.834 - ETA: 5s - loss: 1.4894 - acc: 0.834 - ETA: 5s - loss: 1.4883 - acc: 0.834 - ETA: 5s - loss: 1.4870 - acc: 0.835 - ETA: 5s - loss: 1.4860 - acc: 0.835 - ETA: 5s - loss: 1.4854 - acc: 0.835 - ETA: 5s - loss: 1.4842 - acc: 0.835 - ETA: 5s - loss: 1.4834 - acc: 0.835 - ETA: 5s - loss: 1.4825 - acc: 0.835 - ETA: 5s - loss: 1.4816 - acc: 0.835 - ETA: 4s - loss: 1.4807 - acc: 0.836 - ETA: 4s - loss: 1.4799 - acc: 0.836 - ETA: 4s - loss: 1.4791 - acc: 0.836 - ETA: 4s - loss: 1.4785 - acc: 0.836 - ETA: 4s - loss: 1.4779 - acc: 0.836 - ETA: 4s - loss: 1.4769 - acc: 0.836 - ETA: 4s - loss: 1.4759 - acc: 0.836 - ETA: 4s - loss: 1.4751 - acc: 0.836 - ETA: 4s - loss: 1.4743 - acc: 0.836 - ETA: 4s - loss: 1.4734 - acc: 0.836 - ETA: 4s - loss: 1.4724 - acc: 0.837 - ETA: 4s - loss: 1.4716 - acc: 0.837 - ETA: 4s - loss: 1.4708 - acc: 0.837 - ETA: 4s - loss: 1.4699 - acc: 0.837 - ETA: 4s - loss: 1.4689 - acc: 0.837 - ETA: 4s - loss: 1.4681 - acc: 0.837 - ETA: 4s - loss: 1.4673 - acc: 0.837 - ETA: 4s - loss: 1.4664 - acc: 0.838 - ETA: 3s - loss: 1.4656 - acc: 0.838 - ETA: 3s - loss: 1.4647 - acc: 0.838 - ETA: 3s - loss: 1.4641 - acc: 0.838 - ETA: 3s - loss: 1.4633 - acc: 0.838 - ETA: 3s - loss: 1.4624 - acc: 0.838 - ETA: 3s - loss: 1.4616 - acc: 0.838 - ETA: 3s - loss: 1.4606 - acc: 0.838 - ETA: 3s - loss: 1.4600 - acc: 0.838 - ETA: 3s - loss: 1.4591 - acc: 0.839 - ETA: 3s - loss: 1.4584 - acc: 0.839 - ETA: 3s - loss: 1.4576 - acc: 0.839 - ETA: 3s - loss: 1.4570 - acc: 0.839 - ETA: 3s - loss: 1.4561 - acc: 0.839 - ETA: 3s - loss: 1.4554 - acc: 0.839 - ETA: 3s - loss: 1.4547 - acc: 0.839 - ETA: 3s - loss: 1.4541 - acc: 0.839 - ETA: 3s - loss: 1.4532 - acc: 0.839 - ETA: 2s - loss: 1.4522 - acc: 0.839 - ETA: 2s - loss: 1.4514 - acc: 0.840 - ETA: 2s - loss: 1.4504 - acc: 0.840 - ETA: 2s - loss: 1.4495 - acc: 0.840 - ETA: 2s - loss: 1.4490 - acc: 0.840 - ETA: 2s - loss: 1.4483 - acc: 0.840 - ETA: 2s - loss: 1.4475 - acc: 0.840 - ETA: 2s - loss: 1.4468 - acc: 0.840 - ETA: 2s - loss: 1.4459 - acc: 0.840 - ETA: 2s - loss: 1.4449 - acc: 0.841 - ETA: 2s - loss: 1.4440 - acc: 0.841 - ETA: 2s - loss: 1.4433 - acc: 0.841 - ETA: 2s - loss: 1.4425 - acc: 0.841 - ETA: 2s - loss: 1.4420 - acc: 0.841 - ETA: 2s - loss: 1.4413 - acc: 0.841 - ETA: 2s - loss: 1.4405 - acc: 0.841 - ETA: 2s - loss: 1.4396 - acc: 0.842 - ETA: 2s - loss: 1.4390 - acc: 0.842 - ETA: 1s - loss: 1.4379 - acc: 0.842 - ETA: 1s - loss: 1.4371 - acc: 0.842 - ETA: 1s - loss: 1.4364 - acc: 0.842 - ETA: 1s - loss: 1.4357 - acc: 0.843 - ETA: 1s - loss: 1.4348 - acc: 0.843 - ETA: 1s - loss: 1.4338 - acc: 0.843 - ETA: 1s - loss: 1.4330 - acc: 0.843 - ETA: 1s - loss: 1.4322 - acc: 0.843 - ETA: 1s - loss: 1.4314 - acc: 0.844 - ETA: 1s - loss: 1.4305 - acc: 0.844 - ETA: 1s - loss: 1.4297 - acc: 0.844 - ETA: 1s - loss: 1.4289 - acc: 0.844 - ETA: 1s - loss: 1.4283 - acc: 0.844 - ETA: 1s - loss: 1.4276 - acc: 0.8446"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - ETA: 1s - loss: 1.4268 - acc: 0.844 - ETA: 1s - loss: 1.4261 - acc: 0.844 - ETA: 0s - loss: 1.4250 - acc: 0.845 - ETA: 0s - loss: 1.4242 - acc: 0.845 - ETA: 0s - loss: 1.4232 - acc: 0.845 - ETA: 0s - loss: 1.4225 - acc: 0.845 - ETA: 0s - loss: 1.4215 - acc: 0.846 - ETA: 0s - loss: 1.4208 - acc: 0.846 - ETA: 0s - loss: 1.4201 - acc: 0.846 - ETA: 0s - loss: 1.4195 - acc: 0.846 - ETA: 0s - loss: 1.4189 - acc: 0.846 - ETA: 0s - loss: 1.4183 - acc: 0.846 - ETA: 0s - loss: 1.4174 - acc: 0.846 - ETA: 0s - loss: 1.4167 - acc: 0.846 - ETA: 0s - loss: 1.4160 - acc: 0.846 - ETA: 0s - loss: 1.4152 - acc: 0.847 - ETA: 0s - loss: 1.4145 - acc: 0.847 - ETA: 0s - loss: 1.4138 - acc: 0.847 - ETA: 0s - loss: 1.4133 - acc: 0.847 - 15s 242us/step - loss: 1.4126 - acc: 0.8475 - val_loss: 0.9591 - val_acc: 0.9849\n",
      "Epoch 3/12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54912/60000 [==========================>...] - ETA: 13s - loss: 1.2044 - acc: 0.89 - ETA: 13s - loss: 1.1907 - acc: 0.89 - ETA: 13s - loss: 1.1925 - acc: 0.90 - ETA: 13s - loss: 1.1897 - acc: 0.90 - ETA: 13s - loss: 1.1988 - acc: 0.89 - ETA: 13s - loss: 1.2119 - acc: 0.88 - ETA: 12s - loss: 1.2207 - acc: 0.88 - ETA: 12s - loss: 1.2234 - acc: 0.88 - ETA: 12s - loss: 1.2190 - acc: 0.88 - ETA: 12s - loss: 1.2152 - acc: 0.88 - ETA: 12s - loss: 1.2142 - acc: 0.89 - ETA: 12s - loss: 1.2122 - acc: 0.89 - ETA: 12s - loss: 1.2169 - acc: 0.89 - ETA: 12s - loss: 1.2172 - acc: 0.88 - ETA: 12s - loss: 1.2167 - acc: 0.88 - ETA: 12s - loss: 1.2145 - acc: 0.88 - ETA: 12s - loss: 1.2133 - acc: 0.89 - ETA: 12s - loss: 1.2126 - acc: 0.89 - ETA: 12s - loss: 1.2134 - acc: 0.89 - ETA: 12s - loss: 1.2141 - acc: 0.88 - ETA: 12s - loss: 1.2143 - acc: 0.88 - ETA: 12s - loss: 1.2153 - acc: 0.88 - ETA: 12s - loss: 1.2143 - acc: 0.88 - ETA: 12s - loss: 1.2154 - acc: 0.88 - ETA: 12s - loss: 1.2133 - acc: 0.88 - ETA: 12s - loss: 1.2119 - acc: 0.88 - ETA: 12s - loss: 1.2118 - acc: 0.88 - ETA: 11s - loss: 1.2109 - acc: 0.88 - ETA: 11s - loss: 1.2112 - acc: 0.88 - ETA: 11s - loss: 1.2102 - acc: 0.88 - ETA: 11s - loss: 1.2081 - acc: 0.88 - ETA: 11s - loss: 1.2083 - acc: 0.88 - ETA: 11s - loss: 1.2073 - acc: 0.88 - ETA: 11s - loss: 1.2071 - acc: 0.88 - ETA: 11s - loss: 1.2066 - acc: 0.88 - ETA: 11s - loss: 1.2053 - acc: 0.89 - ETA: 11s - loss: 1.2048 - acc: 0.89 - ETA: 11s - loss: 1.2060 - acc: 0.89 - ETA: 11s - loss: 1.2047 - acc: 0.89 - ETA: 11s - loss: 1.2046 - acc: 0.89 - ETA: 11s - loss: 1.2042 - acc: 0.89 - ETA: 11s - loss: 1.2046 - acc: 0.89 - ETA: 11s - loss: 1.2035 - acc: 0.89 - ETA: 11s - loss: 1.2030 - acc: 0.89 - ETA: 10s - loss: 1.2026 - acc: 0.89 - ETA: 10s - loss: 1.2016 - acc: 0.89 - ETA: 10s - loss: 1.2014 - acc: 0.89 - ETA: 10s - loss: 1.2013 - acc: 0.89 - ETA: 10s - loss: 1.2003 - acc: 0.89 - ETA: 10s - loss: 1.2005 - acc: 0.89 - ETA: 10s - loss: 1.2000 - acc: 0.89 - ETA: 10s - loss: 1.1997 - acc: 0.89 - ETA: 10s - loss: 1.1990 - acc: 0.89 - ETA: 10s - loss: 1.1987 - acc: 0.89 - ETA: 10s - loss: 1.1989 - acc: 0.89 - ETA: 10s - loss: 1.1985 - acc: 0.89 - ETA: 10s - loss: 1.1985 - acc: 0.89 - ETA: 10s - loss: 1.1977 - acc: 0.89 - ETA: 10s - loss: 1.1963 - acc: 0.89 - ETA: 10s - loss: 1.1961 - acc: 0.89 - ETA: 10s - loss: 1.1960 - acc: 0.89 - ETA: 10s - loss: 1.1956 - acc: 0.89 - ETA: 10s - loss: 1.1948 - acc: 0.89 - ETA: 9s - loss: 1.1940 - acc: 0.8903 - ETA: 9s - loss: 1.1936 - acc: 0.890 - ETA: 9s - loss: 1.1933 - acc: 0.890 - ETA: 9s - loss: 1.1922 - acc: 0.890 - ETA: 9s - loss: 1.1917 - acc: 0.890 - ETA: 9s - loss: 1.1919 - acc: 0.890 - ETA: 9s - loss: 1.1914 - acc: 0.890 - ETA: 9s - loss: 1.1912 - acc: 0.889 - ETA: 9s - loss: 1.1906 - acc: 0.889 - ETA: 9s - loss: 1.1898 - acc: 0.890 - ETA: 9s - loss: 1.1894 - acc: 0.890 - ETA: 9s - loss: 1.1888 - acc: 0.890 - ETA: 9s - loss: 1.1885 - acc: 0.890 - ETA: 9s - loss: 1.1880 - acc: 0.889 - ETA: 9s - loss: 1.1874 - acc: 0.889 - ETA: 9s - loss: 1.1866 - acc: 0.890 - ETA: 9s - loss: 1.1863 - acc: 0.890 - ETA: 8s - loss: 1.1858 - acc: 0.890 - ETA: 8s - loss: 1.1853 - acc: 0.890 - ETA: 8s - loss: 1.1849 - acc: 0.890 - ETA: 8s - loss: 1.1841 - acc: 0.890 - ETA: 8s - loss: 1.1829 - acc: 0.890 - ETA: 8s - loss: 1.1820 - acc: 0.891 - ETA: 8s - loss: 1.1812 - acc: 0.891 - ETA: 8s - loss: 1.1809 - acc: 0.891 - ETA: 8s - loss: 1.1809 - acc: 0.891 - ETA: 8s - loss: 1.1801 - acc: 0.891 - ETA: 8s - loss: 1.1793 - acc: 0.891 - ETA: 8s - loss: 1.1791 - acc: 0.891 - ETA: 8s - loss: 1.1785 - acc: 0.891 - ETA: 8s - loss: 1.1786 - acc: 0.891 - ETA: 8s - loss: 1.1780 - acc: 0.891 - ETA: 8s - loss: 1.1772 - acc: 0.891 - ETA: 8s - loss: 1.1763 - acc: 0.891 - ETA: 7s - loss: 1.1760 - acc: 0.891 - ETA: 7s - loss: 1.1751 - acc: 0.892 - ETA: 7s - loss: 1.1745 - acc: 0.892 - ETA: 7s - loss: 1.1737 - acc: 0.892 - ETA: 7s - loss: 1.1738 - acc: 0.892 - ETA: 7s - loss: 1.1736 - acc: 0.892 - ETA: 7s - loss: 1.1728 - acc: 0.892 - ETA: 7s - loss: 1.1723 - acc: 0.892 - ETA: 7s - loss: 1.1720 - acc: 0.892 - ETA: 7s - loss: 1.1719 - acc: 0.892 - ETA: 7s - loss: 1.1717 - acc: 0.892 - ETA: 7s - loss: 1.1711 - acc: 0.892 - ETA: 7s - loss: 1.1703 - acc: 0.892 - ETA: 7s - loss: 1.1700 - acc: 0.892 - ETA: 7s - loss: 1.1691 - acc: 0.892 - ETA: 7s - loss: 1.1688 - acc: 0.892 - ETA: 7s - loss: 1.1683 - acc: 0.892 - ETA: 7s - loss: 1.1680 - acc: 0.892 - ETA: 6s - loss: 1.1679 - acc: 0.892 - ETA: 6s - loss: 1.1675 - acc: 0.892 - ETA: 6s - loss: 1.1669 - acc: 0.892 - ETA: 6s - loss: 1.1660 - acc: 0.892 - ETA: 6s - loss: 1.1655 - acc: 0.892 - ETA: 6s - loss: 1.1650 - acc: 0.892 - ETA: 6s - loss: 1.1645 - acc: 0.892 - ETA: 6s - loss: 1.1642 - acc: 0.892 - ETA: 6s - loss: 1.1641 - acc: 0.892 - ETA: 6s - loss: 1.1637 - acc: 0.892 - ETA: 6s - loss: 1.1631 - acc: 0.892 - ETA: 6s - loss: 1.1624 - acc: 0.892 - ETA: 6s - loss: 1.1617 - acc: 0.892 - ETA: 6s - loss: 1.1615 - acc: 0.892 - ETA: 6s - loss: 1.1612 - acc: 0.892 - ETA: 6s - loss: 1.1602 - acc: 0.892 - ETA: 6s - loss: 1.1600 - acc: 0.892 - ETA: 5s - loss: 1.1596 - acc: 0.892 - ETA: 5s - loss: 1.1592 - acc: 0.892 - ETA: 5s - loss: 1.1586 - acc: 0.892 - ETA: 5s - loss: 1.1582 - acc: 0.892 - ETA: 5s - loss: 1.1579 - acc: 0.892 - ETA: 5s - loss: 1.1574 - acc: 0.892 - ETA: 5s - loss: 1.1568 - acc: 0.892 - ETA: 5s - loss: 1.1561 - acc: 0.892 - ETA: 5s - loss: 1.1561 - acc: 0.892 - ETA: 5s - loss: 1.1556 - acc: 0.892 - ETA: 5s - loss: 1.1551 - acc: 0.892 - ETA: 5s - loss: 1.1547 - acc: 0.892 - ETA: 5s - loss: 1.1543 - acc: 0.892 - ETA: 5s - loss: 1.1541 - acc: 0.892 - ETA: 5s - loss: 1.1536 - acc: 0.892 - ETA: 5s - loss: 1.1533 - acc: 0.892 - ETA: 5s - loss: 1.1527 - acc: 0.892 - ETA: 4s - loss: 1.1524 - acc: 0.892 - ETA: 4s - loss: 1.1524 - acc: 0.892 - ETA: 4s - loss: 1.1522 - acc: 0.892 - ETA: 4s - loss: 1.1520 - acc: 0.892 - ETA: 4s - loss: 1.1516 - acc: 0.892 - ETA: 4s - loss: 1.1514 - acc: 0.892 - ETA: 4s - loss: 1.1508 - acc: 0.892 - ETA: 4s - loss: 1.1506 - acc: 0.892 - ETA: 4s - loss: 1.1501 - acc: 0.892 - ETA: 4s - loss: 1.1494 - acc: 0.892 - ETA: 4s - loss: 1.1491 - acc: 0.892 - ETA: 4s - loss: 1.1484 - acc: 0.892 - ETA: 4s - loss: 1.1479 - acc: 0.892 - ETA: 4s - loss: 1.1474 - acc: 0.892 - ETA: 4s - loss: 1.1470 - acc: 0.892 - ETA: 4s - loss: 1.1463 - acc: 0.893 - ETA: 4s - loss: 1.1459 - acc: 0.893 - ETA: 4s - loss: 1.1455 - acc: 0.893 - ETA: 3s - loss: 1.1451 - acc: 0.893 - ETA: 3s - loss: 1.1445 - acc: 0.893 - ETA: 3s - loss: 1.1441 - acc: 0.893 - ETA: 3s - loss: 1.1436 - acc: 0.892 - ETA: 3s - loss: 1.1430 - acc: 0.892 - ETA: 3s - loss: 1.1425 - acc: 0.892 - ETA: 3s - loss: 1.1420 - acc: 0.892 - ETA: 3s - loss: 1.1416 - acc: 0.893 - ETA: 3s - loss: 1.1410 - acc: 0.893 - ETA: 3s - loss: 1.1405 - acc: 0.893 - ETA: 3s - loss: 1.1400 - acc: 0.893 - ETA: 3s - loss: 1.1396 - acc: 0.893 - ETA: 3s - loss: 1.1391 - acc: 0.893 - ETA: 3s - loss: 1.1387 - acc: 0.893 - ETA: 3s - loss: 1.1382 - acc: 0.893 - ETA: 3s - loss: 1.1378 - acc: 0.893 - ETA: 2s - loss: 1.1373 - acc: 0.893 - ETA: 2s - loss: 1.1369 - acc: 0.893 - ETA: 2s - loss: 1.1364 - acc: 0.893 - ETA: 2s - loss: 1.1359 - acc: 0.893 - ETA: 2s - loss: 1.1353 - acc: 0.893 - ETA: 2s - loss: 1.1349 - acc: 0.893 - ETA: 2s - loss: 1.1342 - acc: 0.893 - ETA: 2s - loss: 1.1340 - acc: 0.893 - ETA: 2s - loss: 1.1336 - acc: 0.893 - ETA: 2s - loss: 1.1328 - acc: 0.893 - ETA: 2s - loss: 1.1326 - acc: 0.893 - ETA: 2s - loss: 1.1323 - acc: 0.893 - ETA: 2s - loss: 1.1317 - acc: 0.893 - ETA: 2s - loss: 1.1313 - acc: 0.893 - ETA: 2s - loss: 1.1311 - acc: 0.893 - ETA: 2s - loss: 1.1307 - acc: 0.893 - ETA: 2s - loss: 1.1301 - acc: 0.893 - ETA: 1s - loss: 1.1299 - acc: 0.893 - ETA: 1s - loss: 1.1297 - acc: 0.893 - ETA: 1s - loss: 1.1293 - acc: 0.893 - ETA: 1s - loss: 1.1291 - acc: 0.893 - ETA: 1s - loss: 1.1288 - acc: 0.893 - ETA: 1s - loss: 1.1284 - acc: 0.893 - ETA: 1s - loss: 1.1281 - acc: 0.893 - ETA: 1s - loss: 1.1278 - acc: 0.893 - ETA: 1s - loss: 1.1275 - acc: 0.893 - ETA: 1s - loss: 1.1273 - acc: 0.893 - ETA: 1s - loss: 1.1269 - acc: 0.893 - ETA: 1s - loss: 1.1264 - acc: 0.893 - ETA: 1s - loss: 1.1259 - acc: 0.893 - ETA: 1s - loss: 1.1255 - acc: 0.893 - ETA: 1s - loss: 1.1252 - acc: 0.893260000/60000 [==============================] - ETA: 1s - loss: 1.1246 - acc: 0.893 - ETA: 1s - loss: 1.1240 - acc: 0.893 - ETA: 0s - loss: 1.1236 - acc: 0.893 - ETA: 0s - loss: 1.1233 - acc: 0.893 - ETA: 0s - loss: 1.1228 - acc: 0.893 - ETA: 0s - loss: 1.1225 - acc: 0.893 - ETA: 0s - loss: 1.1219 - acc: 0.893 - ETA: 0s - loss: 1.1214 - acc: 0.893 - ETA: 0s - loss: 1.1211 - acc: 0.893 - ETA: 0s - loss: 1.1207 - acc: 0.893 - ETA: 0s - loss: 1.1203 - acc: 0.893 - ETA: 0s - loss: 1.1199 - acc: 0.893 - ETA: 0s - loss: 1.1199 - acc: 0.893 - ETA: 0s - loss: 1.1194 - acc: 0.893 - ETA: 0s - loss: 1.1190 - acc: 0.893 - ETA: 0s - loss: 1.1187 - acc: 0.893 - ETA: 0s - loss: 1.1182 - acc: 0.893 - ETA: 0s - loss: 1.1177 - acc: 0.893 - ETA: 0s - loss: 1.1174 - acc: 0.893 - 14s 239us/step - loss: 1.1171 - acc: 0.8936 - val_loss: 0.7316 - val_acc: 0.9857\n",
      "Epoch 4/12\n",
      "  640/60000 [..............................] - ETA: 13s - loss: 0.9931 - acc: 0.92 - ETA: 13s - loss: 1.0203 - acc: 0.89 - ETA: 13s - loss: 1.0234 - acc: 0.9047"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-09f8f6f6dec4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m12\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpairs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0my_pairs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m )\n",
      "\u001b[1;32mc:\\users\\guillaume\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m   1637\u001b[0m           \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1638\u001b[0m           \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1639\u001b[1;33m           validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1640\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1641\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32mc:\\users\\guillaume\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    213\u001b[0m           \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 215\u001b[1;33m         \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    216\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    217\u001b[0m           \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\guillaume\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2984\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2985\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[1;32m-> 2986\u001b[1;33m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[0;32m   2987\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2988\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\guillaume\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1439\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1440\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(\n",
    "    [x_train],\n",
    "    [y_train],\n",
    "    batch_size=128,\n",
    "    epochs=12,\n",
    "    validation_data=([pairs],[y_pairs])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The piece of code below returns the predicted accuracy of the model on the validation set. The validation set is composed of pairs of images and the model has to decide if this pair of images represents the same images or not. The accuracy evaluates its ability to correctly indentify a good pair in the model or correctly rejecting a bad one. We compute the distance between two embeddings vectors to compute the model prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tensorflow.python.keras.engine.input_layer.InputLayer at 0x1b72c9ac550>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x1b72c9ac278>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x1b7390d0208>,\n",
       " <tensorflow.python.keras.layers.pooling.MaxPooling2D at 0x1b7390d0ac8>,\n",
       " <tensorflow.python.keras.layers.core.Dropout at 0x1b7390d06d8>,\n",
       " <tensorflow.python.keras.layers.core.Flatten at 0x1b7390d0940>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x1b76a60ab70>,\n",
       " <tensorflow.python.keras.layers.core.Dropout at 0x1b76a63c908>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x1b76a63ce48>]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod = tf.keras.Model(model.layers[0].input, model.layers[8].output)\n",
    "mod.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.11091362, -0.03499316, -0.12692009, ...,  0.11661121,\n",
       "         0.07927541,  0.11329965],\n",
       "       [-0.08944936,  0.03793859, -0.10108349, ..., -0.08085226,\n",
       "         0.04425016,  0.09394018],\n",
       "       [ 0.09185854, -0.17601787,  0.07947102, ...,  0.1084177 ,\n",
       "        -0.17120726, -0.12618414],\n",
       "       ...,\n",
       "       [-0.10432974,  0.0470138 , -0.11694739, ..., -0.09679005,\n",
       "         0.05145971,  0.10915656],\n",
       "       [-0.05835063,  0.02034933, -0.03391842, ..., -0.046302  ,\n",
       "         0.03105641,  0.02408063],\n",
       "       [-0.10100399,  0.0462327 , -0.11376633, ..., -0.09265392,\n",
       "         0.05160645,  0.10619567]], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict=mod.predict(pairs)\n",
    "print(len(predict[0]))\n",
    "predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.012136796569824219\n"
     ]
    }
   ],
   "source": [
    "print(np.sum([np.sum(predict[i] for i in range(len(predict)))])/len(predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000,)\n",
      "0.7497577667236328\n",
      "0.9342\n",
      "0.002\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "emb_raw = tf.constant(predict)\n",
    "\n",
    "actual_issame = tf.constant(issame)\n",
    "emb = tf.math.l2_normalize(emb_raw,0)\n",
    "\n",
    "# Normalizes\n",
    "#emb = tf.math.l2_normalize(emb_,0)\n",
    "\n",
    "# Separates the pairs\n",
    "emb1 = emb[0::2]\n",
    "emb2 = emb[1::2]\n",
    "\n",
    "# Computes distance between pairs\n",
    "diff = tf.squared_difference(emb1,emb2)\n",
    "dist = tf.reduce_sum(diff,1)\n",
    "\n",
    "dist = tf.reshape(tf.stack([dist,dist], axis=-1), [-1])\n",
    "print(dist.shape)\n",
    "best_threshold = 0\n",
    "#for t in np.arange(0,1,0.001):\n",
    "#t = 0.01\n",
    "\n",
    "actual_issame_bool = tf.cast(actual_issame,dtype=tf.bool)\n",
    "\n",
    "def fn(t):\n",
    "    less = tf.less(dist,t)\n",
    "\n",
    "    acc = tf.logical_not(tf.logical_xor(less,actual_issame_bool))\n",
    "    acc = tf.cast(acc,tf.float32)\n",
    "    \n",
    "    out = tf.reshape(tf.reduce_sum(acc),[])\n",
    "    \n",
    "    return out\n",
    "\n",
    "\n",
    "thresholds = tf.range(0,1,0.001)\n",
    "apply_t = tf.map_fn(fn, thresholds)\n",
    "best_t = tf.argmax(apply_t)\n",
    "\n",
    "best = thresholds[best_t]\n",
    "\n",
    "# Redo the manipulation with the best threshold\n",
    "less = tf.less(dist,best)\n",
    "#less = tf.cast(less,tf.float32)\n",
    "\n",
    "acc = tf.logical_not(tf.logical_xor(less,actual_issame_bool))\n",
    "acc = tf.cast(acc,tf.float32)\n",
    "\n",
    "out = tf.reshape(tf.reduce_sum(acc),[])\n",
    "out = tf.divide(out,len(predict))\n",
    "    \n",
    "with tf.Session() as sess:\n",
    "    t1 = time.time()\n",
    "    dist_,best_ = sess.run([out,best])\n",
    "    t2 = time.time()\n",
    "    print(t2-t1)\n",
    "    print(dist_)\n",
    "    print(best_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(np.squeeze(x_train[10]))\n",
    "print(y_train[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 24, 24, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 128)               1179776   \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 1,199,882\n",
      "Trainable params: 1,199,882\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/12\n",
      "54656/60000 [==========================>...] - ETA: 11:32 - loss: 2.3099 - acc: 0.11 - ETA: 3:58 - loss: 2.2466 - acc: 0.1615 - ETA: 2:28 - loss: 2.1419 - acc: 0.232 - ETA: 1:49 - loss: 2.0485 - acc: 0.275 - ETA: 1:27 - loss: 1.9879 - acc: 0.308 - ETA: 1:14 - loss: 1.8592 - acc: 0.366 - ETA: 1:04 - loss: 1.8003 - acc: 0.385 - ETA: 57s - loss: 1.7449 - acc: 0.409 - ETA: 51s - loss: 1.6686 - acc: 0.43 - ETA: 47s - loss: 1.5879 - acc: 0.46 - ETA: 44s - loss: 1.5306 - acc: 0.48 - ETA: 41s - loss: 1.4794 - acc: 0.50 - ETA: 38s - loss: 1.4109 - acc: 0.53 - ETA: 36s - loss: 1.3590 - acc: 0.54 - ETA: 34s - loss: 1.3086 - acc: 0.56 - ETA: 33s - loss: 1.2620 - acc: 0.58 - ETA: 31s - loss: 1.2199 - acc: 0.59 - ETA: 30s - loss: 1.1920 - acc: 0.60 - ETA: 29s - loss: 1.1640 - acc: 0.61 - ETA: 28s - loss: 1.1249 - acc: 0.63 - ETA: 27s - loss: 1.0899 - acc: 0.64 - ETA: 26s - loss: 1.0601 - acc: 0.65 - ETA: 26s - loss: 1.0361 - acc: 0.66 - ETA: 25s - loss: 1.0081 - acc: 0.67 - ETA: 24s - loss: 0.9818 - acc: 0.68 - ETA: 24s - loss: 0.9605 - acc: 0.68 - ETA: 23s - loss: 0.9412 - acc: 0.69 - ETA: 23s - loss: 0.9216 - acc: 0.69 - ETA: 22s - loss: 0.9006 - acc: 0.70 - ETA: 22s - loss: 0.8858 - acc: 0.71 - ETA: 21s - loss: 0.8746 - acc: 0.71 - ETA: 21s - loss: 0.8568 - acc: 0.72 - ETA: 20s - loss: 0.8420 - acc: 0.72 - ETA: 20s - loss: 0.8289 - acc: 0.73 - ETA: 20s - loss: 0.8141 - acc: 0.73 - ETA: 19s - loss: 0.8010 - acc: 0.74 - ETA: 19s - loss: 0.7870 - acc: 0.74 - ETA: 19s - loss: 0.7740 - acc: 0.75 - ETA: 18s - loss: 0.7643 - acc: 0.75 - ETA: 18s - loss: 0.7507 - acc: 0.75 - ETA: 18s - loss: 0.7409 - acc: 0.76 - ETA: 18s - loss: 0.7315 - acc: 0.76 - ETA: 17s - loss: 0.7189 - acc: 0.77 - ETA: 17s - loss: 0.7098 - acc: 0.77 - ETA: 17s - loss: 0.7014 - acc: 0.77 - ETA: 17s - loss: 0.6917 - acc: 0.77 - ETA: 16s - loss: 0.6834 - acc: 0.78 - ETA: 16s - loss: 0.6741 - acc: 0.78 - ETA: 16s - loss: 0.6651 - acc: 0.78 - ETA: 16s - loss: 0.6573 - acc: 0.79 - ETA: 16s - loss: 0.6486 - acc: 0.79 - ETA: 15s - loss: 0.6423 - acc: 0.79 - ETA: 15s - loss: 0.6353 - acc: 0.79 - ETA: 15s - loss: 0.6293 - acc: 0.79 - ETA: 15s - loss: 0.6233 - acc: 0.80 - ETA: 15s - loss: 0.6159 - acc: 0.80 - ETA: 15s - loss: 0.6084 - acc: 0.80 - ETA: 14s - loss: 0.6011 - acc: 0.80 - ETA: 14s - loss: 0.5949 - acc: 0.81 - ETA: 14s - loss: 0.5885 - acc: 0.81 - ETA: 14s - loss: 0.5833 - acc: 0.81 - ETA: 14s - loss: 0.5779 - acc: 0.81 - ETA: 14s - loss: 0.5716 - acc: 0.81 - ETA: 13s - loss: 0.5645 - acc: 0.82 - ETA: 13s - loss: 0.5587 - acc: 0.82 - ETA: 13s - loss: 0.5539 - acc: 0.82 - ETA: 13s - loss: 0.5482 - acc: 0.82 - ETA: 13s - loss: 0.5449 - acc: 0.82 - ETA: 13s - loss: 0.5405 - acc: 0.82 - ETA: 13s - loss: 0.5360 - acc: 0.83 - ETA: 13s - loss: 0.5311 - acc: 0.83 - ETA: 12s - loss: 0.5256 - acc: 0.83 - ETA: 12s - loss: 0.5213 - acc: 0.83 - ETA: 12s - loss: 0.5168 - acc: 0.83 - ETA: 12s - loss: 0.5130 - acc: 0.83 - ETA: 12s - loss: 0.5082 - acc: 0.83 - ETA: 12s - loss: 0.5036 - acc: 0.84 - ETA: 12s - loss: 0.4985 - acc: 0.84 - ETA: 12s - loss: 0.4947 - acc: 0.84 - ETA: 11s - loss: 0.4913 - acc: 0.84 - ETA: 11s - loss: 0.4868 - acc: 0.84 - ETA: 11s - loss: 0.4847 - acc: 0.84 - ETA: 11s - loss: 0.4824 - acc: 0.84 - ETA: 11s - loss: 0.4800 - acc: 0.84 - ETA: 11s - loss: 0.4771 - acc: 0.85 - ETA: 11s - loss: 0.4728 - acc: 0.85 - ETA: 11s - loss: 0.4696 - acc: 0.85 - ETA: 11s - loss: 0.4664 - acc: 0.85 - ETA: 10s - loss: 0.4628 - acc: 0.85 - ETA: 10s - loss: 0.4599 - acc: 0.85 - ETA: 10s - loss: 0.4571 - acc: 0.85 - ETA: 10s - loss: 0.4541 - acc: 0.85 - ETA: 10s - loss: 0.4508 - acc: 0.85 - ETA: 10s - loss: 0.4475 - acc: 0.85 - ETA: 10s - loss: 0.4450 - acc: 0.86 - ETA: 10s - loss: 0.4415 - acc: 0.86 - ETA: 10s - loss: 0.4390 - acc: 0.86 - ETA: 10s - loss: 0.4366 - acc: 0.86 - ETA: 9s - loss: 0.4340 - acc: 0.8644 - ETA: 9s - loss: 0.4314 - acc: 0.865 - ETA: 9s - loss: 0.4284 - acc: 0.866 - ETA: 9s - loss: 0.4270 - acc: 0.866 - ETA: 9s - loss: 0.4248 - acc: 0.867 - ETA: 9s - loss: 0.4217 - acc: 0.868 - ETA: 9s - loss: 0.4191 - acc: 0.869 - ETA: 9s - loss: 0.4178 - acc: 0.869 - ETA: 9s - loss: 0.4153 - acc: 0.870 - ETA: 9s - loss: 0.4137 - acc: 0.870 - ETA: 9s - loss: 0.4111 - acc: 0.871 - ETA: 8s - loss: 0.4091 - acc: 0.872 - ETA: 8s - loss: 0.4068 - acc: 0.873 - ETA: 8s - loss: 0.4051 - acc: 0.873 - ETA: 8s - loss: 0.4029 - acc: 0.874 - ETA: 8s - loss: 0.4018 - acc: 0.874 - ETA: 8s - loss: 0.3999 - acc: 0.875 - ETA: 8s - loss: 0.3977 - acc: 0.876 - ETA: 8s - loss: 0.3955 - acc: 0.876 - ETA: 8s - loss: 0.3937 - acc: 0.877 - ETA: 8s - loss: 0.3917 - acc: 0.878 - ETA: 8s - loss: 0.3897 - acc: 0.878 - ETA: 8s - loss: 0.3872 - acc: 0.879 - ETA: 7s - loss: 0.3857 - acc: 0.880 - ETA: 7s - loss: 0.3836 - acc: 0.880 - ETA: 7s - loss: 0.3815 - acc: 0.881 - ETA: 7s - loss: 0.3795 - acc: 0.882 - ETA: 7s - loss: 0.3774 - acc: 0.882 - ETA: 7s - loss: 0.3759 - acc: 0.883 - ETA: 7s - loss: 0.3740 - acc: 0.883 - ETA: 7s - loss: 0.3718 - acc: 0.884 - ETA: 7s - loss: 0.3701 - acc: 0.885 - ETA: 7s - loss: 0.3686 - acc: 0.885 - ETA: 7s - loss: 0.3672 - acc: 0.886 - ETA: 7s - loss: 0.3653 - acc: 0.886 - ETA: 7s - loss: 0.3634 - acc: 0.887 - ETA: 6s - loss: 0.3614 - acc: 0.888 - ETA: 6s - loss: 0.3607 - acc: 0.888 - ETA: 6s - loss: 0.3592 - acc: 0.888 - ETA: 6s - loss: 0.3575 - acc: 0.889 - ETA: 6s - loss: 0.3561 - acc: 0.889 - ETA: 6s - loss: 0.3544 - acc: 0.890 - ETA: 6s - loss: 0.3526 - acc: 0.890 - ETA: 6s - loss: 0.3513 - acc: 0.891 - ETA: 6s - loss: 0.3495 - acc: 0.891 - ETA: 6s - loss: 0.3482 - acc: 0.892 - ETA: 6s - loss: 0.3465 - acc: 0.892 - ETA: 6s - loss: 0.3449 - acc: 0.893 - ETA: 6s - loss: 0.3439 - acc: 0.893 - ETA: 5s - loss: 0.3425 - acc: 0.894 - ETA: 5s - loss: 0.3417 - acc: 0.894 - ETA: 5s - loss: 0.3402 - acc: 0.894 - ETA: 5s - loss: 0.3386 - acc: 0.895 - ETA: 5s - loss: 0.3373 - acc: 0.895 - ETA: 5s - loss: 0.3361 - acc: 0.896 - ETA: 5s - loss: 0.3345 - acc: 0.896 - ETA: 5s - loss: 0.3334 - acc: 0.897 - ETA: 5s - loss: 0.3321 - acc: 0.897 - ETA: 5s - loss: 0.3307 - acc: 0.898 - ETA: 5s - loss: 0.3292 - acc: 0.898 - ETA: 5s - loss: 0.3283 - acc: 0.898 - ETA: 5s - loss: 0.3270 - acc: 0.899 - ETA: 4s - loss: 0.3258 - acc: 0.899 - ETA: 4s - loss: 0.3248 - acc: 0.899 - ETA: 4s - loss: 0.3236 - acc: 0.900 - ETA: 4s - loss: 0.3222 - acc: 0.900 - ETA: 4s - loss: 0.3209 - acc: 0.901 - ETA: 4s - loss: 0.3196 - acc: 0.901 - ETA: 4s - loss: 0.3189 - acc: 0.901 - ETA: 4s - loss: 0.3176 - acc: 0.902 - ETA: 4s - loss: 0.3166 - acc: 0.902 - ETA: 4s - loss: 0.3153 - acc: 0.903 - ETA: 4s - loss: 0.3143 - acc: 0.903 - ETA: 4s - loss: 0.3133 - acc: 0.903 - ETA: 4s - loss: 0.3125 - acc: 0.904 - ETA: 4s - loss: 0.3114 - acc: 0.904 - ETA: 4s - loss: 0.3103 - acc: 0.904 - ETA: 3s - loss: 0.3092 - acc: 0.905 - ETA: 3s - loss: 0.3078 - acc: 0.905 - ETA: 3s - loss: 0.3066 - acc: 0.905 - ETA: 3s - loss: 0.3053 - acc: 0.906 - ETA: 3s - loss: 0.3043 - acc: 0.906 - ETA: 3s - loss: 0.3035 - acc: 0.906 - ETA: 3s - loss: 0.3026 - acc: 0.907 - ETA: 3s - loss: 0.3013 - acc: 0.907 - ETA: 3s - loss: 0.3002 - acc: 0.907 - ETA: 3s - loss: 0.2993 - acc: 0.908 - ETA: 3s - loss: 0.2986 - acc: 0.908 - ETA: 3s - loss: 0.2976 - acc: 0.908 - ETA: 3s - loss: 0.2965 - acc: 0.909 - ETA: 3s - loss: 0.2953 - acc: 0.909 - ETA: 2s - loss: 0.2947 - acc: 0.909 - ETA: 2s - loss: 0.2938 - acc: 0.909 - ETA: 2s - loss: 0.2926 - acc: 0.910 - ETA: 2s - loss: 0.2918 - acc: 0.910 - ETA: 2s - loss: 0.2909 - acc: 0.910 - ETA: 2s - loss: 0.2897 - acc: 0.911 - ETA: 2s - loss: 0.2889 - acc: 0.911 - ETA: 2s - loss: 0.2881 - acc: 0.911 - ETA: 2s - loss: 0.2870 - acc: 0.911 - ETA: 2s - loss: 0.2864 - acc: 0.912 - ETA: 2s - loss: 0.2856 - acc: 0.912 - ETA: 2s - loss: 0.2848 - acc: 0.912 - ETA: 2s - loss: 0.2840 - acc: 0.913 - ETA: 2s - loss: 0.2831 - acc: 0.913 - ETA: 2s - loss: 0.2825 - acc: 0.913 - ETA: 1s - loss: 0.2817 - acc: 0.913 - ETA: 1s - loss: 0.2808 - acc: 0.913 - ETA: 1s - loss: 0.2799 - acc: 0.914 - ETA: 1s - loss: 0.2790 - acc: 0.914 - ETA: 1s - loss: 0.2781 - acc: 0.914 - ETA: 1s - loss: 0.2777 - acc: 0.914 - ETA: 1s - loss: 0.2771 - acc: 0.915 - ETA: 1s - loss: 0.2763 - acc: 0.915 - ETA: 1s - loss: 0.2755 - acc: 0.915 - ETA: 1s - loss: 0.2748 - acc: 0.915960000/60000 [==============================] - ETA: 1s - loss: 0.2741 - acc: 0.916 - ETA: 1s - loss: 0.2733 - acc: 0.916 - ETA: 1s - loss: 0.2727 - acc: 0.916 - ETA: 1s - loss: 0.2721 - acc: 0.916 - ETA: 1s - loss: 0.2717 - acc: 0.916 - ETA: 0s - loss: 0.2709 - acc: 0.917 - ETA: 0s - loss: 0.2702 - acc: 0.917 - ETA: 0s - loss: 0.2694 - acc: 0.917 - ETA: 0s - loss: 0.2688 - acc: 0.917 - ETA: 0s - loss: 0.2681 - acc: 0.918 - ETA: 0s - loss: 0.2673 - acc: 0.918 - ETA: 0s - loss: 0.2664 - acc: 0.918 - ETA: 0s - loss: 0.2656 - acc: 0.918 - ETA: 0s - loss: 0.2650 - acc: 0.919 - ETA: 0s - loss: 0.2642 - acc: 0.919 - ETA: 0s - loss: 0.2637 - acc: 0.919 - ETA: 0s - loss: 0.2630 - acc: 0.919 - ETA: 0s - loss: 0.2624 - acc: 0.919 - ETA: 0s - loss: 0.2616 - acc: 0.920 - ETA: 0s - loss: 0.2611 - acc: 0.920 - 16s 266us/step - loss: 0.2606 - acc: 0.9204 - val_loss: 0.0568 - val_acc: 0.9810\n",
      "Epoch 2/12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54912/60000 [==========================>...] - ETA: 13s - loss: 0.0504 - acc: 0.98 - ETA: 13s - loss: 0.1103 - acc: 0.96 - ETA: 13s - loss: 0.0983 - acc: 0.96 - ETA: 13s - loss: 0.1153 - acc: 0.96 - ETA: 13s - loss: 0.1209 - acc: 0.96 - ETA: 13s - loss: 0.1286 - acc: 0.96 - ETA: 13s - loss: 0.1285 - acc: 0.96 - ETA: 13s - loss: 0.1204 - acc: 0.96 - ETA: 13s - loss: 0.1226 - acc: 0.96 - ETA: 13s - loss: 0.1176 - acc: 0.96 - ETA: 12s - loss: 0.1161 - acc: 0.96 - ETA: 12s - loss: 0.1137 - acc: 0.96 - ETA: 12s - loss: 0.1101 - acc: 0.97 - ETA: 12s - loss: 0.1088 - acc: 0.97 - ETA: 12s - loss: 0.1094 - acc: 0.97 - ETA: 12s - loss: 0.1077 - acc: 0.97 - ETA: 12s - loss: 0.1085 - acc: 0.97 - ETA: 12s - loss: 0.1103 - acc: 0.97 - ETA: 12s - loss: 0.1104 - acc: 0.97 - ETA: 12s - loss: 0.1082 - acc: 0.97 - ETA: 12s - loss: 0.1089 - acc: 0.97 - ETA: 12s - loss: 0.1073 - acc: 0.97 - ETA: 12s - loss: 0.1065 - acc: 0.97 - ETA: 12s - loss: 0.1074 - acc: 0.97 - ETA: 12s - loss: 0.1059 - acc: 0.97 - ETA: 12s - loss: 0.1064 - acc: 0.97 - ETA: 12s - loss: 0.1073 - acc: 0.96 - ETA: 12s - loss: 0.1048 - acc: 0.97 - ETA: 11s - loss: 0.1059 - acc: 0.97 - ETA: 11s - loss: 0.1063 - acc: 0.97 - ETA: 11s - loss: 0.1074 - acc: 0.96 - ETA: 11s - loss: 0.1068 - acc: 0.97 - ETA: 11s - loss: 0.1049 - acc: 0.97 - ETA: 11s - loss: 0.1041 - acc: 0.97 - ETA: 11s - loss: 0.1035 - acc: 0.97 - ETA: 11s - loss: 0.1030 - acc: 0.97 - ETA: 11s - loss: 0.1019 - acc: 0.97 - ETA: 11s - loss: 0.1021 - acc: 0.97 - ETA: 11s - loss: 0.1017 - acc: 0.97 - ETA: 11s - loss: 0.0998 - acc: 0.97 - ETA: 11s - loss: 0.1002 - acc: 0.97 - ETA: 11s - loss: 0.1000 - acc: 0.97 - ETA: 11s - loss: 0.1009 - acc: 0.97 - ETA: 11s - loss: 0.1006 - acc: 0.97 - ETA: 10s - loss: 0.0999 - acc: 0.97 - ETA: 10s - loss: 0.0994 - acc: 0.97 - ETA: 10s - loss: 0.0988 - acc: 0.97 - ETA: 10s - loss: 0.0995 - acc: 0.97 - ETA: 10s - loss: 0.0999 - acc: 0.97 - ETA: 10s - loss: 0.0990 - acc: 0.97 - ETA: 10s - loss: 0.0988 - acc: 0.97 - ETA: 10s - loss: 0.0980 - acc: 0.97 - ETA: 10s - loss: 0.0973 - acc: 0.97 - ETA: 10s - loss: 0.0971 - acc: 0.97 - ETA: 10s - loss: 0.0966 - acc: 0.97 - ETA: 10s - loss: 0.0968 - acc: 0.97 - ETA: 10s - loss: 0.0976 - acc: 0.97 - ETA: 10s - loss: 0.0981 - acc: 0.97 - ETA: 10s - loss: 0.0978 - acc: 0.97 - ETA: 10s - loss: 0.0981 - acc: 0.97 - ETA: 10s - loss: 0.0998 - acc: 0.97 - ETA: 9s - loss: 0.0998 - acc: 0.9700 - ETA: 9s - loss: 0.0996 - acc: 0.970 - ETA: 9s - loss: 0.0996 - acc: 0.970 - ETA: 9s - loss: 0.0992 - acc: 0.970 - ETA: 9s - loss: 0.0998 - acc: 0.970 - ETA: 9s - loss: 0.0995 - acc: 0.970 - ETA: 9s - loss: 0.0992 - acc: 0.970 - ETA: 9s - loss: 0.0987 - acc: 0.970 - ETA: 9s - loss: 0.0986 - acc: 0.970 - ETA: 9s - loss: 0.0989 - acc: 0.970 - ETA: 9s - loss: 0.0992 - acc: 0.970 - ETA: 9s - loss: 0.0993 - acc: 0.970 - ETA: 9s - loss: 0.0988 - acc: 0.970 - ETA: 9s - loss: 0.0986 - acc: 0.970 - ETA: 9s - loss: 0.0982 - acc: 0.971 - ETA: 9s - loss: 0.0981 - acc: 0.971 - ETA: 9s - loss: 0.0982 - acc: 0.971 - ETA: 8s - loss: 0.0985 - acc: 0.971 - ETA: 8s - loss: 0.0980 - acc: 0.971 - ETA: 8s - loss: 0.0980 - acc: 0.971 - ETA: 8s - loss: 0.0981 - acc: 0.971 - ETA: 8s - loss: 0.0986 - acc: 0.971 - ETA: 8s - loss: 0.0980 - acc: 0.971 - ETA: 8s - loss: 0.0977 - acc: 0.971 - ETA: 8s - loss: 0.0975 - acc: 0.971 - ETA: 8s - loss: 0.0976 - acc: 0.971 - ETA: 8s - loss: 0.0975 - acc: 0.971 - ETA: 8s - loss: 0.0980 - acc: 0.971 - ETA: 8s - loss: 0.0975 - acc: 0.971 - ETA: 8s - loss: 0.0974 - acc: 0.971 - ETA: 8s - loss: 0.0975 - acc: 0.971 - ETA: 8s - loss: 0.0973 - acc: 0.971 - ETA: 8s - loss: 0.0972 - acc: 0.971 - ETA: 8s - loss: 0.0969 - acc: 0.971 - ETA: 7s - loss: 0.0971 - acc: 0.971 - ETA: 7s - loss: 0.0974 - acc: 0.971 - ETA: 7s - loss: 0.0973 - acc: 0.971 - ETA: 7s - loss: 0.0972 - acc: 0.971 - ETA: 7s - loss: 0.0966 - acc: 0.971 - ETA: 7s - loss: 0.0961 - acc: 0.972 - ETA: 7s - loss: 0.0959 - acc: 0.972 - ETA: 7s - loss: 0.0961 - acc: 0.972 - ETA: 7s - loss: 0.0962 - acc: 0.972 - ETA: 7s - loss: 0.0956 - acc: 0.972 - ETA: 7s - loss: 0.0957 - acc: 0.972 - ETA: 7s - loss: 0.0954 - acc: 0.972 - ETA: 7s - loss: 0.0950 - acc: 0.972 - ETA: 7s - loss: 0.0951 - acc: 0.972 - ETA: 7s - loss: 0.0954 - acc: 0.972 - ETA: 7s - loss: 0.0955 - acc: 0.972 - ETA: 7s - loss: 0.0959 - acc: 0.972 - ETA: 6s - loss: 0.0957 - acc: 0.972 - ETA: 6s - loss: 0.0953 - acc: 0.972 - ETA: 6s - loss: 0.0953 - acc: 0.972 - ETA: 6s - loss: 0.0958 - acc: 0.972 - ETA: 6s - loss: 0.0955 - acc: 0.972 - ETA: 6s - loss: 0.0961 - acc: 0.972 - ETA: 6s - loss: 0.0960 - acc: 0.972 - ETA: 6s - loss: 0.0958 - acc: 0.972 - ETA: 6s - loss: 0.0954 - acc: 0.972 - ETA: 6s - loss: 0.0953 - acc: 0.972 - ETA: 6s - loss: 0.0951 - acc: 0.972 - ETA: 6s - loss: 0.0951 - acc: 0.972 - ETA: 6s - loss: 0.0949 - acc: 0.972 - ETA: 6s - loss: 0.0951 - acc: 0.972 - ETA: 6s - loss: 0.0948 - acc: 0.972 - ETA: 6s - loss: 0.0944 - acc: 0.972 - ETA: 6s - loss: 0.0945 - acc: 0.972 - ETA: 6s - loss: 0.0942 - acc: 0.972 - ETA: 5s - loss: 0.0941 - acc: 0.972 - ETA: 5s - loss: 0.0939 - acc: 0.972 - ETA: 5s - loss: 0.0938 - acc: 0.972 - ETA: 5s - loss: 0.0934 - acc: 0.972 - ETA: 5s - loss: 0.0932 - acc: 0.972 - ETA: 5s - loss: 0.0934 - acc: 0.972 - ETA: 5s - loss: 0.0940 - acc: 0.972 - ETA: 5s - loss: 0.0940 - acc: 0.972 - ETA: 5s - loss: 0.0943 - acc: 0.972 - ETA: 5s - loss: 0.0940 - acc: 0.972 - ETA: 5s - loss: 0.0938 - acc: 0.972 - ETA: 5s - loss: 0.0937 - acc: 0.972 - ETA: 5s - loss: 0.0938 - acc: 0.972 - ETA: 5s - loss: 0.0939 - acc: 0.972 - ETA: 5s - loss: 0.0942 - acc: 0.972 - ETA: 5s - loss: 0.0937 - acc: 0.972 - ETA: 5s - loss: 0.0936 - acc: 0.972 - ETA: 4s - loss: 0.0934 - acc: 0.972 - ETA: 4s - loss: 0.0933 - acc: 0.972 - ETA: 4s - loss: 0.0930 - acc: 0.972 - ETA: 4s - loss: 0.0928 - acc: 0.973 - ETA: 4s - loss: 0.0927 - acc: 0.973 - ETA: 4s - loss: 0.0924 - acc: 0.973 - ETA: 4s - loss: 0.0927 - acc: 0.973 - ETA: 4s - loss: 0.0925 - acc: 0.973 - ETA: 4s - loss: 0.0926 - acc: 0.973 - ETA: 4s - loss: 0.0927 - acc: 0.972 - ETA: 4s - loss: 0.0926 - acc: 0.972 - ETA: 4s - loss: 0.0925 - acc: 0.972 - ETA: 4s - loss: 0.0922 - acc: 0.973 - ETA: 4s - loss: 0.0923 - acc: 0.973 - ETA: 4s - loss: 0.0921 - acc: 0.973 - ETA: 4s - loss: 0.0919 - acc: 0.973 - ETA: 4s - loss: 0.0916 - acc: 0.973 - ETA: 4s - loss: 0.0921 - acc: 0.973 - ETA: 3s - loss: 0.0919 - acc: 0.973 - ETA: 3s - loss: 0.0916 - acc: 0.973 - ETA: 3s - loss: 0.0915 - acc: 0.973 - ETA: 3s - loss: 0.0914 - acc: 0.973 - ETA: 3s - loss: 0.0914 - acc: 0.973 - ETA: 3s - loss: 0.0917 - acc: 0.973 - ETA: 3s - loss: 0.0918 - acc: 0.973 - ETA: 3s - loss: 0.0915 - acc: 0.973 - ETA: 3s - loss: 0.0912 - acc: 0.973 - ETA: 3s - loss: 0.0910 - acc: 0.973 - ETA: 3s - loss: 0.0907 - acc: 0.973 - ETA: 3s - loss: 0.0907 - acc: 0.973 - ETA: 3s - loss: 0.0904 - acc: 0.973 - ETA: 3s - loss: 0.0905 - acc: 0.973 - ETA: 3s - loss: 0.0905 - acc: 0.973 - ETA: 3s - loss: 0.0906 - acc: 0.973 - ETA: 3s - loss: 0.0909 - acc: 0.973 - ETA: 2s - loss: 0.0908 - acc: 0.973 - ETA: 2s - loss: 0.0908 - acc: 0.973 - ETA: 2s - loss: 0.0907 - acc: 0.973 - ETA: 2s - loss: 0.0907 - acc: 0.973 - ETA: 2s - loss: 0.0907 - acc: 0.973 - ETA: 2s - loss: 0.0909 - acc: 0.973 - ETA: 2s - loss: 0.0907 - acc: 0.973 - ETA: 2s - loss: 0.0904 - acc: 0.973 - ETA: 2s - loss: 0.0908 - acc: 0.973 - ETA: 2s - loss: 0.0906 - acc: 0.973 - ETA: 2s - loss: 0.0904 - acc: 0.973 - ETA: 2s - loss: 0.0900 - acc: 0.973 - ETA: 2s - loss: 0.0899 - acc: 0.973 - ETA: 2s - loss: 0.0897 - acc: 0.973 - ETA: 2s - loss: 0.0896 - acc: 0.973 - ETA: 2s - loss: 0.0894 - acc: 0.973 - ETA: 2s - loss: 0.0891 - acc: 0.973 - ETA: 2s - loss: 0.0892 - acc: 0.973 - ETA: 1s - loss: 0.0892 - acc: 0.973 - ETA: 1s - loss: 0.0893 - acc: 0.973 - ETA: 1s - loss: 0.0891 - acc: 0.973 - ETA: 1s - loss: 0.0890 - acc: 0.973 - ETA: 1s - loss: 0.0889 - acc: 0.973 - ETA: 1s - loss: 0.0887 - acc: 0.974 - ETA: 1s - loss: 0.0885 - acc: 0.974 - ETA: 1s - loss: 0.0882 - acc: 0.974 - ETA: 1s - loss: 0.0880 - acc: 0.974 - ETA: 1s - loss: 0.0879 - acc: 0.974 - ETA: 1s - loss: 0.0877 - acc: 0.974 - ETA: 1s - loss: 0.0878 - acc: 0.974 - ETA: 1s - loss: 0.0876 - acc: 0.974 - ETA: 1s - loss: 0.0876 - acc: 0.974 - ETA: 1s - loss: 0.0874 - acc: 0.9744"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - ETA: 1s - loss: 0.0872 - acc: 0.974 - ETA: 1s - loss: 0.0872 - acc: 0.974 - ETA: 0s - loss: 0.0871 - acc: 0.974 - ETA: 0s - loss: 0.0871 - acc: 0.974 - ETA: 0s - loss: 0.0874 - acc: 0.974 - ETA: 0s - loss: 0.0876 - acc: 0.974 - ETA: 0s - loss: 0.0875 - acc: 0.974 - ETA: 0s - loss: 0.0878 - acc: 0.974 - ETA: 0s - loss: 0.0878 - acc: 0.974 - ETA: 0s - loss: 0.0878 - acc: 0.974 - ETA: 0s - loss: 0.0880 - acc: 0.974 - ETA: 0s - loss: 0.0877 - acc: 0.974 - ETA: 0s - loss: 0.0879 - acc: 0.974 - ETA: 0s - loss: 0.0880 - acc: 0.974 - ETA: 0s - loss: 0.0883 - acc: 0.974 - ETA: 0s - loss: 0.0883 - acc: 0.974 - ETA: 0s - loss: 0.0883 - acc: 0.974 - ETA: 0s - loss: 0.0882 - acc: 0.974 - ETA: 0s - loss: 0.0880 - acc: 0.974 - 14s 234us/step - loss: 0.0880 - acc: 0.9745 - val_loss: 0.0411 - val_acc: 0.9860\n",
      "Epoch 3/12\n",
      "54912/60000 [==========================>...] - ETA: 13s - loss: 0.0427 - acc: 0.98 - ETA: 13s - loss: 0.0435 - acc: 0.98 - ETA: 13s - loss: 0.0372 - acc: 0.98 - ETA: 13s - loss: 0.0512 - acc: 0.98 - ETA: 13s - loss: 0.0578 - acc: 0.98 - ETA: 12s - loss: 0.0593 - acc: 0.98 - ETA: 12s - loss: 0.0652 - acc: 0.97 - ETA: 12s - loss: 0.0633 - acc: 0.97 - ETA: 12s - loss: 0.0705 - acc: 0.97 - ETA: 12s - loss: 0.0693 - acc: 0.97 - ETA: 12s - loss: 0.0715 - acc: 0.97 - ETA: 12s - loss: 0.0700 - acc: 0.97 - ETA: 12s - loss: 0.0697 - acc: 0.97 - ETA: 12s - loss: 0.0676 - acc: 0.97 - ETA: 12s - loss: 0.0669 - acc: 0.97 - ETA: 12s - loss: 0.0659 - acc: 0.98 - ETA: 12s - loss: 0.0666 - acc: 0.97 - ETA: 12s - loss: 0.0677 - acc: 0.97 - ETA: 12s - loss: 0.0671 - acc: 0.97 - ETA: 12s - loss: 0.0661 - acc: 0.98 - ETA: 12s - loss: 0.0675 - acc: 0.97 - ETA: 12s - loss: 0.0671 - acc: 0.97 - ETA: 12s - loss: 0.0663 - acc: 0.98 - ETA: 12s - loss: 0.0658 - acc: 0.98 - ETA: 11s - loss: 0.0661 - acc: 0.97 - ETA: 11s - loss: 0.0658 - acc: 0.97 - ETA: 11s - loss: 0.0662 - acc: 0.97 - ETA: 11s - loss: 0.0656 - acc: 0.97 - ETA: 11s - loss: 0.0643 - acc: 0.97 - ETA: 11s - loss: 0.0640 - acc: 0.97 - ETA: 11s - loss: 0.0635 - acc: 0.98 - ETA: 11s - loss: 0.0635 - acc: 0.97 - ETA: 11s - loss: 0.0636 - acc: 0.97 - ETA: 11s - loss: 0.0645 - acc: 0.97 - ETA: 11s - loss: 0.0660 - acc: 0.97 - ETA: 11s - loss: 0.0650 - acc: 0.98 - ETA: 11s - loss: 0.0643 - acc: 0.98 - ETA: 11s - loss: 0.0646 - acc: 0.98 - ETA: 11s - loss: 0.0640 - acc: 0.98 - ETA: 11s - loss: 0.0649 - acc: 0.98 - ETA: 11s - loss: 0.0646 - acc: 0.98 - ETA: 11s - loss: 0.0646 - acc: 0.98 - ETA: 10s - loss: 0.0648 - acc: 0.98 - ETA: 10s - loss: 0.0653 - acc: 0.97 - ETA: 10s - loss: 0.0665 - acc: 0.97 - ETA: 10s - loss: 0.0666 - acc: 0.97 - ETA: 10s - loss: 0.0667 - acc: 0.97 - ETA: 10s - loss: 0.0666 - acc: 0.97 - ETA: 10s - loss: 0.0669 - acc: 0.97 - ETA: 10s - loss: 0.0666 - acc: 0.97 - ETA: 10s - loss: 0.0670 - acc: 0.97 - ETA: 10s - loss: 0.0678 - acc: 0.97 - ETA: 10s - loss: 0.0678 - acc: 0.97 - ETA: 10s - loss: 0.0685 - acc: 0.97 - ETA: 10s - loss: 0.0681 - acc: 0.97 - ETA: 10s - loss: 0.0684 - acc: 0.97 - ETA: 10s - loss: 0.0685 - acc: 0.97 - ETA: 10s - loss: 0.0690 - acc: 0.97 - ETA: 10s - loss: 0.0683 - acc: 0.97 - ETA: 9s - loss: 0.0680 - acc: 0.9791 - ETA: 9s - loss: 0.0680 - acc: 0.979 - ETA: 9s - loss: 0.0681 - acc: 0.979 - ETA: 9s - loss: 0.0680 - acc: 0.979 - ETA: 9s - loss: 0.0683 - acc: 0.978 - ETA: 9s - loss: 0.0691 - acc: 0.978 - ETA: 9s - loss: 0.0697 - acc: 0.978 - ETA: 9s - loss: 0.0695 - acc: 0.978 - ETA: 9s - loss: 0.0699 - acc: 0.978 - ETA: 9s - loss: 0.0695 - acc: 0.978 - ETA: 9s - loss: 0.0705 - acc: 0.978 - ETA: 9s - loss: 0.0708 - acc: 0.978 - ETA: 9s - loss: 0.0702 - acc: 0.978 - ETA: 9s - loss: 0.0700 - acc: 0.978 - ETA: 9s - loss: 0.0697 - acc: 0.978 - ETA: 9s - loss: 0.0697 - acc: 0.978 - ETA: 9s - loss: 0.0693 - acc: 0.978 - ETA: 9s - loss: 0.0692 - acc: 0.978 - ETA: 8s - loss: 0.0690 - acc: 0.978 - ETA: 8s - loss: 0.0694 - acc: 0.978 - ETA: 8s - loss: 0.0698 - acc: 0.978 - ETA: 8s - loss: 0.0694 - acc: 0.978 - ETA: 8s - loss: 0.0694 - acc: 0.978 - ETA: 8s - loss: 0.0692 - acc: 0.978 - ETA: 8s - loss: 0.0690 - acc: 0.978 - ETA: 8s - loss: 0.0691 - acc: 0.978 - ETA: 8s - loss: 0.0691 - acc: 0.978 - ETA: 8s - loss: 0.0697 - acc: 0.978 - ETA: 8s - loss: 0.0703 - acc: 0.978 - ETA: 8s - loss: 0.0704 - acc: 0.978 - ETA: 8s - loss: 0.0705 - acc: 0.978 - ETA: 8s - loss: 0.0708 - acc: 0.978 - ETA: 8s - loss: 0.0711 - acc: 0.978 - ETA: 8s - loss: 0.0708 - acc: 0.978 - ETA: 8s - loss: 0.0703 - acc: 0.978 - ETA: 8s - loss: 0.0698 - acc: 0.978 - ETA: 7s - loss: 0.0693 - acc: 0.979 - ETA: 7s - loss: 0.0690 - acc: 0.979 - ETA: 7s - loss: 0.0692 - acc: 0.979 - ETA: 7s - loss: 0.0690 - acc: 0.979 - ETA: 7s - loss: 0.0688 - acc: 0.979 - ETA: 7s - loss: 0.0687 - acc: 0.979 - ETA: 7s - loss: 0.0686 - acc: 0.979 - ETA: 7s - loss: 0.0687 - acc: 0.979 - ETA: 7s - loss: 0.0691 - acc: 0.979 - ETA: 7s - loss: 0.0690 - acc: 0.979 - ETA: 7s - loss: 0.0686 - acc: 0.979 - ETA: 7s - loss: 0.0690 - acc: 0.979 - ETA: 7s - loss: 0.0689 - acc: 0.979 - ETA: 7s - loss: 0.0687 - acc: 0.979 - ETA: 7s - loss: 0.0686 - acc: 0.979 - ETA: 7s - loss: 0.0684 - acc: 0.979 - ETA: 7s - loss: 0.0681 - acc: 0.979 - ETA: 6s - loss: 0.0683 - acc: 0.979 - ETA: 6s - loss: 0.0687 - acc: 0.979 - ETA: 6s - loss: 0.0685 - acc: 0.979 - ETA: 6s - loss: 0.0684 - acc: 0.979 - ETA: 6s - loss: 0.0683 - acc: 0.979 - ETA: 6s - loss: 0.0685 - acc: 0.979 - ETA: 6s - loss: 0.0685 - acc: 0.979 - ETA: 6s - loss: 0.0681 - acc: 0.979 - ETA: 6s - loss: 0.0678 - acc: 0.979 - ETA: 6s - loss: 0.0675 - acc: 0.979 - ETA: 6s - loss: 0.0674 - acc: 0.979 - ETA: 6s - loss: 0.0674 - acc: 0.979 - ETA: 6s - loss: 0.0671 - acc: 0.979 - ETA: 6s - loss: 0.0673 - acc: 0.979 - ETA: 6s - loss: 0.0672 - acc: 0.979 - ETA: 6s - loss: 0.0671 - acc: 0.979 - ETA: 6s - loss: 0.0670 - acc: 0.979 - ETA: 6s - loss: 0.0670 - acc: 0.979 - ETA: 5s - loss: 0.0670 - acc: 0.979 - ETA: 5s - loss: 0.0669 - acc: 0.979 - ETA: 5s - loss: 0.0665 - acc: 0.980 - ETA: 5s - loss: 0.0669 - acc: 0.979 - ETA: 5s - loss: 0.0670 - acc: 0.979 - ETA: 5s - loss: 0.0667 - acc: 0.979 - ETA: 5s - loss: 0.0667 - acc: 0.979 - ETA: 5s - loss: 0.0668 - acc: 0.979 - ETA: 5s - loss: 0.0666 - acc: 0.979 - ETA: 5s - loss: 0.0667 - acc: 0.979 - ETA: 5s - loss: 0.0668 - acc: 0.979 - ETA: 5s - loss: 0.0670 - acc: 0.979 - ETA: 5s - loss: 0.0671 - acc: 0.979 - ETA: 5s - loss: 0.0669 - acc: 0.979 - ETA: 5s - loss: 0.0669 - acc: 0.979 - ETA: 5s - loss: 0.0670 - acc: 0.979 - ETA: 5s - loss: 0.0674 - acc: 0.979 - ETA: 4s - loss: 0.0678 - acc: 0.979 - ETA: 4s - loss: 0.0677 - acc: 0.979 - ETA: 4s - loss: 0.0675 - acc: 0.979 - ETA: 4s - loss: 0.0674 - acc: 0.979 - ETA: 4s - loss: 0.0673 - acc: 0.979 - ETA: 4s - loss: 0.0671 - acc: 0.979 - ETA: 4s - loss: 0.0670 - acc: 0.979 - ETA: 4s - loss: 0.0668 - acc: 0.979 - ETA: 4s - loss: 0.0668 - acc: 0.979 - ETA: 4s - loss: 0.0668 - acc: 0.979 - ETA: 4s - loss: 0.0666 - acc: 0.979 - ETA: 4s - loss: 0.0663 - acc: 0.980 - ETA: 4s - loss: 0.0666 - acc: 0.980 - ETA: 4s - loss: 0.0664 - acc: 0.980 - ETA: 4s - loss: 0.0665 - acc: 0.980 - ETA: 4s - loss: 0.0665 - acc: 0.980 - ETA: 4s - loss: 0.0666 - acc: 0.980 - ETA: 4s - loss: 0.0665 - acc: 0.980 - ETA: 3s - loss: 0.0664 - acc: 0.980 - ETA: 3s - loss: 0.0664 - acc: 0.980 - ETA: 3s - loss: 0.0662 - acc: 0.980 - ETA: 3s - loss: 0.0660 - acc: 0.980 - ETA: 3s - loss: 0.0662 - acc: 0.980 - ETA: 3s - loss: 0.0660 - acc: 0.980 - ETA: 3s - loss: 0.0663 - acc: 0.980 - ETA: 3s - loss: 0.0663 - acc: 0.980 - ETA: 3s - loss: 0.0664 - acc: 0.980 - ETA: 3s - loss: 0.0662 - acc: 0.980 - ETA: 3s - loss: 0.0662 - acc: 0.980 - ETA: 3s - loss: 0.0660 - acc: 0.980 - ETA: 3s - loss: 0.0660 - acc: 0.980 - ETA: 3s - loss: 0.0660 - acc: 0.980 - ETA: 3s - loss: 0.0661 - acc: 0.980 - ETA: 3s - loss: 0.0660 - acc: 0.980 - ETA: 3s - loss: 0.0659 - acc: 0.980 - ETA: 2s - loss: 0.0663 - acc: 0.980 - ETA: 2s - loss: 0.0665 - acc: 0.979 - ETA: 2s - loss: 0.0663 - acc: 0.979 - ETA: 2s - loss: 0.0664 - acc: 0.979 - ETA: 2s - loss: 0.0662 - acc: 0.980 - ETA: 2s - loss: 0.0664 - acc: 0.979 - ETA: 2s - loss: 0.0664 - acc: 0.979 - ETA: 2s - loss: 0.0666 - acc: 0.979 - ETA: 2s - loss: 0.0668 - acc: 0.979 - ETA: 2s - loss: 0.0666 - acc: 0.979 - ETA: 2s - loss: 0.0671 - acc: 0.979 - ETA: 2s - loss: 0.0674 - acc: 0.979 - ETA: 2s - loss: 0.0673 - acc: 0.979 - ETA: 2s - loss: 0.0672 - acc: 0.979 - ETA: 2s - loss: 0.0672 - acc: 0.979 - ETA: 2s - loss: 0.0671 - acc: 0.979 - ETA: 2s - loss: 0.0672 - acc: 0.979 - ETA: 2s - loss: 0.0670 - acc: 0.979 - ETA: 1s - loss: 0.0670 - acc: 0.979 - ETA: 1s - loss: 0.0672 - acc: 0.979 - ETA: 1s - loss: 0.0671 - acc: 0.979 - ETA: 1s - loss: 0.0669 - acc: 0.979 - ETA: 1s - loss: 0.0668 - acc: 0.979 - ETA: 1s - loss: 0.0669 - acc: 0.979 - ETA: 1s - loss: 0.0670 - acc: 0.979 - ETA: 1s - loss: 0.0671 - acc: 0.979 - ETA: 1s - loss: 0.0670 - acc: 0.979 - ETA: 1s - loss: 0.0669 - acc: 0.979 - ETA: 1s - loss: 0.0669 - acc: 0.979 - ETA: 1s - loss: 0.0668 - acc: 0.979 - ETA: 1s - loss: 0.0668 - acc: 0.979 - ETA: 1s - loss: 0.0668 - acc: 0.979 - ETA: 1s - loss: 0.0667 - acc: 0.979960000/60000 [==============================] - ETA: 1s - loss: 0.0665 - acc: 0.980 - ETA: 1s - loss: 0.0664 - acc: 0.980 - ETA: 0s - loss: 0.0663 - acc: 0.980 - ETA: 0s - loss: 0.0664 - acc: 0.980 - ETA: 0s - loss: 0.0666 - acc: 0.979 - ETA: 0s - loss: 0.0665 - acc: 0.980 - ETA: 0s - loss: 0.0665 - acc: 0.980 - ETA: 0s - loss: 0.0664 - acc: 0.980 - ETA: 0s - loss: 0.0664 - acc: 0.980 - ETA: 0s - loss: 0.0665 - acc: 0.980 - ETA: 0s - loss: 0.0664 - acc: 0.980 - ETA: 0s - loss: 0.0663 - acc: 0.980 - ETA: 0s - loss: 0.0664 - acc: 0.980 - ETA: 0s - loss: 0.0663 - acc: 0.980 - ETA: 0s - loss: 0.0663 - acc: 0.980 - ETA: 0s - loss: 0.0665 - acc: 0.980 - ETA: 0s - loss: 0.0664 - acc: 0.980 - ETA: 0s - loss: 0.0664 - acc: 0.980 - ETA: 0s - loss: 0.0663 - acc: 0.980 - 14s 234us/step - loss: 0.0664 - acc: 0.9802 - val_loss: 0.0369 - val_acc: 0.9875\n",
      "Epoch 4/12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54912/60000 [==========================>...] - ETA: 13s - loss: 0.0481 - acc: 0.98 - ETA: 13s - loss: 0.0657 - acc: 0.98 - ETA: 13s - loss: 0.0697 - acc: 0.97 - ETA: 13s - loss: 0.0576 - acc: 0.98 - ETA: 13s - loss: 0.0490 - acc: 0.98 - ETA: 13s - loss: 0.0537 - acc: 0.98 - ETA: 12s - loss: 0.0550 - acc: 0.98 - ETA: 12s - loss: 0.0572 - acc: 0.98 - ETA: 12s - loss: 0.0608 - acc: 0.98 - ETA: 12s - loss: 0.0577 - acc: 0.98 - ETA: 12s - loss: 0.0604 - acc: 0.98 - ETA: 12s - loss: 0.0599 - acc: 0.98 - ETA: 12s - loss: 0.0596 - acc: 0.98 - ETA: 12s - loss: 0.0579 - acc: 0.98 - ETA: 12s - loss: 0.0583 - acc: 0.98 - ETA: 12s - loss: 0.0571 - acc: 0.98 - ETA: 12s - loss: 0.0589 - acc: 0.98 - ETA: 12s - loss: 0.0572 - acc: 0.98 - ETA: 12s - loss: 0.0578 - acc: 0.98 - ETA: 12s - loss: 0.0574 - acc: 0.98 - ETA: 12s - loss: 0.0563 - acc: 0.98 - ETA: 12s - loss: 0.0557 - acc: 0.98 - ETA: 12s - loss: 0.0556 - acc: 0.98 - ETA: 12s - loss: 0.0546 - acc: 0.98 - ETA: 12s - loss: 0.0571 - acc: 0.98 - ETA: 11s - loss: 0.0564 - acc: 0.98 - ETA: 11s - loss: 0.0559 - acc: 0.98 - ETA: 11s - loss: 0.0554 - acc: 0.98 - ETA: 11s - loss: 0.0539 - acc: 0.98 - ETA: 11s - loss: 0.0539 - acc: 0.98 - ETA: 11s - loss: 0.0548 - acc: 0.98 - ETA: 11s - loss: 0.0537 - acc: 0.98 - ETA: 11s - loss: 0.0542 - acc: 0.98 - ETA: 11s - loss: 0.0540 - acc: 0.98 - ETA: 11s - loss: 0.0540 - acc: 0.98 - ETA: 11s - loss: 0.0529 - acc: 0.98 - ETA: 11s - loss: 0.0534 - acc: 0.98 - ETA: 11s - loss: 0.0533 - acc: 0.98 - ETA: 11s - loss: 0.0525 - acc: 0.98 - ETA: 11s - loss: 0.0532 - acc: 0.98 - ETA: 11s - loss: 0.0524 - acc: 0.98 - ETA: 11s - loss: 0.0531 - acc: 0.98 - ETA: 10s - loss: 0.0524 - acc: 0.98 - ETA: 10s - loss: 0.0535 - acc: 0.98 - ETA: 10s - loss: 0.0529 - acc: 0.98 - ETA: 10s - loss: 0.0525 - acc: 0.98 - ETA: 10s - loss: 0.0522 - acc: 0.98 - ETA: 10s - loss: 0.0525 - acc: 0.98 - ETA: 10s - loss: 0.0521 - acc: 0.98 - ETA: 10s - loss: 0.0525 - acc: 0.98 - ETA: 10s - loss: 0.0524 - acc: 0.98 - ETA: 10s - loss: 0.0520 - acc: 0.98 - ETA: 10s - loss: 0.0517 - acc: 0.98 - ETA: 10s - loss: 0.0516 - acc: 0.98 - ETA: 10s - loss: 0.0517 - acc: 0.98 - ETA: 10s - loss: 0.0513 - acc: 0.98 - ETA: 10s - loss: 0.0510 - acc: 0.98 - ETA: 10s - loss: 0.0517 - acc: 0.98 - ETA: 10s - loss: 0.0519 - acc: 0.98 - ETA: 10s - loss: 0.0518 - acc: 0.98 - ETA: 9s - loss: 0.0515 - acc: 0.9851 - ETA: 9s - loss: 0.0514 - acc: 0.985 - ETA: 9s - loss: 0.0517 - acc: 0.985 - ETA: 9s - loss: 0.0521 - acc: 0.985 - ETA: 9s - loss: 0.0533 - acc: 0.985 - ETA: 9s - loss: 0.0537 - acc: 0.985 - ETA: 9s - loss: 0.0540 - acc: 0.984 - ETA: 9s - loss: 0.0547 - acc: 0.984 - ETA: 9s - loss: 0.0549 - acc: 0.984 - ETA: 9s - loss: 0.0548 - acc: 0.984 - ETA: 9s - loss: 0.0558 - acc: 0.984 - ETA: 9s - loss: 0.0553 - acc: 0.984 - ETA: 9s - loss: 0.0550 - acc: 0.984 - ETA: 9s - loss: 0.0554 - acc: 0.984 - ETA: 9s - loss: 0.0553 - acc: 0.984 - ETA: 9s - loss: 0.0559 - acc: 0.984 - ETA: 9s - loss: 0.0559 - acc: 0.984 - ETA: 8s - loss: 0.0563 - acc: 0.984 - ETA: 8s - loss: 0.0562 - acc: 0.984 - ETA: 8s - loss: 0.0562 - acc: 0.984 - ETA: 8s - loss: 0.0559 - acc: 0.984 - ETA: 8s - loss: 0.0560 - acc: 0.984 - ETA: 8s - loss: 0.0559 - acc: 0.984 - ETA: 8s - loss: 0.0562 - acc: 0.984 - ETA: 8s - loss: 0.0564 - acc: 0.984 - ETA: 8s - loss: 0.0565 - acc: 0.984 - ETA: 8s - loss: 0.0568 - acc: 0.984 - ETA: 8s - loss: 0.0567 - acc: 0.984 - ETA: 8s - loss: 0.0573 - acc: 0.983 - ETA: 8s - loss: 0.0572 - acc: 0.983 - ETA: 8s - loss: 0.0575 - acc: 0.983 - ETA: 8s - loss: 0.0575 - acc: 0.983 - ETA: 8s - loss: 0.0576 - acc: 0.983 - ETA: 8s - loss: 0.0574 - acc: 0.983 - ETA: 8s - loss: 0.0572 - acc: 0.983 - ETA: 7s - loss: 0.0568 - acc: 0.984 - ETA: 7s - loss: 0.0567 - acc: 0.984 - ETA: 7s - loss: 0.0567 - acc: 0.984 - ETA: 7s - loss: 0.0567 - acc: 0.984 - ETA: 7s - loss: 0.0568 - acc: 0.983 - ETA: 7s - loss: 0.0569 - acc: 0.983 - ETA: 7s - loss: 0.0571 - acc: 0.983 - ETA: 7s - loss: 0.0576 - acc: 0.983 - ETA: 7s - loss: 0.0578 - acc: 0.983 - ETA: 7s - loss: 0.0577 - acc: 0.983 - ETA: 7s - loss: 0.0580 - acc: 0.983 - ETA: 7s - loss: 0.0580 - acc: 0.983 - ETA: 7s - loss: 0.0577 - acc: 0.983 - ETA: 7s - loss: 0.0577 - acc: 0.983 - ETA: 7s - loss: 0.0573 - acc: 0.983 - ETA: 7s - loss: 0.0572 - acc: 0.983 - ETA: 7s - loss: 0.0571 - acc: 0.983 - ETA: 6s - loss: 0.0572 - acc: 0.983 - ETA: 6s - loss: 0.0571 - acc: 0.983 - ETA: 6s - loss: 0.0573 - acc: 0.983 - ETA: 6s - loss: 0.0571 - acc: 0.983 - ETA: 6s - loss: 0.0572 - acc: 0.983 - ETA: 6s - loss: 0.0575 - acc: 0.983 - ETA: 6s - loss: 0.0572 - acc: 0.983 - ETA: 6s - loss: 0.0570 - acc: 0.983 - ETA: 6s - loss: 0.0568 - acc: 0.983 - ETA: 6s - loss: 0.0565 - acc: 0.984 - ETA: 6s - loss: 0.0564 - acc: 0.984 - ETA: 6s - loss: 0.0564 - acc: 0.984 - ETA: 6s - loss: 0.0564 - acc: 0.983 - ETA: 6s - loss: 0.0563 - acc: 0.984 - ETA: 6s - loss: 0.0560 - acc: 0.984 - ETA: 6s - loss: 0.0560 - acc: 0.984 - ETA: 6s - loss: 0.0558 - acc: 0.984 - ETA: 6s - loss: 0.0558 - acc: 0.984 - ETA: 5s - loss: 0.0557 - acc: 0.984 - ETA: 5s - loss: 0.0555 - acc: 0.984 - ETA: 5s - loss: 0.0554 - acc: 0.984 - ETA: 5s - loss: 0.0555 - acc: 0.984 - ETA: 5s - loss: 0.0555 - acc: 0.984 - ETA: 5s - loss: 0.0555 - acc: 0.984 - ETA: 5s - loss: 0.0554 - acc: 0.984 - ETA: 5s - loss: 0.0554 - acc: 0.984 - ETA: 5s - loss: 0.0556 - acc: 0.984 - ETA: 5s - loss: 0.0554 - acc: 0.984 - ETA: 5s - loss: 0.0557 - acc: 0.984 - ETA: 5s - loss: 0.0558 - acc: 0.984 - ETA: 5s - loss: 0.0560 - acc: 0.983 - ETA: 5s - loss: 0.0559 - acc: 0.984 - ETA: 5s - loss: 0.0562 - acc: 0.983 - ETA: 5s - loss: 0.0562 - acc: 0.983 - ETA: 5s - loss: 0.0561 - acc: 0.983 - ETA: 4s - loss: 0.0559 - acc: 0.984 - ETA: 4s - loss: 0.0559 - acc: 0.983 - ETA: 4s - loss: 0.0558 - acc: 0.983 - ETA: 4s - loss: 0.0556 - acc: 0.984 - ETA: 4s - loss: 0.0557 - acc: 0.984 - ETA: 4s - loss: 0.0554 - acc: 0.984 - ETA: 4s - loss: 0.0554 - acc: 0.984 - ETA: 4s - loss: 0.0553 - acc: 0.984 - ETA: 4s - loss: 0.0556 - acc: 0.984 - ETA: 4s - loss: 0.0554 - acc: 0.984 - ETA: 4s - loss: 0.0552 - acc: 0.984 - ETA: 4s - loss: 0.0553 - acc: 0.984 - ETA: 4s - loss: 0.0555 - acc: 0.984 - ETA: 4s - loss: 0.0556 - acc: 0.984 - ETA: 4s - loss: 0.0555 - acc: 0.984 - ETA: 4s - loss: 0.0557 - acc: 0.984 - ETA: 4s - loss: 0.0559 - acc: 0.983 - ETA: 4s - loss: 0.0557 - acc: 0.984 - ETA: 3s - loss: 0.0557 - acc: 0.984 - ETA: 3s - loss: 0.0555 - acc: 0.984 - ETA: 3s - loss: 0.0555 - acc: 0.984 - ETA: 3s - loss: 0.0554 - acc: 0.984 - ETA: 3s - loss: 0.0552 - acc: 0.984 - ETA: 3s - loss: 0.0552 - acc: 0.984 - ETA: 3s - loss: 0.0550 - acc: 0.984 - ETA: 3s - loss: 0.0549 - acc: 0.984 - ETA: 3s - loss: 0.0548 - acc: 0.984 - ETA: 3s - loss: 0.0548 - acc: 0.984 - ETA: 3s - loss: 0.0548 - acc: 0.984 - ETA: 3s - loss: 0.0547 - acc: 0.984 - ETA: 3s - loss: 0.0548 - acc: 0.984 - ETA: 3s - loss: 0.0547 - acc: 0.984 - ETA: 3s - loss: 0.0546 - acc: 0.984 - ETA: 3s - loss: 0.0546 - acc: 0.984 - ETA: 3s - loss: 0.0549 - acc: 0.984 - ETA: 2s - loss: 0.0549 - acc: 0.984 - ETA: 2s - loss: 0.0549 - acc: 0.984 - ETA: 2s - loss: 0.0547 - acc: 0.984 - ETA: 2s - loss: 0.0546 - acc: 0.984 - ETA: 2s - loss: 0.0547 - acc: 0.984 - ETA: 2s - loss: 0.0545 - acc: 0.984 - ETA: 2s - loss: 0.0544 - acc: 0.984 - ETA: 2s - loss: 0.0542 - acc: 0.984 - ETA: 2s - loss: 0.0542 - acc: 0.984 - ETA: 2s - loss: 0.0541 - acc: 0.984 - ETA: 2s - loss: 0.0546 - acc: 0.984 - ETA: 2s - loss: 0.0547 - acc: 0.984 - ETA: 2s - loss: 0.0547 - acc: 0.984 - ETA: 2s - loss: 0.0547 - acc: 0.984 - ETA: 2s - loss: 0.0547 - acc: 0.984 - ETA: 2s - loss: 0.0546 - acc: 0.984 - ETA: 2s - loss: 0.0546 - acc: 0.984 - ETA: 2s - loss: 0.0548 - acc: 0.984 - ETA: 1s - loss: 0.0550 - acc: 0.984 - ETA: 1s - loss: 0.0549 - acc: 0.984 - ETA: 1s - loss: 0.0548 - acc: 0.984 - ETA: 1s - loss: 0.0550 - acc: 0.984 - ETA: 1s - loss: 0.0549 - acc: 0.984 - ETA: 1s - loss: 0.0549 - acc: 0.984 - ETA: 1s - loss: 0.0549 - acc: 0.984 - ETA: 1s - loss: 0.0550 - acc: 0.984 - ETA: 1s - loss: 0.0552 - acc: 0.984 - ETA: 1s - loss: 0.0552 - acc: 0.984 - ETA: 1s - loss: 0.0552 - acc: 0.984 - ETA: 1s - loss: 0.0553 - acc: 0.984 - ETA: 1s - loss: 0.0552 - acc: 0.984 - ETA: 1s - loss: 0.0551 - acc: 0.984 - ETA: 1s - loss: 0.0550 - acc: 0.9842"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - ETA: 1s - loss: 0.0550 - acc: 0.984 - ETA: 1s - loss: 0.0553 - acc: 0.984 - ETA: 0s - loss: 0.0554 - acc: 0.984 - ETA: 0s - loss: 0.0554 - acc: 0.984 - ETA: 0s - loss: 0.0554 - acc: 0.984 - ETA: 0s - loss: 0.0554 - acc: 0.984 - ETA: 0s - loss: 0.0554 - acc: 0.984 - ETA: 0s - loss: 0.0553 - acc: 0.984 - ETA: 0s - loss: 0.0551 - acc: 0.984 - ETA: 0s - loss: 0.0550 - acc: 0.984 - ETA: 0s - loss: 0.0551 - acc: 0.984 - ETA: 0s - loss: 0.0551 - acc: 0.984 - ETA: 0s - loss: 0.0552 - acc: 0.984 - ETA: 0s - loss: 0.0553 - acc: 0.984 - ETA: 0s - loss: 0.0552 - acc: 0.984 - ETA: 0s - loss: 0.0551 - acc: 0.984 - ETA: 0s - loss: 0.0550 - acc: 0.984 - ETA: 0s - loss: 0.0549 - acc: 0.984 - ETA: 0s - loss: 0.0549 - acc: 0.984 - 14s 234us/step - loss: 0.0548 - acc: 0.9843 - val_loss: 0.0320 - val_acc: 0.9891\n",
      "Epoch 5/12\n",
      "54912/60000 [==========================>...] - ETA: 14s - loss: 0.1232 - acc: 0.97 - ETA: 13s - loss: 0.0952 - acc: 0.97 - ETA: 13s - loss: 0.0794 - acc: 0.98 - ETA: 13s - loss: 0.0701 - acc: 0.97 - ETA: 13s - loss: 0.0639 - acc: 0.98 - ETA: 13s - loss: 0.0568 - acc: 0.98 - ETA: 13s - loss: 0.0573 - acc: 0.98 - ETA: 13s - loss: 0.0611 - acc: 0.98 - ETA: 12s - loss: 0.0587 - acc: 0.98 - ETA: 12s - loss: 0.0590 - acc: 0.98 - ETA: 12s - loss: 0.0565 - acc: 0.98 - ETA: 12s - loss: 0.0596 - acc: 0.98 - ETA: 12s - loss: 0.0560 - acc: 0.98 - ETA: 12s - loss: 0.0548 - acc: 0.98 - ETA: 12s - loss: 0.0541 - acc: 0.98 - ETA: 12s - loss: 0.0534 - acc: 0.98 - ETA: 12s - loss: 0.0528 - acc: 0.98 - ETA: 12s - loss: 0.0522 - acc: 0.98 - ETA: 12s - loss: 0.0519 - acc: 0.98 - ETA: 12s - loss: 0.0514 - acc: 0.98 - ETA: 12s - loss: 0.0500 - acc: 0.98 - ETA: 12s - loss: 0.0495 - acc: 0.98 - ETA: 12s - loss: 0.0491 - acc: 0.98 - ETA: 12s - loss: 0.0494 - acc: 0.98 - ETA: 12s - loss: 0.0480 - acc: 0.98 - ETA: 11s - loss: 0.0480 - acc: 0.98 - ETA: 11s - loss: 0.0479 - acc: 0.98 - ETA: 11s - loss: 0.0469 - acc: 0.98 - ETA: 11s - loss: 0.0479 - acc: 0.98 - ETA: 11s - loss: 0.0498 - acc: 0.98 - ETA: 11s - loss: 0.0503 - acc: 0.98 - ETA: 11s - loss: 0.0520 - acc: 0.98 - ETA: 11s - loss: 0.0539 - acc: 0.98 - ETA: 11s - loss: 0.0537 - acc: 0.98 - ETA: 11s - loss: 0.0539 - acc: 0.98 - ETA: 11s - loss: 0.0553 - acc: 0.98 - ETA: 11s - loss: 0.0547 - acc: 0.98 - ETA: 11s - loss: 0.0539 - acc: 0.98 - ETA: 11s - loss: 0.0549 - acc: 0.98 - ETA: 11s - loss: 0.0544 - acc: 0.98 - ETA: 11s - loss: 0.0537 - acc: 0.98 - ETA: 11s - loss: 0.0538 - acc: 0.98 - ETA: 10s - loss: 0.0540 - acc: 0.98 - ETA: 10s - loss: 0.0538 - acc: 0.98 - ETA: 10s - loss: 0.0540 - acc: 0.98 - ETA: 10s - loss: 0.0540 - acc: 0.98 - ETA: 10s - loss: 0.0536 - acc: 0.98 - ETA: 10s - loss: 0.0530 - acc: 0.98 - ETA: 10s - loss: 0.0533 - acc: 0.98 - ETA: 10s - loss: 0.0532 - acc: 0.98 - ETA: 10s - loss: 0.0532 - acc: 0.98 - ETA: 10s - loss: 0.0528 - acc: 0.98 - ETA: 10s - loss: 0.0529 - acc: 0.98 - ETA: 10s - loss: 0.0527 - acc: 0.98 - ETA: 10s - loss: 0.0529 - acc: 0.98 - ETA: 10s - loss: 0.0535 - acc: 0.98 - ETA: 10s - loss: 0.0534 - acc: 0.98 - ETA: 10s - loss: 0.0535 - acc: 0.98 - ETA: 10s - loss: 0.0530 - acc: 0.98 - ETA: 10s - loss: 0.0539 - acc: 0.98 - ETA: 9s - loss: 0.0537 - acc: 0.9833 - ETA: 9s - loss: 0.0532 - acc: 0.983 - ETA: 9s - loss: 0.0531 - acc: 0.983 - ETA: 9s - loss: 0.0531 - acc: 0.983 - ETA: 9s - loss: 0.0530 - acc: 0.983 - ETA: 9s - loss: 0.0533 - acc: 0.983 - ETA: 9s - loss: 0.0530 - acc: 0.983 - ETA: 9s - loss: 0.0529 - acc: 0.983 - ETA: 9s - loss: 0.0528 - acc: 0.983 - ETA: 9s - loss: 0.0524 - acc: 0.983 - ETA: 9s - loss: 0.0524 - acc: 0.984 - ETA: 9s - loss: 0.0524 - acc: 0.984 - ETA: 9s - loss: 0.0519 - acc: 0.984 - ETA: 9s - loss: 0.0521 - acc: 0.984 - ETA: 9s - loss: 0.0516 - acc: 0.984 - ETA: 9s - loss: 0.0513 - acc: 0.984 - ETA: 9s - loss: 0.0508 - acc: 0.984 - ETA: 8s - loss: 0.0507 - acc: 0.984 - ETA: 8s - loss: 0.0507 - acc: 0.984 - ETA: 8s - loss: 0.0506 - acc: 0.984 - ETA: 8s - loss: 0.0507 - acc: 0.984 - ETA: 8s - loss: 0.0507 - acc: 0.984 - ETA: 8s - loss: 0.0503 - acc: 0.984 - ETA: 8s - loss: 0.0503 - acc: 0.984 - ETA: 8s - loss: 0.0506 - acc: 0.984 - ETA: 8s - loss: 0.0503 - acc: 0.984 - ETA: 8s - loss: 0.0506 - acc: 0.984 - ETA: 8s - loss: 0.0510 - acc: 0.984 - ETA: 8s - loss: 0.0507 - acc: 0.984 - ETA: 8s - loss: 0.0510 - acc: 0.984 - ETA: 8s - loss: 0.0510 - acc: 0.984 - ETA: 8s - loss: 0.0507 - acc: 0.984 - ETA: 8s - loss: 0.0507 - acc: 0.984 - ETA: 8s - loss: 0.0504 - acc: 0.984 - ETA: 8s - loss: 0.0503 - acc: 0.984 - ETA: 7s - loss: 0.0501 - acc: 0.984 - ETA: 7s - loss: 0.0503 - acc: 0.984 - ETA: 7s - loss: 0.0507 - acc: 0.984 - ETA: 7s - loss: 0.0506 - acc: 0.984 - ETA: 7s - loss: 0.0504 - acc: 0.984 - ETA: 7s - loss: 0.0501 - acc: 0.984 - ETA: 7s - loss: 0.0500 - acc: 0.984 - ETA: 7s - loss: 0.0498 - acc: 0.984 - ETA: 7s - loss: 0.0499 - acc: 0.984 - ETA: 7s - loss: 0.0500 - acc: 0.984 - ETA: 7s - loss: 0.0500 - acc: 0.984 - ETA: 7s - loss: 0.0499 - acc: 0.984 - ETA: 7s - loss: 0.0501 - acc: 0.984 - ETA: 7s - loss: 0.0500 - acc: 0.984 - ETA: 7s - loss: 0.0500 - acc: 0.984 - ETA: 7s - loss: 0.0499 - acc: 0.984 - ETA: 7s - loss: 0.0496 - acc: 0.984 - ETA: 6s - loss: 0.0496 - acc: 0.984 - ETA: 6s - loss: 0.0495 - acc: 0.984 - ETA: 6s - loss: 0.0496 - acc: 0.984 - ETA: 6s - loss: 0.0496 - acc: 0.984 - ETA: 6s - loss: 0.0495 - acc: 0.984 - ETA: 6s - loss: 0.0495 - acc: 0.984 - ETA: 6s - loss: 0.0496 - acc: 0.984 - ETA: 6s - loss: 0.0496 - acc: 0.984 - ETA: 6s - loss: 0.0498 - acc: 0.984 - ETA: 6s - loss: 0.0500 - acc: 0.984 - ETA: 6s - loss: 0.0499 - acc: 0.984 - ETA: 6s - loss: 0.0498 - acc: 0.984 - ETA: 6s - loss: 0.0503 - acc: 0.984 - ETA: 6s - loss: 0.0502 - acc: 0.984 - ETA: 6s - loss: 0.0503 - acc: 0.984 - ETA: 6s - loss: 0.0503 - acc: 0.984 - ETA: 6s - loss: 0.0502 - acc: 0.984 - ETA: 6s - loss: 0.0501 - acc: 0.984 - ETA: 5s - loss: 0.0499 - acc: 0.984 - ETA: 5s - loss: 0.0501 - acc: 0.984 - ETA: 5s - loss: 0.0504 - acc: 0.984 - ETA: 5s - loss: 0.0503 - acc: 0.984 - ETA: 5s - loss: 0.0502 - acc: 0.984 - ETA: 5s - loss: 0.0501 - acc: 0.984 - ETA: 5s - loss: 0.0501 - acc: 0.984 - ETA: 5s - loss: 0.0502 - acc: 0.984 - ETA: 5s - loss: 0.0502 - acc: 0.984 - ETA: 5s - loss: 0.0502 - acc: 0.984 - ETA: 5s - loss: 0.0503 - acc: 0.984 - ETA: 5s - loss: 0.0502 - acc: 0.984 - ETA: 5s - loss: 0.0501 - acc: 0.984 - ETA: 5s - loss: 0.0504 - acc: 0.984 - ETA: 5s - loss: 0.0505 - acc: 0.984 - ETA: 5s - loss: 0.0504 - acc: 0.984 - ETA: 5s - loss: 0.0503 - acc: 0.984 - ETA: 4s - loss: 0.0502 - acc: 0.984 - ETA: 4s - loss: 0.0501 - acc: 0.984 - ETA: 4s - loss: 0.0502 - acc: 0.984 - ETA: 4s - loss: 0.0501 - acc: 0.984 - ETA: 4s - loss: 0.0500 - acc: 0.984 - ETA: 4s - loss: 0.0500 - acc: 0.984 - ETA: 4s - loss: 0.0500 - acc: 0.984 - ETA: 4s - loss: 0.0499 - acc: 0.984 - ETA: 4s - loss: 0.0496 - acc: 0.984 - ETA: 4s - loss: 0.0494 - acc: 0.984 - ETA: 4s - loss: 0.0497 - acc: 0.984 - ETA: 4s - loss: 0.0494 - acc: 0.984 - ETA: 4s - loss: 0.0496 - acc: 0.984 - ETA: 4s - loss: 0.0495 - acc: 0.984 - ETA: 4s - loss: 0.0496 - acc: 0.984 - ETA: 4s - loss: 0.0496 - acc: 0.984 - ETA: 4s - loss: 0.0497 - acc: 0.984 - ETA: 4s - loss: 0.0495 - acc: 0.984 - ETA: 3s - loss: 0.0493 - acc: 0.984 - ETA: 3s - loss: 0.0492 - acc: 0.984 - ETA: 3s - loss: 0.0491 - acc: 0.984 - ETA: 3s - loss: 0.0493 - acc: 0.984 - ETA: 3s - loss: 0.0492 - acc: 0.984 - ETA: 3s - loss: 0.0490 - acc: 0.984 - ETA: 3s - loss: 0.0490 - acc: 0.984 - ETA: 3s - loss: 0.0490 - acc: 0.984 - ETA: 3s - loss: 0.0490 - acc: 0.984 - ETA: 3s - loss: 0.0489 - acc: 0.984 - ETA: 3s - loss: 0.0492 - acc: 0.984 - ETA: 3s - loss: 0.0492 - acc: 0.984 - ETA: 3s - loss: 0.0492 - acc: 0.984 - ETA: 3s - loss: 0.0492 - acc: 0.984 - ETA: 3s - loss: 0.0491 - acc: 0.984 - ETA: 3s - loss: 0.0491 - acc: 0.984 - ETA: 3s - loss: 0.0491 - acc: 0.984 - ETA: 2s - loss: 0.0491 - acc: 0.984 - ETA: 2s - loss: 0.0491 - acc: 0.984 - ETA: 2s - loss: 0.0491 - acc: 0.984 - ETA: 2s - loss: 0.0491 - acc: 0.984 - ETA: 2s - loss: 0.0489 - acc: 0.984 - ETA: 2s - loss: 0.0487 - acc: 0.984 - ETA: 2s - loss: 0.0487 - acc: 0.984 - ETA: 2s - loss: 0.0486 - acc: 0.984 - ETA: 2s - loss: 0.0488 - acc: 0.984 - ETA: 2s - loss: 0.0487 - acc: 0.984 - ETA: 2s - loss: 0.0486 - acc: 0.984 - ETA: 2s - loss: 0.0485 - acc: 0.984 - ETA: 2s - loss: 0.0484 - acc: 0.984 - ETA: 2s - loss: 0.0483 - acc: 0.984 - ETA: 2s - loss: 0.0485 - acc: 0.984 - ETA: 2s - loss: 0.0485 - acc: 0.984 - ETA: 2s - loss: 0.0485 - acc: 0.984 - ETA: 1s - loss: 0.0484 - acc: 0.984 - ETA: 1s - loss: 0.0485 - acc: 0.984 - ETA: 1s - loss: 0.0484 - acc: 0.984 - ETA: 1s - loss: 0.0484 - acc: 0.984 - ETA: 1s - loss: 0.0487 - acc: 0.984 - ETA: 1s - loss: 0.0489 - acc: 0.984 - ETA: 1s - loss: 0.0489 - acc: 0.984 - ETA: 1s - loss: 0.0491 - acc: 0.984 - ETA: 1s - loss: 0.0489 - acc: 0.984 - ETA: 1s - loss: 0.0489 - acc: 0.984 - ETA: 1s - loss: 0.0488 - acc: 0.984 - ETA: 1s - loss: 0.0490 - acc: 0.984 - ETA: 1s - loss: 0.0489 - acc: 0.984 - ETA: 1s - loss: 0.0488 - acc: 0.984 - ETA: 1s - loss: 0.0488 - acc: 0.984 - ETA: 1s - loss: 0.0486 - acc: 0.984760000/60000 [==============================] - ETA: 1s - loss: 0.0485 - acc: 0.984 - ETA: 1s - loss: 0.0484 - acc: 0.984 - ETA: 0s - loss: 0.0483 - acc: 0.984 - ETA: 0s - loss: 0.0481 - acc: 0.984 - ETA: 0s - loss: 0.0483 - acc: 0.984 - ETA: 0s - loss: 0.0485 - acc: 0.984 - ETA: 0s - loss: 0.0485 - acc: 0.984 - ETA: 0s - loss: 0.0484 - acc: 0.984 - ETA: 0s - loss: 0.0483 - acc: 0.984 - ETA: 0s - loss: 0.0484 - acc: 0.984 - ETA: 0s - loss: 0.0483 - acc: 0.984 - ETA: 0s - loss: 0.0482 - acc: 0.984 - ETA: 0s - loss: 0.0484 - acc: 0.984 - ETA: 0s - loss: 0.0485 - acc: 0.984 - ETA: 0s - loss: 0.0484 - acc: 0.984 - ETA: 0s - loss: 0.0484 - acc: 0.984 - ETA: 0s - loss: 0.0484 - acc: 0.984 - ETA: 0s - loss: 0.0483 - acc: 0.984 - ETA: 0s - loss: 0.0482 - acc: 0.985 - 14s 234us/step - loss: 0.0481 - acc: 0.9850 - val_loss: 0.0322 - val_acc: 0.9898\n",
      "Epoch 6/12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54912/60000 [==========================>...] - ETA: 13s - loss: 0.0087 - acc: 1.00 - ETA: 13s - loss: 0.0102 - acc: 0.99 - ETA: 13s - loss: 0.0214 - acc: 0.99 - ETA: 13s - loss: 0.0307 - acc: 0.99 - ETA: 13s - loss: 0.0374 - acc: 0.98 - ETA: 13s - loss: 0.0461 - acc: 0.98 - ETA: 13s - loss: 0.0478 - acc: 0.98 - ETA: 12s - loss: 0.0463 - acc: 0.98 - ETA: 12s - loss: 0.0440 - acc: 0.98 - ETA: 12s - loss: 0.0444 - acc: 0.98 - ETA: 12s - loss: 0.0441 - acc: 0.98 - ETA: 12s - loss: 0.0460 - acc: 0.98 - ETA: 12s - loss: 0.0436 - acc: 0.98 - ETA: 12s - loss: 0.0419 - acc: 0.98 - ETA: 12s - loss: 0.0406 - acc: 0.98 - ETA: 12s - loss: 0.0398 - acc: 0.98 - ETA: 12s - loss: 0.0403 - acc: 0.98 - ETA: 12s - loss: 0.0392 - acc: 0.98 - ETA: 12s - loss: 0.0374 - acc: 0.98 - ETA: 12s - loss: 0.0374 - acc: 0.98 - ETA: 12s - loss: 0.0384 - acc: 0.98 - ETA: 12s - loss: 0.0376 - acc: 0.98 - ETA: 12s - loss: 0.0383 - acc: 0.98 - ETA: 12s - loss: 0.0389 - acc: 0.98 - ETA: 12s - loss: 0.0386 - acc: 0.98 - ETA: 11s - loss: 0.0379 - acc: 0.98 - ETA: 11s - loss: 0.0388 - acc: 0.98 - ETA: 11s - loss: 0.0384 - acc: 0.98 - ETA: 11s - loss: 0.0399 - acc: 0.98 - ETA: 11s - loss: 0.0403 - acc: 0.98 - ETA: 11s - loss: 0.0394 - acc: 0.98 - ETA: 11s - loss: 0.0402 - acc: 0.98 - ETA: 11s - loss: 0.0405 - acc: 0.98 - ETA: 11s - loss: 0.0401 - acc: 0.98 - ETA: 11s - loss: 0.0398 - acc: 0.98 - ETA: 11s - loss: 0.0403 - acc: 0.98 - ETA: 11s - loss: 0.0404 - acc: 0.98 - ETA: 11s - loss: 0.0406 - acc: 0.98 - ETA: 11s - loss: 0.0401 - acc: 0.98 - ETA: 11s - loss: 0.0395 - acc: 0.98 - ETA: 11s - loss: 0.0396 - acc: 0.98 - ETA: 11s - loss: 0.0395 - acc: 0.98 - ETA: 11s - loss: 0.0391 - acc: 0.98 - ETA: 10s - loss: 0.0391 - acc: 0.98 - ETA: 10s - loss: 0.0387 - acc: 0.98 - ETA: 10s - loss: 0.0384 - acc: 0.98 - ETA: 10s - loss: 0.0397 - acc: 0.98 - ETA: 10s - loss: 0.0390 - acc: 0.98 - ETA: 10s - loss: 0.0391 - acc: 0.98 - ETA: 10s - loss: 0.0388 - acc: 0.98 - ETA: 10s - loss: 0.0386 - acc: 0.98 - ETA: 10s - loss: 0.0387 - acc: 0.98 - ETA: 10s - loss: 0.0390 - acc: 0.98 - ETA: 10s - loss: 0.0388 - acc: 0.98 - ETA: 10s - loss: 0.0387 - acc: 0.98 - ETA: 10s - loss: 0.0383 - acc: 0.98 - ETA: 10s - loss: 0.0383 - acc: 0.98 - ETA: 10s - loss: 0.0383 - acc: 0.98 - ETA: 10s - loss: 0.0385 - acc: 0.98 - ETA: 10s - loss: 0.0385 - acc: 0.98 - ETA: 9s - loss: 0.0383 - acc: 0.9879 - ETA: 9s - loss: 0.0392 - acc: 0.987 - ETA: 9s - loss: 0.0394 - acc: 0.987 - ETA: 9s - loss: 0.0394 - acc: 0.987 - ETA: 9s - loss: 0.0392 - acc: 0.987 - ETA: 9s - loss: 0.0397 - acc: 0.987 - ETA: 9s - loss: 0.0396 - acc: 0.987 - ETA: 9s - loss: 0.0396 - acc: 0.987 - ETA: 9s - loss: 0.0396 - acc: 0.987 - ETA: 9s - loss: 0.0398 - acc: 0.987 - ETA: 9s - loss: 0.0396 - acc: 0.987 - ETA: 9s - loss: 0.0395 - acc: 0.987 - ETA: 9s - loss: 0.0395 - acc: 0.987 - ETA: 9s - loss: 0.0399 - acc: 0.987 - ETA: 9s - loss: 0.0398 - acc: 0.987 - ETA: 9s - loss: 0.0395 - acc: 0.987 - ETA: 9s - loss: 0.0393 - acc: 0.987 - ETA: 9s - loss: 0.0393 - acc: 0.987 - ETA: 8s - loss: 0.0392 - acc: 0.987 - ETA: 8s - loss: 0.0394 - acc: 0.987 - ETA: 8s - loss: 0.0394 - acc: 0.987 - ETA: 8s - loss: 0.0401 - acc: 0.987 - ETA: 8s - loss: 0.0400 - acc: 0.987 - ETA: 8s - loss: 0.0399 - acc: 0.987 - ETA: 8s - loss: 0.0400 - acc: 0.987 - ETA: 8s - loss: 0.0402 - acc: 0.987 - ETA: 8s - loss: 0.0405 - acc: 0.987 - ETA: 8s - loss: 0.0405 - acc: 0.987 - ETA: 8s - loss: 0.0412 - acc: 0.987 - ETA: 8s - loss: 0.0410 - acc: 0.987 - ETA: 8s - loss: 0.0408 - acc: 0.987 - ETA: 8s - loss: 0.0411 - acc: 0.987 - ETA: 8s - loss: 0.0413 - acc: 0.987 - ETA: 8s - loss: 0.0414 - acc: 0.987 - ETA: 8s - loss: 0.0412 - acc: 0.987 - ETA: 7s - loss: 0.0412 - acc: 0.987 - ETA: 7s - loss: 0.0414 - acc: 0.987 - ETA: 7s - loss: 0.0414 - acc: 0.986 - ETA: 7s - loss: 0.0415 - acc: 0.986 - ETA: 7s - loss: 0.0415 - acc: 0.986 - ETA: 7s - loss: 0.0414 - acc: 0.986 - ETA: 7s - loss: 0.0415 - acc: 0.986 - ETA: 7s - loss: 0.0414 - acc: 0.986 - ETA: 7s - loss: 0.0413 - acc: 0.986 - ETA: 7s - loss: 0.0413 - acc: 0.986 - ETA: 7s - loss: 0.0411 - acc: 0.986 - ETA: 7s - loss: 0.0411 - acc: 0.986 - ETA: 7s - loss: 0.0408 - acc: 0.987 - ETA: 7s - loss: 0.0407 - acc: 0.987 - ETA: 7s - loss: 0.0410 - acc: 0.986 - ETA: 7s - loss: 0.0411 - acc: 0.986 - ETA: 7s - loss: 0.0411 - acc: 0.986 - ETA: 6s - loss: 0.0410 - acc: 0.986 - ETA: 6s - loss: 0.0410 - acc: 0.987 - ETA: 6s - loss: 0.0410 - acc: 0.987 - ETA: 6s - loss: 0.0409 - acc: 0.987 - ETA: 6s - loss: 0.0409 - acc: 0.987 - ETA: 6s - loss: 0.0406 - acc: 0.987 - ETA: 6s - loss: 0.0412 - acc: 0.986 - ETA: 6s - loss: 0.0410 - acc: 0.986 - ETA: 6s - loss: 0.0410 - acc: 0.987 - ETA: 6s - loss: 0.0410 - acc: 0.987 - ETA: 6s - loss: 0.0413 - acc: 0.987 - ETA: 6s - loss: 0.0416 - acc: 0.986 - ETA: 6s - loss: 0.0415 - acc: 0.986 - ETA: 6s - loss: 0.0414 - acc: 0.987 - ETA: 6s - loss: 0.0414 - acc: 0.987 - ETA: 6s - loss: 0.0414 - acc: 0.986 - ETA: 6s - loss: 0.0413 - acc: 0.986 - ETA: 6s - loss: 0.0411 - acc: 0.987 - ETA: 5s - loss: 0.0409 - acc: 0.987 - ETA: 5s - loss: 0.0412 - acc: 0.987 - ETA: 5s - loss: 0.0411 - acc: 0.987 - ETA: 5s - loss: 0.0409 - acc: 0.987 - ETA: 5s - loss: 0.0407 - acc: 0.987 - ETA: 5s - loss: 0.0407 - acc: 0.987 - ETA: 5s - loss: 0.0406 - acc: 0.987 - ETA: 5s - loss: 0.0408 - acc: 0.987 - ETA: 5s - loss: 0.0407 - acc: 0.987 - ETA: 5s - loss: 0.0407 - acc: 0.987 - ETA: 5s - loss: 0.0408 - acc: 0.987 - ETA: 5s - loss: 0.0408 - acc: 0.987 - ETA: 5s - loss: 0.0406 - acc: 0.987 - ETA: 5s - loss: 0.0405 - acc: 0.987 - ETA: 5s - loss: 0.0407 - acc: 0.987 - ETA: 5s - loss: 0.0410 - acc: 0.987 - ETA: 5s - loss: 0.0410 - acc: 0.987 - ETA: 4s - loss: 0.0409 - acc: 0.987 - ETA: 4s - loss: 0.0411 - acc: 0.987 - ETA: 4s - loss: 0.0414 - acc: 0.986 - ETA: 4s - loss: 0.0412 - acc: 0.986 - ETA: 4s - loss: 0.0411 - acc: 0.987 - ETA: 4s - loss: 0.0409 - acc: 0.987 - ETA: 4s - loss: 0.0409 - acc: 0.987 - ETA: 4s - loss: 0.0407 - acc: 0.987 - ETA: 4s - loss: 0.0407 - acc: 0.987 - ETA: 4s - loss: 0.0406 - acc: 0.987 - ETA: 4s - loss: 0.0405 - acc: 0.987 - ETA: 4s - loss: 0.0404 - acc: 0.987 - ETA: 4s - loss: 0.0403 - acc: 0.987 - ETA: 4s - loss: 0.0402 - acc: 0.987 - ETA: 4s - loss: 0.0403 - acc: 0.987 - ETA: 4s - loss: 0.0402 - acc: 0.987 - ETA: 4s - loss: 0.0401 - acc: 0.987 - ETA: 4s - loss: 0.0400 - acc: 0.987 - ETA: 3s - loss: 0.0399 - acc: 0.987 - ETA: 3s - loss: 0.0400 - acc: 0.987 - ETA: 3s - loss: 0.0400 - acc: 0.987 - ETA: 3s - loss: 0.0399 - acc: 0.987 - ETA: 3s - loss: 0.0398 - acc: 0.987 - ETA: 3s - loss: 0.0397 - acc: 0.987 - ETA: 3s - loss: 0.0399 - acc: 0.987 - ETA: 3s - loss: 0.0398 - acc: 0.987 - ETA: 3s - loss: 0.0401 - acc: 0.987 - ETA: 3s - loss: 0.0399 - acc: 0.987 - ETA: 3s - loss: 0.0399 - acc: 0.987 - ETA: 3s - loss: 0.0398 - acc: 0.987 - ETA: 3s - loss: 0.0399 - acc: 0.987 - ETA: 3s - loss: 0.0399 - acc: 0.987 - ETA: 3s - loss: 0.0397 - acc: 0.987 - ETA: 3s - loss: 0.0396 - acc: 0.987 - ETA: 3s - loss: 0.0398 - acc: 0.987 - ETA: 2s - loss: 0.0397 - acc: 0.987 - ETA: 2s - loss: 0.0400 - acc: 0.987 - ETA: 2s - loss: 0.0404 - acc: 0.987 - ETA: 2s - loss: 0.0405 - acc: 0.987 - ETA: 2s - loss: 0.0408 - acc: 0.987 - ETA: 2s - loss: 0.0408 - acc: 0.987 - ETA: 2s - loss: 0.0408 - acc: 0.987 - ETA: 2s - loss: 0.0408 - acc: 0.987 - ETA: 2s - loss: 0.0408 - acc: 0.987 - ETA: 2s - loss: 0.0408 - acc: 0.987 - ETA: 2s - loss: 0.0409 - acc: 0.987 - ETA: 2s - loss: 0.0409 - acc: 0.987 - ETA: 2s - loss: 0.0409 - acc: 0.987 - ETA: 2s - loss: 0.0409 - acc: 0.987 - ETA: 2s - loss: 0.0410 - acc: 0.987 - ETA: 2s - loss: 0.0411 - acc: 0.987 - ETA: 2s - loss: 0.0410 - acc: 0.987 - ETA: 2s - loss: 0.0409 - acc: 0.987 - ETA: 1s - loss: 0.0409 - acc: 0.987 - ETA: 1s - loss: 0.0410 - acc: 0.987 - ETA: 1s - loss: 0.0409 - acc: 0.987 - ETA: 1s - loss: 0.0410 - acc: 0.987 - ETA: 1s - loss: 0.0411 - acc: 0.987 - ETA: 1s - loss: 0.0411 - acc: 0.987 - ETA: 1s - loss: 0.0412 - acc: 0.987 - ETA: 1s - loss: 0.0411 - acc: 0.987 - ETA: 1s - loss: 0.0412 - acc: 0.987 - ETA: 1s - loss: 0.0411 - acc: 0.987 - ETA: 1s - loss: 0.0412 - acc: 0.987 - ETA: 1s - loss: 0.0412 - acc: 0.987 - ETA: 1s - loss: 0.0412 - acc: 0.987 - ETA: 1s - loss: 0.0412 - acc: 0.987 - ETA: 1s - loss: 0.0413 - acc: 0.9870"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - ETA: 1s - loss: 0.0414 - acc: 0.986 - ETA: 1s - loss: 0.0415 - acc: 0.987 - ETA: 0s - loss: 0.0415 - acc: 0.986 - ETA: 0s - loss: 0.0415 - acc: 0.986 - ETA: 0s - loss: 0.0417 - acc: 0.986 - ETA: 0s - loss: 0.0418 - acc: 0.986 - ETA: 0s - loss: 0.0417 - acc: 0.986 - ETA: 0s - loss: 0.0421 - acc: 0.986 - ETA: 0s - loss: 0.0420 - acc: 0.986 - ETA: 0s - loss: 0.0420 - acc: 0.986 - ETA: 0s - loss: 0.0421 - acc: 0.986 - ETA: 0s - loss: 0.0422 - acc: 0.986 - ETA: 0s - loss: 0.0421 - acc: 0.986 - ETA: 0s - loss: 0.0421 - acc: 0.986 - ETA: 0s - loss: 0.0420 - acc: 0.986 - ETA: 0s - loss: 0.0420 - acc: 0.987 - ETA: 0s - loss: 0.0419 - acc: 0.987 - ETA: 0s - loss: 0.0419 - acc: 0.987 - ETA: 0s - loss: 0.0421 - acc: 0.987 - 14s 236us/step - loss: 0.0422 - acc: 0.9869 - val_loss: 0.0336 - val_acc: 0.9894\n",
      "Epoch 7/12\n",
      "54912/60000 [==========================>...] - ETA: 13s - loss: 0.0223 - acc: 0.99 - ETA: 13s - loss: 0.0323 - acc: 0.98 - ETA: 13s - loss: 0.0353 - acc: 0.98 - ETA: 13s - loss: 0.0357 - acc: 0.98 - ETA: 12s - loss: 0.0332 - acc: 0.98 - ETA: 12s - loss: 0.0308 - acc: 0.98 - ETA: 12s - loss: 0.0324 - acc: 0.98 - ETA: 12s - loss: 0.0319 - acc: 0.98 - ETA: 12s - loss: 0.0292 - acc: 0.98 - ETA: 12s - loss: 0.0336 - acc: 0.98 - ETA: 12s - loss: 0.0361 - acc: 0.98 - ETA: 12s - loss: 0.0396 - acc: 0.98 - ETA: 12s - loss: 0.0394 - acc: 0.98 - ETA: 12s - loss: 0.0405 - acc: 0.98 - ETA: 12s - loss: 0.0406 - acc: 0.98 - ETA: 12s - loss: 0.0387 - acc: 0.98 - ETA: 12s - loss: 0.0379 - acc: 0.98 - ETA: 12s - loss: 0.0432 - acc: 0.98 - ETA: 12s - loss: 0.0421 - acc: 0.98 - ETA: 12s - loss: 0.0413 - acc: 0.98 - ETA: 12s - loss: 0.0408 - acc: 0.98 - ETA: 12s - loss: 0.0427 - acc: 0.98 - ETA: 12s - loss: 0.0430 - acc: 0.98 - ETA: 12s - loss: 0.0417 - acc: 0.98 - ETA: 12s - loss: 0.0417 - acc: 0.98 - ETA: 11s - loss: 0.0408 - acc: 0.98 - ETA: 11s - loss: 0.0399 - acc: 0.98 - ETA: 11s - loss: 0.0409 - acc: 0.98 - ETA: 11s - loss: 0.0407 - acc: 0.98 - ETA: 11s - loss: 0.0403 - acc: 0.98 - ETA: 11s - loss: 0.0395 - acc: 0.98 - ETA: 11s - loss: 0.0389 - acc: 0.98 - ETA: 11s - loss: 0.0385 - acc: 0.98 - ETA: 11s - loss: 0.0389 - acc: 0.98 - ETA: 11s - loss: 0.0383 - acc: 0.98 - ETA: 11s - loss: 0.0386 - acc: 0.98 - ETA: 11s - loss: 0.0378 - acc: 0.98 - ETA: 11s - loss: 0.0380 - acc: 0.98 - ETA: 11s - loss: 0.0374 - acc: 0.98 - ETA: 11s - loss: 0.0377 - acc: 0.98 - ETA: 11s - loss: 0.0373 - acc: 0.98 - ETA: 11s - loss: 0.0371 - acc: 0.98 - ETA: 11s - loss: 0.0370 - acc: 0.98 - ETA: 10s - loss: 0.0363 - acc: 0.98 - ETA: 10s - loss: 0.0367 - acc: 0.98 - ETA: 10s - loss: 0.0366 - acc: 0.98 - ETA: 10s - loss: 0.0369 - acc: 0.98 - ETA: 10s - loss: 0.0375 - acc: 0.98 - ETA: 10s - loss: 0.0376 - acc: 0.98 - ETA: 10s - loss: 0.0373 - acc: 0.98 - ETA: 10s - loss: 0.0372 - acc: 0.98 - ETA: 10s - loss: 0.0370 - acc: 0.98 - ETA: 10s - loss: 0.0371 - acc: 0.98 - ETA: 10s - loss: 0.0370 - acc: 0.98 - ETA: 10s - loss: 0.0367 - acc: 0.98 - ETA: 10s - loss: 0.0367 - acc: 0.98 - ETA: 10s - loss: 0.0365 - acc: 0.98 - ETA: 10s - loss: 0.0364 - acc: 0.98 - ETA: 10s - loss: 0.0368 - acc: 0.98 - ETA: 10s - loss: 0.0368 - acc: 0.98 - ETA: 10s - loss: 0.0367 - acc: 0.98 - ETA: 9s - loss: 0.0373 - acc: 0.9878 - ETA: 9s - loss: 0.0375 - acc: 0.987 - ETA: 9s - loss: 0.0379 - acc: 0.987 - ETA: 9s - loss: 0.0376 - acc: 0.987 - ETA: 9s - loss: 0.0376 - acc: 0.988 - ETA: 9s - loss: 0.0373 - acc: 0.988 - ETA: 9s - loss: 0.0373 - acc: 0.988 - ETA: 9s - loss: 0.0371 - acc: 0.988 - ETA: 9s - loss: 0.0367 - acc: 0.988 - ETA: 9s - loss: 0.0364 - acc: 0.988 - ETA: 9s - loss: 0.0360 - acc: 0.988 - ETA: 9s - loss: 0.0357 - acc: 0.988 - ETA: 9s - loss: 0.0358 - acc: 0.988 - ETA: 9s - loss: 0.0356 - acc: 0.988 - ETA: 9s - loss: 0.0356 - acc: 0.988 - ETA: 9s - loss: 0.0355 - acc: 0.988 - ETA: 9s - loss: 0.0360 - acc: 0.988 - ETA: 8s - loss: 0.0364 - acc: 0.988 - ETA: 8s - loss: 0.0361 - acc: 0.988 - ETA: 8s - loss: 0.0363 - acc: 0.988 - ETA: 8s - loss: 0.0364 - acc: 0.988 - ETA: 8s - loss: 0.0366 - acc: 0.988 - ETA: 8s - loss: 0.0366 - acc: 0.988 - ETA: 8s - loss: 0.0364 - acc: 0.988 - ETA: 8s - loss: 0.0363 - acc: 0.988 - ETA: 8s - loss: 0.0361 - acc: 0.988 - ETA: 8s - loss: 0.0360 - acc: 0.988 - ETA: 8s - loss: 0.0360 - acc: 0.988 - ETA: 8s - loss: 0.0361 - acc: 0.988 - ETA: 8s - loss: 0.0361 - acc: 0.988 - ETA: 8s - loss: 0.0360 - acc: 0.988 - ETA: 8s - loss: 0.0361 - acc: 0.988 - ETA: 8s - loss: 0.0359 - acc: 0.988 - ETA: 8s - loss: 0.0361 - acc: 0.988 - ETA: 7s - loss: 0.0360 - acc: 0.988 - ETA: 7s - loss: 0.0358 - acc: 0.988 - ETA: 7s - loss: 0.0363 - acc: 0.988 - ETA: 7s - loss: 0.0364 - acc: 0.988 - ETA: 7s - loss: 0.0363 - acc: 0.988 - ETA: 7s - loss: 0.0366 - acc: 0.988 - ETA: 7s - loss: 0.0366 - acc: 0.988 - ETA: 7s - loss: 0.0368 - acc: 0.988 - ETA: 7s - loss: 0.0368 - acc: 0.988 - ETA: 7s - loss: 0.0366 - acc: 0.988 - ETA: 7s - loss: 0.0366 - acc: 0.988 - ETA: 7s - loss: 0.0369 - acc: 0.988 - ETA: 7s - loss: 0.0368 - acc: 0.988 - ETA: 7s - loss: 0.0367 - acc: 0.988 - ETA: 7s - loss: 0.0370 - acc: 0.988 - ETA: 7s - loss: 0.0369 - acc: 0.988 - ETA: 7s - loss: 0.0367 - acc: 0.988 - ETA: 7s - loss: 0.0366 - acc: 0.988 - ETA: 6s - loss: 0.0364 - acc: 0.988 - ETA: 6s - loss: 0.0370 - acc: 0.988 - ETA: 6s - loss: 0.0369 - acc: 0.988 - ETA: 6s - loss: 0.0375 - acc: 0.988 - ETA: 6s - loss: 0.0375 - acc: 0.988 - ETA: 6s - loss: 0.0376 - acc: 0.988 - ETA: 6s - loss: 0.0375 - acc: 0.988 - ETA: 6s - loss: 0.0376 - acc: 0.988 - ETA: 6s - loss: 0.0378 - acc: 0.988 - ETA: 6s - loss: 0.0378 - acc: 0.988 - ETA: 6s - loss: 0.0378 - acc: 0.988 - ETA: 6s - loss: 0.0378 - acc: 0.988 - ETA: 6s - loss: 0.0377 - acc: 0.988 - ETA: 6s - loss: 0.0380 - acc: 0.988 - ETA: 6s - loss: 0.0379 - acc: 0.988 - ETA: 6s - loss: 0.0376 - acc: 0.988 - ETA: 6s - loss: 0.0376 - acc: 0.988 - ETA: 5s - loss: 0.0376 - acc: 0.988 - ETA: 5s - loss: 0.0374 - acc: 0.988 - ETA: 5s - loss: 0.0375 - acc: 0.988 - ETA: 5s - loss: 0.0374 - acc: 0.988 - ETA: 5s - loss: 0.0377 - acc: 0.988 - ETA: 5s - loss: 0.0376 - acc: 0.988 - ETA: 5s - loss: 0.0374 - acc: 0.988 - ETA: 5s - loss: 0.0375 - acc: 0.988 - ETA: 5s - loss: 0.0373 - acc: 0.988 - ETA: 5s - loss: 0.0378 - acc: 0.988 - ETA: 5s - loss: 0.0377 - acc: 0.988 - ETA: 5s - loss: 0.0378 - acc: 0.988 - ETA: 5s - loss: 0.0379 - acc: 0.988 - ETA: 5s - loss: 0.0378 - acc: 0.988 - ETA: 5s - loss: 0.0380 - acc: 0.988 - ETA: 5s - loss: 0.0378 - acc: 0.988 - ETA: 5s - loss: 0.0380 - acc: 0.988 - ETA: 5s - loss: 0.0381 - acc: 0.988 - ETA: 4s - loss: 0.0381 - acc: 0.988 - ETA: 4s - loss: 0.0383 - acc: 0.988 - ETA: 4s - loss: 0.0383 - acc: 0.988 - ETA: 4s - loss: 0.0383 - acc: 0.988 - ETA: 4s - loss: 0.0381 - acc: 0.988 - ETA: 4s - loss: 0.0380 - acc: 0.988 - ETA: 4s - loss: 0.0381 - acc: 0.988 - ETA: 4s - loss: 0.0380 - acc: 0.988 - ETA: 4s - loss: 0.0381 - acc: 0.988 - ETA: 4s - loss: 0.0382 - acc: 0.988 - ETA: 4s - loss: 0.0384 - acc: 0.988 - ETA: 4s - loss: 0.0384 - acc: 0.988 - ETA: 4s - loss: 0.0383 - acc: 0.988 - ETA: 4s - loss: 0.0385 - acc: 0.988 - ETA: 4s - loss: 0.0384 - acc: 0.988 - ETA: 4s - loss: 0.0384 - acc: 0.988 - ETA: 4s - loss: 0.0383 - acc: 0.988 - ETA: 3s - loss: 0.0383 - acc: 0.988 - ETA: 3s - loss: 0.0384 - acc: 0.988 - ETA: 3s - loss: 0.0383 - acc: 0.988 - ETA: 3s - loss: 0.0384 - acc: 0.988 - ETA: 3s - loss: 0.0383 - acc: 0.988 - ETA: 3s - loss: 0.0382 - acc: 0.988 - ETA: 3s - loss: 0.0381 - acc: 0.988 - ETA: 3s - loss: 0.0382 - acc: 0.988 - ETA: 3s - loss: 0.0383 - acc: 0.988 - ETA: 3s - loss: 0.0382 - acc: 0.988 - ETA: 3s - loss: 0.0383 - acc: 0.988 - ETA: 3s - loss: 0.0382 - acc: 0.988 - ETA: 3s - loss: 0.0381 - acc: 0.988 - ETA: 3s - loss: 0.0380 - acc: 0.988 - ETA: 3s - loss: 0.0379 - acc: 0.988 - ETA: 3s - loss: 0.0379 - acc: 0.988 - ETA: 3s - loss: 0.0377 - acc: 0.988 - ETA: 2s - loss: 0.0378 - acc: 0.988 - ETA: 2s - loss: 0.0380 - acc: 0.988 - ETA: 2s - loss: 0.0379 - acc: 0.988 - ETA: 2s - loss: 0.0378 - acc: 0.988 - ETA: 2s - loss: 0.0380 - acc: 0.988 - ETA: 2s - loss: 0.0379 - acc: 0.988 - ETA: 2s - loss: 0.0379 - acc: 0.988 - ETA: 2s - loss: 0.0378 - acc: 0.988 - ETA: 2s - loss: 0.0378 - acc: 0.988 - ETA: 2s - loss: 0.0382 - acc: 0.988 - ETA: 2s - loss: 0.0382 - acc: 0.988 - ETA: 2s - loss: 0.0383 - acc: 0.988 - ETA: 2s - loss: 0.0383 - acc: 0.988 - ETA: 2s - loss: 0.0382 - acc: 0.988 - ETA: 2s - loss: 0.0381 - acc: 0.988 - ETA: 2s - loss: 0.0379 - acc: 0.988 - ETA: 2s - loss: 0.0379 - acc: 0.988 - ETA: 2s - loss: 0.0381 - acc: 0.988 - ETA: 1s - loss: 0.0382 - acc: 0.988 - ETA: 1s - loss: 0.0382 - acc: 0.988 - ETA: 1s - loss: 0.0381 - acc: 0.988 - ETA: 1s - loss: 0.0379 - acc: 0.988 - ETA: 1s - loss: 0.0378 - acc: 0.988 - ETA: 1s - loss: 0.0379 - acc: 0.988 - ETA: 1s - loss: 0.0378 - acc: 0.988 - ETA: 1s - loss: 0.0378 - acc: 0.988 - ETA: 1s - loss: 0.0381 - acc: 0.988 - ETA: 1s - loss: 0.0382 - acc: 0.988 - ETA: 1s - loss: 0.0381 - acc: 0.988 - ETA: 1s - loss: 0.0382 - acc: 0.988 - ETA: 1s - loss: 0.0382 - acc: 0.988 - ETA: 1s - loss: 0.0381 - acc: 0.988 - ETA: 1s - loss: 0.0383 - acc: 0.988760000/60000 [==============================] - ETA: 1s - loss: 0.0383 - acc: 0.988 - ETA: 1s - loss: 0.0384 - acc: 0.988 - ETA: 0s - loss: 0.0384 - acc: 0.988 - ETA: 0s - loss: 0.0384 - acc: 0.988 - ETA: 0s - loss: 0.0385 - acc: 0.988 - ETA: 0s - loss: 0.0386 - acc: 0.988 - ETA: 0s - loss: 0.0385 - acc: 0.988 - ETA: 0s - loss: 0.0385 - acc: 0.988 - ETA: 0s - loss: 0.0384 - acc: 0.988 - ETA: 0s - loss: 0.0384 - acc: 0.988 - ETA: 0s - loss: 0.0384 - acc: 0.988 - ETA: 0s - loss: 0.0382 - acc: 0.988 - ETA: 0s - loss: 0.0382 - acc: 0.988 - ETA: 0s - loss: 0.0382 - acc: 0.988 - ETA: 0s - loss: 0.0382 - acc: 0.988 - ETA: 0s - loss: 0.0382 - acc: 0.988 - ETA: 0s - loss: 0.0382 - acc: 0.988 - ETA: 0s - loss: 0.0383 - acc: 0.988 - ETA: 0s - loss: 0.0383 - acc: 0.988 - 14s 235us/step - loss: 0.0383 - acc: 0.9884 - val_loss: 0.0307 - val_acc: 0.9888\n",
      "Epoch 8/12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54912/60000 [==========================>...] - ETA: 13s - loss: 0.0574 - acc: 0.98 - ETA: 13s - loss: 0.0652 - acc: 0.98 - ETA: 13s - loss: 0.0488 - acc: 0.98 - ETA: 13s - loss: 0.0485 - acc: 0.98 - ETA: 13s - loss: 0.0439 - acc: 0.98 - ETA: 13s - loss: 0.0399 - acc: 0.98 - ETA: 13s - loss: 0.0418 - acc: 0.98 - ETA: 13s - loss: 0.0399 - acc: 0.98 - ETA: 12s - loss: 0.0405 - acc: 0.98 - ETA: 12s - loss: 0.0385 - acc: 0.98 - ETA: 12s - loss: 0.0398 - acc: 0.98 - ETA: 12s - loss: 0.0382 - acc: 0.98 - ETA: 12s - loss: 0.0394 - acc: 0.98 - ETA: 12s - loss: 0.0393 - acc: 0.98 - ETA: 12s - loss: 0.0375 - acc: 0.98 - ETA: 12s - loss: 0.0376 - acc: 0.98 - ETA: 12s - loss: 0.0360 - acc: 0.98 - ETA: 12s - loss: 0.0349 - acc: 0.98 - ETA: 12s - loss: 0.0338 - acc: 0.98 - ETA: 12s - loss: 0.0372 - acc: 0.98 - ETA: 12s - loss: 0.0374 - acc: 0.98 - ETA: 12s - loss: 0.0364 - acc: 0.98 - ETA: 12s - loss: 0.0365 - acc: 0.98 - ETA: 12s - loss: 0.0357 - acc: 0.98 - ETA: 12s - loss: 0.0346 - acc: 0.98 - ETA: 11s - loss: 0.0340 - acc: 0.98 - ETA: 11s - loss: 0.0340 - acc: 0.98 - ETA: 11s - loss: 0.0365 - acc: 0.98 - ETA: 11s - loss: 0.0368 - acc: 0.98 - ETA: 11s - loss: 0.0364 - acc: 0.98 - ETA: 11s - loss: 0.0363 - acc: 0.98 - ETA: 11s - loss: 0.0361 - acc: 0.98 - ETA: 11s - loss: 0.0358 - acc: 0.98 - ETA: 11s - loss: 0.0355 - acc: 0.98 - ETA: 11s - loss: 0.0359 - acc: 0.98 - ETA: 11s - loss: 0.0360 - acc: 0.98 - ETA: 11s - loss: 0.0357 - acc: 0.98 - ETA: 11s - loss: 0.0356 - acc: 0.98 - ETA: 11s - loss: 0.0363 - acc: 0.98 - ETA: 11s - loss: 0.0356 - acc: 0.98 - ETA: 11s - loss: 0.0353 - acc: 0.98 - ETA: 11s - loss: 0.0347 - acc: 0.98 - ETA: 11s - loss: 0.0354 - acc: 0.98 - ETA: 10s - loss: 0.0353 - acc: 0.98 - ETA: 10s - loss: 0.0353 - acc: 0.98 - ETA: 10s - loss: 0.0358 - acc: 0.98 - ETA: 10s - loss: 0.0359 - acc: 0.98 - ETA: 10s - loss: 0.0366 - acc: 0.98 - ETA: 10s - loss: 0.0367 - acc: 0.98 - ETA: 10s - loss: 0.0366 - acc: 0.98 - ETA: 10s - loss: 0.0362 - acc: 0.98 - ETA: 10s - loss: 0.0359 - acc: 0.98 - ETA: 10s - loss: 0.0360 - acc: 0.98 - ETA: 10s - loss: 0.0356 - acc: 0.98 - ETA: 10s - loss: 0.0358 - acc: 0.98 - ETA: 10s - loss: 0.0357 - acc: 0.98 - ETA: 10s - loss: 0.0355 - acc: 0.98 - ETA: 10s - loss: 0.0353 - acc: 0.98 - ETA: 10s - loss: 0.0350 - acc: 0.98 - ETA: 10s - loss: 0.0346 - acc: 0.98 - ETA: 9s - loss: 0.0341 - acc: 0.9890 - ETA: 9s - loss: 0.0339 - acc: 0.989 - ETA: 9s - loss: 0.0341 - acc: 0.989 - ETA: 9s - loss: 0.0346 - acc: 0.988 - ETA: 9s - loss: 0.0344 - acc: 0.989 - ETA: 9s - loss: 0.0342 - acc: 0.989 - ETA: 9s - loss: 0.0340 - acc: 0.989 - ETA: 9s - loss: 0.0342 - acc: 0.989 - ETA: 9s - loss: 0.0343 - acc: 0.989 - ETA: 9s - loss: 0.0346 - acc: 0.989 - ETA: 9s - loss: 0.0346 - acc: 0.989 - ETA: 9s - loss: 0.0350 - acc: 0.988 - ETA: 9s - loss: 0.0349 - acc: 0.988 - ETA: 9s - loss: 0.0347 - acc: 0.988 - ETA: 9s - loss: 0.0347 - acc: 0.988 - ETA: 9s - loss: 0.0347 - acc: 0.988 - ETA: 9s - loss: 0.0344 - acc: 0.989 - ETA: 8s - loss: 0.0344 - acc: 0.989 - ETA: 8s - loss: 0.0344 - acc: 0.989 - ETA: 8s - loss: 0.0342 - acc: 0.989 - ETA: 8s - loss: 0.0345 - acc: 0.989 - ETA: 8s - loss: 0.0342 - acc: 0.989 - ETA: 8s - loss: 0.0339 - acc: 0.989 - ETA: 8s - loss: 0.0341 - acc: 0.989 - ETA: 8s - loss: 0.0338 - acc: 0.989 - ETA: 8s - loss: 0.0337 - acc: 0.989 - ETA: 8s - loss: 0.0337 - acc: 0.989 - ETA: 8s - loss: 0.0335 - acc: 0.989 - ETA: 8s - loss: 0.0336 - acc: 0.989 - ETA: 8s - loss: 0.0333 - acc: 0.989 - ETA: 8s - loss: 0.0334 - acc: 0.989 - ETA: 8s - loss: 0.0332 - acc: 0.989 - ETA: 8s - loss: 0.0334 - acc: 0.989 - ETA: 8s - loss: 0.0332 - acc: 0.989 - ETA: 8s - loss: 0.0333 - acc: 0.989 - ETA: 7s - loss: 0.0333 - acc: 0.989 - ETA: 7s - loss: 0.0332 - acc: 0.989 - ETA: 7s - loss: 0.0330 - acc: 0.989 - ETA: 7s - loss: 0.0330 - acc: 0.989 - ETA: 7s - loss: 0.0333 - acc: 0.989 - ETA: 7s - loss: 0.0331 - acc: 0.989 - ETA: 7s - loss: 0.0331 - acc: 0.989 - ETA: 7s - loss: 0.0331 - acc: 0.989 - ETA: 7s - loss: 0.0331 - acc: 0.989 - ETA: 7s - loss: 0.0329 - acc: 0.989 - ETA: 7s - loss: 0.0328 - acc: 0.989 - ETA: 7s - loss: 0.0328 - acc: 0.989 - ETA: 7s - loss: 0.0326 - acc: 0.989 - ETA: 7s - loss: 0.0331 - acc: 0.989 - ETA: 7s - loss: 0.0332 - acc: 0.989 - ETA: 7s - loss: 0.0332 - acc: 0.989 - ETA: 7s - loss: 0.0330 - acc: 0.989 - ETA: 6s - loss: 0.0334 - acc: 0.989 - ETA: 6s - loss: 0.0333 - acc: 0.989 - ETA: 6s - loss: 0.0333 - acc: 0.989 - ETA: 6s - loss: 0.0331 - acc: 0.989 - ETA: 6s - loss: 0.0330 - acc: 0.989 - ETA: 6s - loss: 0.0330 - acc: 0.989 - ETA: 6s - loss: 0.0328 - acc: 0.989 - ETA: 6s - loss: 0.0328 - acc: 0.989 - ETA: 6s - loss: 0.0328 - acc: 0.989 - ETA: 6s - loss: 0.0326 - acc: 0.989 - ETA: 6s - loss: 0.0327 - acc: 0.989 - ETA: 6s - loss: 0.0325 - acc: 0.989 - ETA: 6s - loss: 0.0327 - acc: 0.989 - ETA: 6s - loss: 0.0327 - acc: 0.989 - ETA: 6s - loss: 0.0330 - acc: 0.989 - ETA: 6s - loss: 0.0329 - acc: 0.989 - ETA: 6s - loss: 0.0327 - acc: 0.989 - ETA: 6s - loss: 0.0326 - acc: 0.989 - ETA: 5s - loss: 0.0324 - acc: 0.989 - ETA: 5s - loss: 0.0325 - acc: 0.989 - ETA: 5s - loss: 0.0329 - acc: 0.989 - ETA: 5s - loss: 0.0328 - acc: 0.989 - ETA: 5s - loss: 0.0328 - acc: 0.989 - ETA: 5s - loss: 0.0328 - acc: 0.989 - ETA: 5s - loss: 0.0327 - acc: 0.989 - ETA: 5s - loss: 0.0327 - acc: 0.989 - ETA: 5s - loss: 0.0327 - acc: 0.989 - ETA: 5s - loss: 0.0329 - acc: 0.989 - ETA: 5s - loss: 0.0328 - acc: 0.989 - ETA: 5s - loss: 0.0329 - acc: 0.989 - ETA: 5s - loss: 0.0332 - acc: 0.989 - ETA: 5s - loss: 0.0333 - acc: 0.989 - ETA: 5s - loss: 0.0333 - acc: 0.989 - ETA: 5s - loss: 0.0336 - acc: 0.989 - ETA: 5s - loss: 0.0336 - acc: 0.989 - ETA: 4s - loss: 0.0338 - acc: 0.989 - ETA: 4s - loss: 0.0338 - acc: 0.989 - ETA: 4s - loss: 0.0339 - acc: 0.989 - ETA: 4s - loss: 0.0340 - acc: 0.989 - ETA: 4s - loss: 0.0340 - acc: 0.989 - ETA: 4s - loss: 0.0338 - acc: 0.989 - ETA: 4s - loss: 0.0338 - acc: 0.989 - ETA: 4s - loss: 0.0337 - acc: 0.989 - ETA: 4s - loss: 0.0336 - acc: 0.989 - ETA: 4s - loss: 0.0335 - acc: 0.989 - ETA: 4s - loss: 0.0337 - acc: 0.989 - ETA: 4s - loss: 0.0337 - acc: 0.989 - ETA: 4s - loss: 0.0337 - acc: 0.989 - ETA: 4s - loss: 0.0337 - acc: 0.989 - ETA: 4s - loss: 0.0336 - acc: 0.989 - ETA: 4s - loss: 0.0336 - acc: 0.989 - ETA: 4s - loss: 0.0338 - acc: 0.989 - ETA: 4s - loss: 0.0338 - acc: 0.989 - ETA: 3s - loss: 0.0337 - acc: 0.989 - ETA: 3s - loss: 0.0337 - acc: 0.989 - ETA: 3s - loss: 0.0336 - acc: 0.989 - ETA: 3s - loss: 0.0337 - acc: 0.989 - ETA: 3s - loss: 0.0336 - acc: 0.989 - ETA: 3s - loss: 0.0335 - acc: 0.989 - ETA: 3s - loss: 0.0336 - acc: 0.989 - ETA: 3s - loss: 0.0337 - acc: 0.989 - ETA: 3s - loss: 0.0336 - acc: 0.989 - ETA: 3s - loss: 0.0336 - acc: 0.989 - ETA: 3s - loss: 0.0337 - acc: 0.989 - ETA: 3s - loss: 0.0336 - acc: 0.989 - ETA: 3s - loss: 0.0334 - acc: 0.989 - ETA: 3s - loss: 0.0335 - acc: 0.989 - ETA: 3s - loss: 0.0334 - acc: 0.989 - ETA: 3s - loss: 0.0335 - acc: 0.989 - ETA: 3s - loss: 0.0338 - acc: 0.989 - ETA: 2s - loss: 0.0338 - acc: 0.989 - ETA: 2s - loss: 0.0337 - acc: 0.989 - ETA: 2s - loss: 0.0338 - acc: 0.989 - ETA: 2s - loss: 0.0338 - acc: 0.989 - ETA: 2s - loss: 0.0338 - acc: 0.989 - ETA: 2s - loss: 0.0338 - acc: 0.989 - ETA: 2s - loss: 0.0338 - acc: 0.989 - ETA: 2s - loss: 0.0338 - acc: 0.989 - ETA: 2s - loss: 0.0338 - acc: 0.989 - ETA: 2s - loss: 0.0337 - acc: 0.989 - ETA: 2s - loss: 0.0339 - acc: 0.989 - ETA: 2s - loss: 0.0338 - acc: 0.989 - ETA: 2s - loss: 0.0337 - acc: 0.989 - ETA: 2s - loss: 0.0337 - acc: 0.989 - ETA: 2s - loss: 0.0338 - acc: 0.989 - ETA: 2s - loss: 0.0339 - acc: 0.989 - ETA: 2s - loss: 0.0340 - acc: 0.989 - ETA: 2s - loss: 0.0339 - acc: 0.989 - ETA: 1s - loss: 0.0339 - acc: 0.989 - ETA: 1s - loss: 0.0339 - acc: 0.989 - ETA: 1s - loss: 0.0339 - acc: 0.989 - ETA: 1s - loss: 0.0339 - acc: 0.989 - ETA: 1s - loss: 0.0339 - acc: 0.989 - ETA: 1s - loss: 0.0339 - acc: 0.989 - ETA: 1s - loss: 0.0338 - acc: 0.989 - ETA: 1s - loss: 0.0337 - acc: 0.989 - ETA: 1s - loss: 0.0337 - acc: 0.989 - ETA: 1s - loss: 0.0336 - acc: 0.989 - ETA: 1s - loss: 0.0337 - acc: 0.989 - ETA: 1s - loss: 0.0338 - acc: 0.989 - ETA: 1s - loss: 0.0338 - acc: 0.989 - ETA: 1s - loss: 0.0338 - acc: 0.989 - ETA: 1s - loss: 0.0338 - acc: 0.9898"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - ETA: 1s - loss: 0.0339 - acc: 0.989 - ETA: 1s - loss: 0.0340 - acc: 0.989 - ETA: 0s - loss: 0.0340 - acc: 0.989 - ETA: 0s - loss: 0.0341 - acc: 0.989 - ETA: 0s - loss: 0.0340 - acc: 0.989 - ETA: 0s - loss: 0.0340 - acc: 0.989 - ETA: 0s - loss: 0.0339 - acc: 0.989 - ETA: 0s - loss: 0.0340 - acc: 0.989 - ETA: 0s - loss: 0.0339 - acc: 0.989 - ETA: 0s - loss: 0.0338 - acc: 0.989 - ETA: 0s - loss: 0.0337 - acc: 0.989 - ETA: 0s - loss: 0.0336 - acc: 0.989 - ETA: 0s - loss: 0.0339 - acc: 0.989 - ETA: 0s - loss: 0.0338 - acc: 0.989 - ETA: 0s - loss: 0.0338 - acc: 0.989 - ETA: 0s - loss: 0.0338 - acc: 0.989 - ETA: 0s - loss: 0.0337 - acc: 0.989 - ETA: 0s - loss: 0.0336 - acc: 0.989 - ETA: 0s - loss: 0.0335 - acc: 0.989 - 14s 234us/step - loss: 0.0334 - acc: 0.9899 - val_loss: 0.0274 - val_acc: 0.9907\n",
      "Epoch 9/12\n",
      "54912/60000 [==========================>...] - ETA: 13s - loss: 0.0046 - acc: 1.00 - ETA: 13s - loss: 0.0120 - acc: 0.99 - ETA: 13s - loss: 0.0098 - acc: 0.99 - ETA: 13s - loss: 0.0114 - acc: 0.99 - ETA: 13s - loss: 0.0126 - acc: 0.99 - ETA: 13s - loss: 0.0135 - acc: 0.99 - ETA: 13s - loss: 0.0187 - acc: 0.99 - ETA: 13s - loss: 0.0241 - acc: 0.99 - ETA: 13s - loss: 0.0223 - acc: 0.99 - ETA: 12s - loss: 0.0227 - acc: 0.99 - ETA: 12s - loss: 0.0219 - acc: 0.99 - ETA: 12s - loss: 0.0219 - acc: 0.99 - ETA: 12s - loss: 0.0211 - acc: 0.99 - ETA: 12s - loss: 0.0237 - acc: 0.99 - ETA: 12s - loss: 0.0280 - acc: 0.99 - ETA: 12s - loss: 0.0289 - acc: 0.99 - ETA: 12s - loss: 0.0288 - acc: 0.99 - ETA: 12s - loss: 0.0316 - acc: 0.99 - ETA: 12s - loss: 0.0313 - acc: 0.99 - ETA: 12s - loss: 0.0313 - acc: 0.99 - ETA: 12s - loss: 0.0316 - acc: 0.99 - ETA: 12s - loss: 0.0329 - acc: 0.98 - ETA: 12s - loss: 0.0328 - acc: 0.98 - ETA: 12s - loss: 0.0335 - acc: 0.98 - ETA: 12s - loss: 0.0336 - acc: 0.98 - ETA: 12s - loss: 0.0342 - acc: 0.98 - ETA: 11s - loss: 0.0336 - acc: 0.98 - ETA: 11s - loss: 0.0328 - acc: 0.99 - ETA: 11s - loss: 0.0323 - acc: 0.99 - ETA: 11s - loss: 0.0316 - acc: 0.99 - ETA: 11s - loss: 0.0316 - acc: 0.99 - ETA: 11s - loss: 0.0308 - acc: 0.99 - ETA: 11s - loss: 0.0312 - acc: 0.99 - ETA: 11s - loss: 0.0311 - acc: 0.99 - ETA: 11s - loss: 0.0312 - acc: 0.99 - ETA: 11s - loss: 0.0325 - acc: 0.99 - ETA: 11s - loss: 0.0322 - acc: 0.99 - ETA: 11s - loss: 0.0324 - acc: 0.99 - ETA: 11s - loss: 0.0325 - acc: 0.99 - ETA: 11s - loss: 0.0323 - acc: 0.99 - ETA: 11s - loss: 0.0322 - acc: 0.99 - ETA: 11s - loss: 0.0320 - acc: 0.99 - ETA: 11s - loss: 0.0317 - acc: 0.99 - ETA: 10s - loss: 0.0314 - acc: 0.99 - ETA: 10s - loss: 0.0314 - acc: 0.99 - ETA: 10s - loss: 0.0313 - acc: 0.99 - ETA: 10s - loss: 0.0311 - acc: 0.99 - ETA: 10s - loss: 0.0308 - acc: 0.99 - ETA: 10s - loss: 0.0304 - acc: 0.99 - ETA: 10s - loss: 0.0298 - acc: 0.99 - ETA: 10s - loss: 0.0296 - acc: 0.99 - ETA: 10s - loss: 0.0294 - acc: 0.99 - ETA: 10s - loss: 0.0298 - acc: 0.99 - ETA: 10s - loss: 0.0308 - acc: 0.99 - ETA: 10s - loss: 0.0306 - acc: 0.99 - ETA: 10s - loss: 0.0303 - acc: 0.99 - ETA: 10s - loss: 0.0306 - acc: 0.99 - ETA: 10s - loss: 0.0302 - acc: 0.99 - ETA: 10s - loss: 0.0304 - acc: 0.99 - ETA: 10s - loss: 0.0301 - acc: 0.99 - ETA: 9s - loss: 0.0305 - acc: 0.9908 - ETA: 9s - loss: 0.0305 - acc: 0.990 - ETA: 9s - loss: 0.0304 - acc: 0.990 - ETA: 9s - loss: 0.0308 - acc: 0.990 - ETA: 9s - loss: 0.0310 - acc: 0.990 - ETA: 9s - loss: 0.0306 - acc: 0.990 - ETA: 9s - loss: 0.0309 - acc: 0.990 - ETA: 9s - loss: 0.0307 - acc: 0.990 - ETA: 9s - loss: 0.0305 - acc: 0.990 - ETA: 9s - loss: 0.0305 - acc: 0.990 - ETA: 9s - loss: 0.0306 - acc: 0.990 - ETA: 9s - loss: 0.0305 - acc: 0.990 - ETA: 9s - loss: 0.0306 - acc: 0.990 - ETA: 9s - loss: 0.0310 - acc: 0.990 - ETA: 9s - loss: 0.0311 - acc: 0.990 - ETA: 9s - loss: 0.0311 - acc: 0.990 - ETA: 9s - loss: 0.0314 - acc: 0.990 - ETA: 9s - loss: 0.0313 - acc: 0.990 - ETA: 8s - loss: 0.0312 - acc: 0.990 - ETA: 8s - loss: 0.0309 - acc: 0.990 - ETA: 8s - loss: 0.0307 - acc: 0.990 - ETA: 8s - loss: 0.0310 - acc: 0.990 - ETA: 8s - loss: 0.0312 - acc: 0.990 - ETA: 8s - loss: 0.0312 - acc: 0.990 - ETA: 8s - loss: 0.0313 - acc: 0.990 - ETA: 8s - loss: 0.0313 - acc: 0.990 - ETA: 8s - loss: 0.0312 - acc: 0.990 - ETA: 8s - loss: 0.0313 - acc: 0.990 - ETA: 8s - loss: 0.0311 - acc: 0.990 - ETA: 8s - loss: 0.0309 - acc: 0.990 - ETA: 8s - loss: 0.0310 - acc: 0.990 - ETA: 8s - loss: 0.0310 - acc: 0.990 - ETA: 8s - loss: 0.0310 - acc: 0.990 - ETA: 8s - loss: 0.0312 - acc: 0.990 - ETA: 8s - loss: 0.0312 - acc: 0.990 - ETA: 7s - loss: 0.0311 - acc: 0.990 - ETA: 7s - loss: 0.0310 - acc: 0.990 - ETA: 7s - loss: 0.0310 - acc: 0.990 - ETA: 7s - loss: 0.0314 - acc: 0.990 - ETA: 7s - loss: 0.0313 - acc: 0.990 - ETA: 7s - loss: 0.0314 - acc: 0.989 - ETA: 7s - loss: 0.0314 - acc: 0.990 - ETA: 7s - loss: 0.0313 - acc: 0.990 - ETA: 7s - loss: 0.0317 - acc: 0.989 - ETA: 7s - loss: 0.0318 - acc: 0.989 - ETA: 7s - loss: 0.0319 - acc: 0.989 - ETA: 7s - loss: 0.0317 - acc: 0.989 - ETA: 7s - loss: 0.0316 - acc: 0.989 - ETA: 7s - loss: 0.0319 - acc: 0.989 - ETA: 7s - loss: 0.0317 - acc: 0.989 - ETA: 7s - loss: 0.0322 - acc: 0.989 - ETA: 7s - loss: 0.0322 - acc: 0.989 - ETA: 7s - loss: 0.0321 - acc: 0.989 - ETA: 6s - loss: 0.0322 - acc: 0.989 - ETA: 6s - loss: 0.0320 - acc: 0.989 - ETA: 6s - loss: 0.0322 - acc: 0.989 - ETA: 6s - loss: 0.0321 - acc: 0.989 - ETA: 6s - loss: 0.0323 - acc: 0.989 - ETA: 6s - loss: 0.0324 - acc: 0.989 - ETA: 6s - loss: 0.0323 - acc: 0.989 - ETA: 6s - loss: 0.0322 - acc: 0.989 - ETA: 6s - loss: 0.0325 - acc: 0.989 - ETA: 6s - loss: 0.0325 - acc: 0.989 - ETA: 6s - loss: 0.0326 - acc: 0.989 - ETA: 6s - loss: 0.0325 - acc: 0.989 - ETA: 6s - loss: 0.0326 - acc: 0.989 - ETA: 6s - loss: 0.0325 - acc: 0.989 - ETA: 6s - loss: 0.0323 - acc: 0.989 - ETA: 6s - loss: 0.0323 - acc: 0.989 - ETA: 6s - loss: 0.0322 - acc: 0.989 - ETA: 6s - loss: 0.0323 - acc: 0.989 - ETA: 5s - loss: 0.0323 - acc: 0.989 - ETA: 5s - loss: 0.0323 - acc: 0.989 - ETA: 5s - loss: 0.0322 - acc: 0.989 - ETA: 5s - loss: 0.0322 - acc: 0.989 - ETA: 5s - loss: 0.0322 - acc: 0.989 - ETA: 5s - loss: 0.0323 - acc: 0.989 - ETA: 5s - loss: 0.0322 - acc: 0.989 - ETA: 5s - loss: 0.0323 - acc: 0.989 - ETA: 5s - loss: 0.0323 - acc: 0.989 - ETA: 5s - loss: 0.0326 - acc: 0.989 - ETA: 5s - loss: 0.0325 - acc: 0.989 - ETA: 5s - loss: 0.0323 - acc: 0.989 - ETA: 5s - loss: 0.0323 - acc: 0.989 - ETA: 5s - loss: 0.0324 - acc: 0.989 - ETA: 5s - loss: 0.0322 - acc: 0.989 - ETA: 5s - loss: 0.0321 - acc: 0.989 - ETA: 5s - loss: 0.0323 - acc: 0.989 - ETA: 5s - loss: 0.0322 - acc: 0.989 - ETA: 4s - loss: 0.0322 - acc: 0.989 - ETA: 4s - loss: 0.0321 - acc: 0.989 - ETA: 4s - loss: 0.0321 - acc: 0.989 - ETA: 4s - loss: 0.0320 - acc: 0.989 - ETA: 4s - loss: 0.0320 - acc: 0.989 - ETA: 4s - loss: 0.0320 - acc: 0.989 - ETA: 4s - loss: 0.0320 - acc: 0.989 - ETA: 4s - loss: 0.0320 - acc: 0.989 - ETA: 4s - loss: 0.0321 - acc: 0.989 - ETA: 4s - loss: 0.0321 - acc: 0.989 - ETA: 4s - loss: 0.0321 - acc: 0.989 - ETA: 4s - loss: 0.0320 - acc: 0.989 - ETA: 4s - loss: 0.0320 - acc: 0.989 - ETA: 4s - loss: 0.0319 - acc: 0.989 - ETA: 4s - loss: 0.0321 - acc: 0.989 - ETA: 4s - loss: 0.0324 - acc: 0.989 - ETA: 4s - loss: 0.0326 - acc: 0.989 - ETA: 3s - loss: 0.0326 - acc: 0.989 - ETA: 3s - loss: 0.0327 - acc: 0.989 - ETA: 3s - loss: 0.0329 - acc: 0.989 - ETA: 3s - loss: 0.0328 - acc: 0.989 - ETA: 3s - loss: 0.0328 - acc: 0.989 - ETA: 3s - loss: 0.0328 - acc: 0.989 - ETA: 3s - loss: 0.0327 - acc: 0.989 - ETA: 3s - loss: 0.0326 - acc: 0.989 - ETA: 3s - loss: 0.0328 - acc: 0.989 - ETA: 3s - loss: 0.0328 - acc: 0.989 - ETA: 3s - loss: 0.0328 - acc: 0.989 - ETA: 3s - loss: 0.0327 - acc: 0.989 - ETA: 3s - loss: 0.0326 - acc: 0.989 - ETA: 3s - loss: 0.0325 - acc: 0.989 - ETA: 3s - loss: 0.0326 - acc: 0.989 - ETA: 3s - loss: 0.0325 - acc: 0.989 - ETA: 3s - loss: 0.0325 - acc: 0.989 - ETA: 2s - loss: 0.0327 - acc: 0.989 - ETA: 2s - loss: 0.0327 - acc: 0.989 - ETA: 2s - loss: 0.0326 - acc: 0.989 - ETA: 2s - loss: 0.0325 - acc: 0.989 - ETA: 2s - loss: 0.0324 - acc: 0.989 - ETA: 2s - loss: 0.0324 - acc: 0.989 - ETA: 2s - loss: 0.0323 - acc: 0.989 - ETA: 2s - loss: 0.0323 - acc: 0.989 - ETA: 2s - loss: 0.0323 - acc: 0.989 - ETA: 2s - loss: 0.0323 - acc: 0.989 - ETA: 2s - loss: 0.0324 - acc: 0.989 - ETA: 2s - loss: 0.0325 - acc: 0.989 - ETA: 2s - loss: 0.0324 - acc: 0.989 - ETA: 2s - loss: 0.0325 - acc: 0.989 - ETA: 2s - loss: 0.0325 - acc: 0.989 - ETA: 2s - loss: 0.0324 - acc: 0.989 - ETA: 2s - loss: 0.0324 - acc: 0.989 - ETA: 2s - loss: 0.0324 - acc: 0.989 - ETA: 1s - loss: 0.0324 - acc: 0.989 - ETA: 1s - loss: 0.0324 - acc: 0.989 - ETA: 1s - loss: 0.0324 - acc: 0.989 - ETA: 1s - loss: 0.0325 - acc: 0.989 - ETA: 1s - loss: 0.0325 - acc: 0.989 - ETA: 1s - loss: 0.0324 - acc: 0.989 - ETA: 1s - loss: 0.0324 - acc: 0.989 - ETA: 1s - loss: 0.0323 - acc: 0.989 - ETA: 1s - loss: 0.0323 - acc: 0.989 - ETA: 1s - loss: 0.0322 - acc: 0.989 - ETA: 1s - loss: 0.0322 - acc: 0.989 - ETA: 1s - loss: 0.0321 - acc: 0.989 - ETA: 1s - loss: 0.0321 - acc: 0.989 - ETA: 1s - loss: 0.0321 - acc: 0.989460000/60000 [==============================] - ETA: 1s - loss: 0.0322 - acc: 0.989 - ETA: 1s - loss: 0.0323 - acc: 0.989 - ETA: 0s - loss: 0.0322 - acc: 0.989 - ETA: 0s - loss: 0.0322 - acc: 0.989 - ETA: 0s - loss: 0.0321 - acc: 0.989 - ETA: 0s - loss: 0.0320 - acc: 0.989 - ETA: 0s - loss: 0.0321 - acc: 0.989 - ETA: 0s - loss: 0.0324 - acc: 0.989 - ETA: 0s - loss: 0.0325 - acc: 0.989 - ETA: 0s - loss: 0.0325 - acc: 0.989 - ETA: 0s - loss: 0.0325 - acc: 0.989 - ETA: 0s - loss: 0.0325 - acc: 0.989 - ETA: 0s - loss: 0.0325 - acc: 0.989 - ETA: 0s - loss: 0.0324 - acc: 0.989 - ETA: 0s - loss: 0.0325 - acc: 0.989 - ETA: 0s - loss: 0.0325 - acc: 0.989 - ETA: 0s - loss: 0.0324 - acc: 0.989 - ETA: 0s - loss: 0.0327 - acc: 0.989 - ETA: 0s - loss: 0.0326 - acc: 0.989 - 14s 241us/step - loss: 0.0327 - acc: 0.9894 - val_loss: 0.0281 - val_acc: 0.9910\n",
      "Epoch 10/12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54912/60000 [==========================>...] - ETA: 13s - loss: 0.0215 - acc: 0.99 - ETA: 13s - loss: 0.0460 - acc: 0.98 - ETA: 13s - loss: 0.0475 - acc: 0.98 - ETA: 13s - loss: 0.0402 - acc: 0.98 - ETA: 13s - loss: 0.0394 - acc: 0.98 - ETA: 13s - loss: 0.0355 - acc: 0.98 - ETA: 13s - loss: 0.0351 - acc: 0.98 - ETA: 13s - loss: 0.0352 - acc: 0.98 - ETA: 13s - loss: 0.0342 - acc: 0.98 - ETA: 12s - loss: 0.0325 - acc: 0.98 - ETA: 12s - loss: 0.0339 - acc: 0.98 - ETA: 12s - loss: 0.0334 - acc: 0.98 - ETA: 12s - loss: 0.0317 - acc: 0.98 - ETA: 12s - loss: 0.0306 - acc: 0.98 - ETA: 12s - loss: 0.0297 - acc: 0.98 - ETA: 12s - loss: 0.0289 - acc: 0.98 - ETA: 12s - loss: 0.0298 - acc: 0.99 - ETA: 12s - loss: 0.0293 - acc: 0.99 - ETA: 12s - loss: 0.0289 - acc: 0.98 - ETA: 12s - loss: 0.0280 - acc: 0.99 - ETA: 12s - loss: 0.0272 - acc: 0.99 - ETA: 12s - loss: 0.0268 - acc: 0.99 - ETA: 12s - loss: 0.0270 - acc: 0.99 - ETA: 12s - loss: 0.0261 - acc: 0.99 - ETA: 12s - loss: 0.0258 - acc: 0.99 - ETA: 12s - loss: 0.0290 - acc: 0.99 - ETA: 11s - loss: 0.0294 - acc: 0.99 - ETA: 11s - loss: 0.0290 - acc: 0.99 - ETA: 11s - loss: 0.0295 - acc: 0.99 - ETA: 11s - loss: 0.0298 - acc: 0.99 - ETA: 11s - loss: 0.0297 - acc: 0.99 - ETA: 11s - loss: 0.0304 - acc: 0.99 - ETA: 11s - loss: 0.0301 - acc: 0.99 - ETA: 11s - loss: 0.0302 - acc: 0.99 - ETA: 11s - loss: 0.0298 - acc: 0.99 - ETA: 11s - loss: 0.0313 - acc: 0.99 - ETA: 11s - loss: 0.0312 - acc: 0.98 - ETA: 11s - loss: 0.0310 - acc: 0.99 - ETA: 11s - loss: 0.0309 - acc: 0.99 - ETA: 11s - loss: 0.0311 - acc: 0.99 - ETA: 11s - loss: 0.0316 - acc: 0.99 - ETA: 11s - loss: 0.0320 - acc: 0.98 - ETA: 11s - loss: 0.0322 - acc: 0.98 - ETA: 11s - loss: 0.0324 - acc: 0.98 - ETA: 10s - loss: 0.0324 - acc: 0.98 - ETA: 10s - loss: 0.0320 - acc: 0.98 - ETA: 10s - loss: 0.0317 - acc: 0.98 - ETA: 10s - loss: 0.0313 - acc: 0.99 - ETA: 10s - loss: 0.0309 - acc: 0.99 - ETA: 10s - loss: 0.0311 - acc: 0.99 - ETA: 10s - loss: 0.0319 - acc: 0.98 - ETA: 10s - loss: 0.0327 - acc: 0.99 - ETA: 10s - loss: 0.0325 - acc: 0.99 - ETA: 10s - loss: 0.0321 - acc: 0.99 - ETA: 10s - loss: 0.0318 - acc: 0.99 - ETA: 10s - loss: 0.0318 - acc: 0.99 - ETA: 10s - loss: 0.0319 - acc: 0.99 - ETA: 10s - loss: 0.0323 - acc: 0.99 - ETA: 10s - loss: 0.0325 - acc: 0.99 - ETA: 10s - loss: 0.0321 - acc: 0.99 - ETA: 10s - loss: 0.0323 - acc: 0.99 - ETA: 9s - loss: 0.0320 - acc: 0.9903 - ETA: 9s - loss: 0.0320 - acc: 0.990 - ETA: 9s - loss: 0.0316 - acc: 0.990 - ETA: 9s - loss: 0.0313 - acc: 0.990 - ETA: 9s - loss: 0.0313 - acc: 0.990 - ETA: 9s - loss: 0.0310 - acc: 0.990 - ETA: 9s - loss: 0.0308 - acc: 0.990 - ETA: 9s - loss: 0.0306 - acc: 0.990 - ETA: 9s - loss: 0.0307 - acc: 0.990 - ETA: 9s - loss: 0.0306 - acc: 0.990 - ETA: 9s - loss: 0.0303 - acc: 0.990 - ETA: 9s - loss: 0.0299 - acc: 0.990 - ETA: 9s - loss: 0.0298 - acc: 0.990 - ETA: 9s - loss: 0.0296 - acc: 0.990 - ETA: 9s - loss: 0.0294 - acc: 0.990 - ETA: 9s - loss: 0.0296 - acc: 0.990 - ETA: 9s - loss: 0.0296 - acc: 0.990 - ETA: 8s - loss: 0.0295 - acc: 0.990 - ETA: 8s - loss: 0.0296 - acc: 0.990 - ETA: 8s - loss: 0.0296 - acc: 0.990 - ETA: 8s - loss: 0.0294 - acc: 0.990 - ETA: 8s - loss: 0.0292 - acc: 0.991 - ETA: 8s - loss: 0.0293 - acc: 0.991 - ETA: 8s - loss: 0.0291 - acc: 0.991 - ETA: 8s - loss: 0.0293 - acc: 0.991 - ETA: 8s - loss: 0.0291 - acc: 0.991 - ETA: 8s - loss: 0.0288 - acc: 0.991 - ETA: 8s - loss: 0.0286 - acc: 0.991 - ETA: 8s - loss: 0.0286 - acc: 0.991 - ETA: 8s - loss: 0.0285 - acc: 0.991 - ETA: 8s - loss: 0.0293 - acc: 0.991 - ETA: 8s - loss: 0.0293 - acc: 0.991 - ETA: 8s - loss: 0.0291 - acc: 0.991 - ETA: 8s - loss: 0.0290 - acc: 0.991 - ETA: 8s - loss: 0.0293 - acc: 0.991 - ETA: 7s - loss: 0.0295 - acc: 0.991 - ETA: 7s - loss: 0.0293 - acc: 0.991 - ETA: 7s - loss: 0.0292 - acc: 0.991 - ETA: 7s - loss: 0.0293 - acc: 0.991 - ETA: 7s - loss: 0.0291 - acc: 0.991 - ETA: 7s - loss: 0.0300 - acc: 0.990 - ETA: 7s - loss: 0.0300 - acc: 0.990 - ETA: 7s - loss: 0.0298 - acc: 0.990 - ETA: 7s - loss: 0.0296 - acc: 0.991 - ETA: 7s - loss: 0.0296 - acc: 0.990 - ETA: 7s - loss: 0.0296 - acc: 0.990 - ETA: 7s - loss: 0.0296 - acc: 0.990 - ETA: 7s - loss: 0.0297 - acc: 0.990 - ETA: 7s - loss: 0.0297 - acc: 0.990 - ETA: 7s - loss: 0.0297 - acc: 0.990 - ETA: 7s - loss: 0.0296 - acc: 0.990 - ETA: 7s - loss: 0.0296 - acc: 0.990 - ETA: 6s - loss: 0.0296 - acc: 0.991 - ETA: 6s - loss: 0.0294 - acc: 0.991 - ETA: 6s - loss: 0.0293 - acc: 0.991 - ETA: 6s - loss: 0.0292 - acc: 0.991 - ETA: 6s - loss: 0.0292 - acc: 0.991 - ETA: 6s - loss: 0.0291 - acc: 0.991 - ETA: 6s - loss: 0.0290 - acc: 0.991 - ETA: 6s - loss: 0.0289 - acc: 0.991 - ETA: 6s - loss: 0.0288 - acc: 0.991 - ETA: 6s - loss: 0.0289 - acc: 0.991 - ETA: 6s - loss: 0.0288 - acc: 0.991 - ETA: 6s - loss: 0.0288 - acc: 0.991 - ETA: 6s - loss: 0.0288 - acc: 0.991 - ETA: 6s - loss: 0.0287 - acc: 0.991 - ETA: 6s - loss: 0.0287 - acc: 0.991 - ETA: 6s - loss: 0.0291 - acc: 0.990 - ETA: 6s - loss: 0.0293 - acc: 0.990 - ETA: 5s - loss: 0.0293 - acc: 0.990 - ETA: 5s - loss: 0.0293 - acc: 0.990 - ETA: 5s - loss: 0.0293 - acc: 0.990 - ETA: 5s - loss: 0.0294 - acc: 0.990 - ETA: 5s - loss: 0.0293 - acc: 0.990 - ETA: 5s - loss: 0.0292 - acc: 0.991 - ETA: 5s - loss: 0.0290 - acc: 0.991 - ETA: 5s - loss: 0.0294 - acc: 0.990 - ETA: 5s - loss: 0.0293 - acc: 0.990 - ETA: 5s - loss: 0.0292 - acc: 0.991 - ETA: 5s - loss: 0.0291 - acc: 0.991 - ETA: 5s - loss: 0.0291 - acc: 0.991 - ETA: 5s - loss: 0.0291 - acc: 0.991 - ETA: 5s - loss: 0.0291 - acc: 0.991 - ETA: 5s - loss: 0.0293 - acc: 0.990 - ETA: 5s - loss: 0.0292 - acc: 0.991 - ETA: 5s - loss: 0.0292 - acc: 0.991 - ETA: 5s - loss: 0.0292 - acc: 0.990 - ETA: 4s - loss: 0.0292 - acc: 0.990 - ETA: 4s - loss: 0.0293 - acc: 0.990 - ETA: 4s - loss: 0.0292 - acc: 0.990 - ETA: 4s - loss: 0.0291 - acc: 0.990 - ETA: 4s - loss: 0.0291 - acc: 0.991 - ETA: 4s - loss: 0.0290 - acc: 0.991 - ETA: 4s - loss: 0.0289 - acc: 0.991 - ETA: 4s - loss: 0.0288 - acc: 0.991 - ETA: 4s - loss: 0.0287 - acc: 0.991 - ETA: 4s - loss: 0.0286 - acc: 0.991 - ETA: 4s - loss: 0.0285 - acc: 0.991 - ETA: 4s - loss: 0.0284 - acc: 0.991 - ETA: 4s - loss: 0.0286 - acc: 0.991 - ETA: 4s - loss: 0.0286 - acc: 0.991 - ETA: 4s - loss: 0.0287 - acc: 0.991 - ETA: 4s - loss: 0.0287 - acc: 0.991 - ETA: 4s - loss: 0.0287 - acc: 0.991 - ETA: 3s - loss: 0.0286 - acc: 0.991 - ETA: 3s - loss: 0.0286 - acc: 0.991 - ETA: 3s - loss: 0.0286 - acc: 0.991 - ETA: 3s - loss: 0.0285 - acc: 0.991 - ETA: 3s - loss: 0.0285 - acc: 0.991 - ETA: 3s - loss: 0.0284 - acc: 0.991 - ETA: 3s - loss: 0.0285 - acc: 0.991 - ETA: 3s - loss: 0.0288 - acc: 0.991 - ETA: 3s - loss: 0.0288 - acc: 0.991 - ETA: 3s - loss: 0.0288 - acc: 0.991 - ETA: 3s - loss: 0.0288 - acc: 0.991 - ETA: 3s - loss: 0.0287 - acc: 0.991 - ETA: 3s - loss: 0.0288 - acc: 0.991 - ETA: 3s - loss: 0.0288 - acc: 0.991 - ETA: 3s - loss: 0.0289 - acc: 0.991 - ETA: 3s - loss: 0.0288 - acc: 0.991 - ETA: 3s - loss: 0.0289 - acc: 0.991 - ETA: 2s - loss: 0.0288 - acc: 0.991 - ETA: 2s - loss: 0.0288 - acc: 0.991 - ETA: 2s - loss: 0.0290 - acc: 0.991 - ETA: 2s - loss: 0.0291 - acc: 0.990 - ETA: 2s - loss: 0.0290 - acc: 0.991 - ETA: 2s - loss: 0.0292 - acc: 0.990 - ETA: 2s - loss: 0.0292 - acc: 0.990 - ETA: 2s - loss: 0.0291 - acc: 0.990 - ETA: 2s - loss: 0.0292 - acc: 0.990 - ETA: 2s - loss: 0.0292 - acc: 0.990 - ETA: 2s - loss: 0.0292 - acc: 0.990 - ETA: 2s - loss: 0.0291 - acc: 0.991 - ETA: 2s - loss: 0.0290 - acc: 0.991 - ETA: 2s - loss: 0.0289 - acc: 0.991 - ETA: 2s - loss: 0.0289 - acc: 0.991 - ETA: 2s - loss: 0.0289 - acc: 0.991 - ETA: 2s - loss: 0.0289 - acc: 0.991 - ETA: 2s - loss: 0.0292 - acc: 0.991 - ETA: 1s - loss: 0.0291 - acc: 0.991 - ETA: 1s - loss: 0.0292 - acc: 0.991 - ETA: 1s - loss: 0.0292 - acc: 0.990 - ETA: 1s - loss: 0.0292 - acc: 0.990 - ETA: 1s - loss: 0.0293 - acc: 0.990 - ETA: 1s - loss: 0.0293 - acc: 0.990 - ETA: 1s - loss: 0.0294 - acc: 0.990 - ETA: 1s - loss: 0.0294 - acc: 0.990 - ETA: 1s - loss: 0.0294 - acc: 0.990 - ETA: 1s - loss: 0.0295 - acc: 0.990 - ETA: 1s - loss: 0.0296 - acc: 0.990 - ETA: 1s - loss: 0.0297 - acc: 0.990 - ETA: 1s - loss: 0.0296 - acc: 0.990 - ETA: 1s - loss: 0.0295 - acc: 0.990 - ETA: 1s - loss: 0.0294 - acc: 0.9908"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - ETA: 1s - loss: 0.0294 - acc: 0.990 - ETA: 1s - loss: 0.0295 - acc: 0.990 - ETA: 0s - loss: 0.0295 - acc: 0.990 - ETA: 0s - loss: 0.0296 - acc: 0.990 - ETA: 0s - loss: 0.0295 - acc: 0.990 - ETA: 0s - loss: 0.0294 - acc: 0.990 - ETA: 0s - loss: 0.0295 - acc: 0.990 - ETA: 0s - loss: 0.0295 - acc: 0.990 - ETA: 0s - loss: 0.0295 - acc: 0.990 - ETA: 0s - loss: 0.0296 - acc: 0.990 - ETA: 0s - loss: 0.0295 - acc: 0.990 - ETA: 0s - loss: 0.0295 - acc: 0.990 - ETA: 0s - loss: 0.0297 - acc: 0.990 - ETA: 0s - loss: 0.0298 - acc: 0.990 - ETA: 0s - loss: 0.0297 - acc: 0.990 - ETA: 0s - loss: 0.0296 - acc: 0.990 - ETA: 0s - loss: 0.0295 - acc: 0.990 - ETA: 0s - loss: 0.0295 - acc: 0.990 - ETA: 0s - loss: 0.0297 - acc: 0.990 - 14s 235us/step - loss: 0.0296 - acc: 0.9909 - val_loss: 0.0274 - val_acc: 0.9914\n",
      "Epoch 11/12\n",
      "57088/60000 [===========================>..] - ETA: 13s - loss: 0.0198 - acc: 0.99 - ETA: 13s - loss: 0.0327 - acc: 0.99 - ETA: 13s - loss: 0.0267 - acc: 0.99 - ETA: 13s - loss: 0.0243 - acc: 0.99 - ETA: 13s - loss: 0.0237 - acc: 0.99 - ETA: 13s - loss: 0.0316 - acc: 0.98 - ETA: 13s - loss: 0.0338 - acc: 0.98 - ETA: 12s - loss: 0.0310 - acc: 0.99 - ETA: 12s - loss: 0.0330 - acc: 0.99 - ETA: 12s - loss: 0.0320 - acc: 0.99 - ETA: 12s - loss: 0.0318 - acc: 0.99 - ETA: 12s - loss: 0.0305 - acc: 0.99 - ETA: 12s - loss: 0.0299 - acc: 0.99 - ETA: 12s - loss: 0.0282 - acc: 0.99 - ETA: 12s - loss: 0.0292 - acc: 0.99 - ETA: 12s - loss: 0.0286 - acc: 0.99 - ETA: 12s - loss: 0.0291 - acc: 0.99 - ETA: 12s - loss: 0.0283 - acc: 0.99 - ETA: 12s - loss: 0.0277 - acc: 0.99 - ETA: 12s - loss: 0.0277 - acc: 0.99 - ETA: 12s - loss: 0.0280 - acc: 0.99 - ETA: 12s - loss: 0.0280 - acc: 0.99 - ETA: 12s - loss: 0.0270 - acc: 0.99 - ETA: 12s - loss: 0.0263 - acc: 0.99 - ETA: 12s - loss: 0.0255 - acc: 0.99 - ETA: 11s - loss: 0.0252 - acc: 0.99 - ETA: 11s - loss: 0.0246 - acc: 0.99 - ETA: 11s - loss: 0.0249 - acc: 0.99 - ETA: 11s - loss: 0.0243 - acc: 0.99 - ETA: 11s - loss: 0.0245 - acc: 0.99 - ETA: 11s - loss: 0.0240 - acc: 0.99 - ETA: 11s - loss: 0.0240 - acc: 0.99 - ETA: 11s - loss: 0.0238 - acc: 0.99 - ETA: 11s - loss: 0.0237 - acc: 0.99 - ETA: 11s - loss: 0.0249 - acc: 0.99 - ETA: 11s - loss: 0.0249 - acc: 0.99 - ETA: 11s - loss: 0.0250 - acc: 0.99 - ETA: 11s - loss: 0.0247 - acc: 0.99 - ETA: 11s - loss: 0.0246 - acc: 0.99 - ETA: 11s - loss: 0.0249 - acc: 0.99 - ETA: 11s - loss: 0.0251 - acc: 0.99 - ETA: 11s - loss: 0.0255 - acc: 0.99 - ETA: 11s - loss: 0.0262 - acc: 0.99 - ETA: 10s - loss: 0.0265 - acc: 0.99 - ETA: 10s - loss: 0.0269 - acc: 0.99 - ETA: 10s - loss: 0.0270 - acc: 0.99 - ETA: 10s - loss: 0.0267 - acc: 0.99 - ETA: 10s - loss: 0.0268 - acc: 0.99 - ETA: 10s - loss: 0.0276 - acc: 0.99 - ETA: 10s - loss: 0.0272 - acc: 0.99 - ETA: 10s - loss: 0.0272 - acc: 0.99 - ETA: 10s - loss: 0.0269 - acc: 0.99 - ETA: 10s - loss: 0.0267 - acc: 0.99 - ETA: 10s - loss: 0.0270 - acc: 0.99 - ETA: 10s - loss: 0.0271 - acc: 0.99 - ETA: 10s - loss: 0.0284 - acc: 0.99 - ETA: 10s - loss: 0.0282 - acc: 0.99 - ETA: 10s - loss: 0.0282 - acc: 0.99 - ETA: 10s - loss: 0.0280 - acc: 0.99 - ETA: 10s - loss: 0.0282 - acc: 0.99 - ETA: 10s - loss: 0.0281 - acc: 0.99 - ETA: 9s - loss: 0.0284 - acc: 0.9903 - ETA: 9s - loss: 0.0282 - acc: 0.990 - ETA: 9s - loss: 0.0279 - acc: 0.990 - ETA: 9s - loss: 0.0279 - acc: 0.990 - ETA: 9s - loss: 0.0279 - acc: 0.990 - ETA: 9s - loss: 0.0278 - acc: 0.990 - ETA: 9s - loss: 0.0277 - acc: 0.990 - ETA: 9s - loss: 0.0284 - acc: 0.990 - ETA: 9s - loss: 0.0282 - acc: 0.990 - ETA: 9s - loss: 0.0280 - acc: 0.990 - ETA: 9s - loss: 0.0279 - acc: 0.990 - ETA: 9s - loss: 0.0279 - acc: 0.990 - ETA: 9s - loss: 0.0278 - acc: 0.990 - ETA: 9s - loss: 0.0275 - acc: 0.990 - ETA: 9s - loss: 0.0273 - acc: 0.990 - ETA: 9s - loss: 0.0278 - acc: 0.990 - ETA: 9s - loss: 0.0279 - acc: 0.990 - ETA: 8s - loss: 0.0277 - acc: 0.990 - ETA: 8s - loss: 0.0274 - acc: 0.991 - ETA: 8s - loss: 0.0278 - acc: 0.990 - ETA: 8s - loss: 0.0276 - acc: 0.990 - ETA: 8s - loss: 0.0275 - acc: 0.991 - ETA: 8s - loss: 0.0274 - acc: 0.991 - ETA: 8s - loss: 0.0271 - acc: 0.991 - ETA: 8s - loss: 0.0272 - acc: 0.991 - ETA: 8s - loss: 0.0274 - acc: 0.990 - ETA: 8s - loss: 0.0273 - acc: 0.990 - ETA: 8s - loss: 0.0272 - acc: 0.990 - ETA: 8s - loss: 0.0277 - acc: 0.990 - ETA: 8s - loss: 0.0277 - acc: 0.990 - ETA: 8s - loss: 0.0276 - acc: 0.990 - ETA: 8s - loss: 0.0276 - acc: 0.990 - ETA: 8s - loss: 0.0276 - acc: 0.990 - ETA: 8s - loss: 0.0276 - acc: 0.990 - ETA: 7s - loss: 0.0276 - acc: 0.990 - ETA: 7s - loss: 0.0276 - acc: 0.990 - ETA: 7s - loss: 0.0277 - acc: 0.990 - ETA: 7s - loss: 0.0281 - acc: 0.990 - ETA: 7s - loss: 0.0282 - acc: 0.990 - ETA: 7s - loss: 0.0282 - acc: 0.990 - ETA: 7s - loss: 0.0280 - acc: 0.990 - ETA: 7s - loss: 0.0281 - acc: 0.990 - ETA: 7s - loss: 0.0284 - acc: 0.990 - ETA: 7s - loss: 0.0287 - acc: 0.990 - ETA: 7s - loss: 0.0286 - acc: 0.990 - ETA: 7s - loss: 0.0285 - acc: 0.990 - ETA: 7s - loss: 0.0284 - acc: 0.990 - ETA: 7s - loss: 0.0282 - acc: 0.990 - ETA: 7s - loss: 0.0282 - acc: 0.990 - ETA: 7s - loss: 0.0281 - acc: 0.990 - ETA: 7s - loss: 0.0279 - acc: 0.990 - ETA: 6s - loss: 0.0278 - acc: 0.990 - ETA: 6s - loss: 0.0279 - acc: 0.990 - ETA: 6s - loss: 0.0281 - acc: 0.990 - ETA: 6s - loss: 0.0280 - acc: 0.990 - ETA: 6s - loss: 0.0278 - acc: 0.990 - ETA: 6s - loss: 0.0283 - acc: 0.990 - ETA: 6s - loss: 0.0284 - acc: 0.990 - ETA: 6s - loss: 0.0285 - acc: 0.990 - ETA: 6s - loss: 0.0287 - acc: 0.990 - ETA: 6s - loss: 0.0286 - acc: 0.990 - ETA: 6s - loss: 0.0285 - acc: 0.990 - ETA: 6s - loss: 0.0284 - acc: 0.990 - ETA: 6s - loss: 0.0284 - acc: 0.990 - ETA: 6s - loss: 0.0283 - acc: 0.990 - ETA: 6s - loss: 0.0284 - acc: 0.990 - ETA: 6s - loss: 0.0282 - acc: 0.990 - ETA: 6s - loss: 0.0283 - acc: 0.990 - ETA: 6s - loss: 0.0285 - acc: 0.990 - ETA: 5s - loss: 0.0285 - acc: 0.990 - ETA: 5s - loss: 0.0284 - acc: 0.990 - ETA: 5s - loss: 0.0286 - acc: 0.990 - ETA: 5s - loss: 0.0288 - acc: 0.990 - ETA: 5s - loss: 0.0289 - acc: 0.990 - ETA: 5s - loss: 0.0288 - acc: 0.990 - ETA: 5s - loss: 0.0289 - acc: 0.990 - ETA: 5s - loss: 0.0290 - acc: 0.990 - ETA: 5s - loss: 0.0289 - acc: 0.990 - ETA: 5s - loss: 0.0288 - acc: 0.990 - ETA: 5s - loss: 0.0289 - acc: 0.990 - ETA: 5s - loss: 0.0290 - acc: 0.990 - ETA: 5s - loss: 0.0290 - acc: 0.990 - ETA: 5s - loss: 0.0289 - acc: 0.990 - ETA: 5s - loss: 0.0291 - acc: 0.990 - ETA: 5s - loss: 0.0290 - acc: 0.990 - ETA: 4s - loss: 0.0292 - acc: 0.990 - ETA: 4s - loss: 0.0292 - acc: 0.990 - ETA: 4s - loss: 0.0292 - acc: 0.990 - ETA: 4s - loss: 0.0293 - acc: 0.990 - ETA: 4s - loss: 0.0292 - acc: 0.990 - ETA: 4s - loss: 0.0290 - acc: 0.990 - ETA: 4s - loss: 0.0289 - acc: 0.990 - ETA: 4s - loss: 0.0289 - acc: 0.990 - ETA: 4s - loss: 0.0289 - acc: 0.990 - ETA: 4s - loss: 0.0290 - acc: 0.990 - ETA: 4s - loss: 0.0290 - acc: 0.990 - ETA: 4s - loss: 0.0290 - acc: 0.990 - ETA: 4s - loss: 0.0289 - acc: 0.990 - ETA: 4s - loss: 0.0288 - acc: 0.990 - ETA: 3s - loss: 0.0287 - acc: 0.990 - ETA: 3s - loss: 0.0286 - acc: 0.990 - ETA: 3s - loss: 0.0286 - acc: 0.991 - ETA: 3s - loss: 0.0288 - acc: 0.991 - ETA: 3s - loss: 0.0288 - acc: 0.990 - ETA: 3s - loss: 0.0287 - acc: 0.991 - ETA: 3s - loss: 0.0288 - acc: 0.990 - ETA: 3s - loss: 0.0286 - acc: 0.991 - ETA: 3s - loss: 0.0285 - acc: 0.991 - ETA: 3s - loss: 0.0284 - acc: 0.991 - ETA: 3s - loss: 0.0283 - acc: 0.991 - ETA: 3s - loss: 0.0284 - acc: 0.991 - ETA: 3s - loss: 0.0287 - acc: 0.990 - ETA: 3s - loss: 0.0287 - acc: 0.990 - ETA: 2s - loss: 0.0290 - acc: 0.990 - ETA: 2s - loss: 0.0290 - acc: 0.990 - ETA: 2s - loss: 0.0289 - acc: 0.990 - ETA: 2s - loss: 0.0289 - acc: 0.990 - ETA: 2s - loss: 0.0289 - acc: 0.990 - ETA: 2s - loss: 0.0288 - acc: 0.990 - ETA: 2s - loss: 0.0288 - acc: 0.990 - ETA: 2s - loss: 0.0287 - acc: 0.990 - ETA: 2s - loss: 0.0288 - acc: 0.990 - ETA: 2s - loss: 0.0287 - acc: 0.990 - ETA: 2s - loss: 0.0286 - acc: 0.990 - ETA: 2s - loss: 0.0286 - acc: 0.990 - ETA: 2s - loss: 0.0285 - acc: 0.990 - ETA: 2s - loss: 0.0285 - acc: 0.990 - ETA: 2s - loss: 0.0286 - acc: 0.990 - ETA: 2s - loss: 0.0285 - acc: 0.991 - ETA: 2s - loss: 0.0285 - acc: 0.991 - ETA: 1s - loss: 0.0284 - acc: 0.991 - ETA: 1s - loss: 0.0283 - acc: 0.991 - ETA: 1s - loss: 0.0282 - acc: 0.991 - ETA: 1s - loss: 0.0283 - acc: 0.991 - ETA: 1s - loss: 0.0282 - acc: 0.991 - ETA: 1s - loss: 0.0283 - acc: 0.990 - ETA: 1s - loss: 0.0284 - acc: 0.990 - ETA: 1s - loss: 0.0284 - acc: 0.990 - ETA: 1s - loss: 0.0283 - acc: 0.990 - ETA: 1s - loss: 0.0283 - acc: 0.990 - ETA: 1s - loss: 0.0283 - acc: 0.990 - ETA: 1s - loss: 0.0282 - acc: 0.990 - ETA: 1s - loss: 0.0282 - acc: 0.990 - ETA: 1s - loss: 0.0281 - acc: 0.990 - ETA: 1s - loss: 0.0280 - acc: 0.991 - ETA: 1s - loss: 0.0280 - acc: 0.991 - ETA: 1s - loss: 0.0280 - acc: 0.991 - ETA: 0s - loss: 0.0280 - acc: 0.991 - ETA: 0s - loss: 0.0280 - acc: 0.991 - ETA: 0s - loss: 0.0280 - acc: 0.991 - ETA: 0s - loss: 0.0279 - acc: 0.991 - ETA: 0s - loss: 0.0281 - acc: 0.990 - ETA: 0s - loss: 0.0281 - acc: 0.990 - ETA: 0s - loss: 0.0281 - acc: 0.990960000/60000 [==============================] - ETA: 0s - loss: 0.0281 - acc: 0.990 - ETA: 0s - loss: 0.0280 - acc: 0.990 - ETA: 0s - loss: 0.0279 - acc: 0.991 - ETA: 0s - loss: 0.0278 - acc: 0.991 - ETA: 0s - loss: 0.0279 - acc: 0.991 - ETA: 0s - loss: 0.0279 - acc: 0.991 - ETA: 0s - loss: 0.0278 - acc: 0.991 - ETA: 0s - loss: 0.0278 - acc: 0.991 - ETA: 0s - loss: 0.0277 - acc: 0.991 - ETA: 0s - loss: 0.0278 - acc: 0.991 - ETA: 0s - loss: 0.0278 - acc: 0.991 - 14s 236us/step - loss: 0.0278 - acc: 0.9910 - val_loss: 0.0297 - val_acc: 0.9917\n",
      "Epoch 12/12\n",
      "12416/60000 [=====>........................] - ETA: 16s - loss: 0.0058 - acc: 1.00 - ETA: 15s - loss: 0.0169 - acc: 0.99 - ETA: 15s - loss: 0.0198 - acc: 0.99 - ETA: 15s - loss: 0.0234 - acc: 0.99 - ETA: 15s - loss: 0.0228 - acc: 0.99 - ETA: 15s - loss: 0.0240 - acc: 0.99 - ETA: 15s - loss: 0.0230 - acc: 0.99 - ETA: 15s - loss: 0.0219 - acc: 0.99 - ETA: 15s - loss: 0.0213 - acc: 0.99 - ETA: 14s - loss: 0.0243 - acc: 0.99 - ETA: 14s - loss: 0.0232 - acc: 0.99 - ETA: 14s - loss: 0.0240 - acc: 0.99 - ETA: 14s - loss: 0.0231 - acc: 0.99 - ETA: 13s - loss: 0.0249 - acc: 0.99 - ETA: 13s - loss: 0.0239 - acc: 0.99 - ETA: 13s - loss: 0.0247 - acc: 0.99 - ETA: 13s - loss: 0.0235 - acc: 0.99 - ETA: 13s - loss: 0.0226 - acc: 0.99 - ETA: 13s - loss: 0.0217 - acc: 0.99 - ETA: 13s - loss: 0.0216 - acc: 0.99 - ETA: 13s - loss: 0.0216 - acc: 0.99 - ETA: 13s - loss: 0.0217 - acc: 0.99 - ETA: 13s - loss: 0.0215 - acc: 0.99 - ETA: 13s - loss: 0.0217 - acc: 0.99 - ETA: 13s - loss: 0.0213 - acc: 0.99 - ETA: 13s - loss: 0.0216 - acc: 0.99 - ETA: 12s - loss: 0.0214 - acc: 0.99 - ETA: 12s - loss: 0.0211 - acc: 0.99 - ETA: 12s - loss: 0.0209 - acc: 0.99 - ETA: 12s - loss: 0.0204 - acc: 0.99 - ETA: 12s - loss: 0.0209 - acc: 0.99 - ETA: 12s - loss: 0.0216 - acc: 0.99 - ETA: 12s - loss: 0.0220 - acc: 0.99 - ETA: 12s - loss: 0.0224 - acc: 0.99 - ETA: 12s - loss: 0.0231 - acc: 0.99 - ETA: 12s - loss: 0.0233 - acc: 0.99 - ETA: 12s - loss: 0.0234 - acc: 0.99 - ETA: 12s - loss: 0.0232 - acc: 0.99 - ETA: 11s - loss: 0.0232 - acc: 0.99 - ETA: 11s - loss: 0.0227 - acc: 0.99 - ETA: 11s - loss: 0.0232 - acc: 0.99 - ETA: 11s - loss: 0.0229 - acc: 0.99 - ETA: 11s - loss: 0.0226 - acc: 0.99 - ETA: 11s - loss: 0.0229 - acc: 0.99 - ETA: 11s - loss: 0.0226 - acc: 0.99 - ETA: 11s - loss: 0.0228 - acc: 0.99 - ETA: 11s - loss: 0.0232 - acc: 0.99 - ETA: 11s - loss: 0.0246 - acc: 0.99 - ETA: 11s - loss: 0.0244 - acc: 0.9923"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-ea19096a25ab>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     25\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m           \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m           validation_data=(x_test, y_test))\n\u001b[0m",
      "\u001b[1;32mc:\\users\\guillaume\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m   1637\u001b[0m           \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1638\u001b[0m           \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1639\u001b[1;33m           validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1640\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1641\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32mc:\\users\\guillaume\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    213\u001b[0m           \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 215\u001b[1;33m         \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    216\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    217\u001b[0m           \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\guillaume\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2984\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2985\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[1;32m-> 2986\u001b[1;33m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[0;32m   2987\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2988\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\guillaume\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1439\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1440\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "input_shape = (28,28,1)\n",
    "batch_size = 128\n",
    "epochs = 12\n",
    "\n",
    "def net(input_shape, num_classes=10):\n",
    "    in_ = tf.keras.Input(input_shape)\n",
    "    x = tf.keras.layers.Conv2D(32, (3,3), activation='relu')(in_)\n",
    "    x = tf.keras.layers.Conv2D(64, (3,3), activation='relu')(x)\n",
    "    x = tf.keras.layers.MaxPooling2D((2,2))(x)\n",
    "    x = tf.keras.layers.Dropout(0.25)(x)\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dropout(0.5)(x)\n",
    "    out_ = tf.keras.layers.Dense(num_classes, activation='softmax')(x)\n",
    "    return tf.keras.Model(inputs=in_,outputs=out_)\n",
    "\n",
    "model = net(input_shape)\n",
    "\n",
    "model.compile(loss=tf.keras.losses.categorical_crossentropy,\n",
    "              optimizer=tf.keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "image_input (InputLayer)     (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 24, 24, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 128)               1179776   \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "out (Dense)                  (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 1,199,882\n",
      "Trainable params: 1,199,882\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "(60000, 28, 28, 1)\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/12\n",
      "56704/60000 [===========================>..] - ETA: 12:30 - loss: 2.3254 - acc: 0.08 - ETA: 4:18 - loss: 2.1928 - acc: 0.2005 - ETA: 2:41 - loss: 2.0791 - acc: 0.270 - ETA: 1:58 - loss: 1.9699 - acc: 0.313 - ETA: 1:34 - loss: 1.8886 - acc: 0.336 - ETA: 1:19 - loss: 1.7878 - acc: 0.377 - ETA: 1:09 - loss: 1.6999 - acc: 0.416 - ETA: 1:01 - loss: 1.6207 - acc: 0.442 - ETA: 55s - loss: 1.5702 - acc: 0.463 - ETA: 50s - loss: 1.4841 - acc: 0.49 - ETA: 47s - loss: 1.4377 - acc: 0.51 - ETA: 43s - loss: 1.3923 - acc: 0.52 - ETA: 41s - loss: 1.3445 - acc: 0.54 - ETA: 39s - loss: 1.3071 - acc: 0.55 - ETA: 36s - loss: 1.2655 - acc: 0.57 - ETA: 35s - loss: 1.2186 - acc: 0.58 - ETA: 33s - loss: 1.1705 - acc: 0.60 - ETA: 31s - loss: 1.1410 - acc: 0.61 - ETA: 30s - loss: 1.1101 - acc: 0.62 - ETA: 29s - loss: 1.0781 - acc: 0.63 - ETA: 28s - loss: 1.0577 - acc: 0.64 - ETA: 28s - loss: 1.0283 - acc: 0.65 - ETA: 27s - loss: 1.0056 - acc: 0.66 - ETA: 26s - loss: 0.9818 - acc: 0.67 - ETA: 26s - loss: 0.9615 - acc: 0.67 - ETA: 25s - loss: 0.9301 - acc: 0.68 - ETA: 24s - loss: 0.9115 - acc: 0.69 - ETA: 23s - loss: 0.8930 - acc: 0.70 - ETA: 23s - loss: 0.8705 - acc: 0.70 - ETA: 22s - loss: 0.8561 - acc: 0.71 - ETA: 22s - loss: 0.8399 - acc: 0.72 - ETA: 21s - loss: 0.8255 - acc: 0.72 - ETA: 21s - loss: 0.8118 - acc: 0.73 - ETA: 21s - loss: 0.7984 - acc: 0.73 - ETA: 20s - loss: 0.7832 - acc: 0.74 - ETA: 20s - loss: 0.7706 - acc: 0.74 - ETA: 20s - loss: 0.7603 - acc: 0.75 - ETA: 19s - loss: 0.7483 - acc: 0.75 - ETA: 19s - loss: 0.7377 - acc: 0.75 - ETA: 19s - loss: 0.7286 - acc: 0.76 - ETA: 18s - loss: 0.7180 - acc: 0.76 - ETA: 18s - loss: 0.7113 - acc: 0.76 - ETA: 18s - loss: 0.7015 - acc: 0.77 - ETA: 18s - loss: 0.6924 - acc: 0.77 - ETA: 17s - loss: 0.6840 - acc: 0.77 - ETA: 17s - loss: 0.6761 - acc: 0.77 - ETA: 17s - loss: 0.6636 - acc: 0.78 - ETA: 16s - loss: 0.6527 - acc: 0.78 - ETA: 16s - loss: 0.6455 - acc: 0.79 - ETA: 16s - loss: 0.6371 - acc: 0.79 - ETA: 16s - loss: 0.6309 - acc: 0.79 - ETA: 16s - loss: 0.6249 - acc: 0.79 - ETA: 15s - loss: 0.6200 - acc: 0.79 - ETA: 15s - loss: 0.6131 - acc: 0.80 - ETA: 15s - loss: 0.6065 - acc: 0.80 - ETA: 15s - loss: 0.6018 - acc: 0.80 - ETA: 15s - loss: 0.5963 - acc: 0.80 - ETA: 15s - loss: 0.5904 - acc: 0.80 - ETA: 14s - loss: 0.5844 - acc: 0.81 - ETA: 14s - loss: 0.5794 - acc: 0.81 - ETA: 14s - loss: 0.5745 - acc: 0.81 - ETA: 14s - loss: 0.5688 - acc: 0.81 - ETA: 14s - loss: 0.5635 - acc: 0.81 - ETA: 14s - loss: 0.5585 - acc: 0.82 - ETA: 14s - loss: 0.5528 - acc: 0.82 - ETA: 13s - loss: 0.5474 - acc: 0.82 - ETA: 13s - loss: 0.5422 - acc: 0.82 - ETA: 13s - loss: 0.5376 - acc: 0.82 - ETA: 13s - loss: 0.5326 - acc: 0.82 - ETA: 13s - loss: 0.5287 - acc: 0.83 - ETA: 13s - loss: 0.5247 - acc: 0.83 - ETA: 13s - loss: 0.5213 - acc: 0.83 - ETA: 12s - loss: 0.5164 - acc: 0.83 - ETA: 12s - loss: 0.5123 - acc: 0.83 - ETA: 12s - loss: 0.5074 - acc: 0.83 - ETA: 12s - loss: 0.5041 - acc: 0.83 - ETA: 12s - loss: 0.4999 - acc: 0.84 - ETA: 12s - loss: 0.4962 - acc: 0.84 - ETA: 12s - loss: 0.4933 - acc: 0.84 - ETA: 12s - loss: 0.4900 - acc: 0.84 - ETA: 12s - loss: 0.4860 - acc: 0.84 - ETA: 11s - loss: 0.4825 - acc: 0.84 - ETA: 11s - loss: 0.4785 - acc: 0.84 - ETA: 11s - loss: 0.4754 - acc: 0.84 - ETA: 11s - loss: 0.4719 - acc: 0.85 - ETA: 11s - loss: 0.4684 - acc: 0.85 - ETA: 11s - loss: 0.4650 - acc: 0.85 - ETA: 11s - loss: 0.4624 - acc: 0.85 - ETA: 11s - loss: 0.4598 - acc: 0.85 - ETA: 11s - loss: 0.4564 - acc: 0.85 - ETA: 10s - loss: 0.4532 - acc: 0.85 - ETA: 10s - loss: 0.4502 - acc: 0.85 - ETA: 10s - loss: 0.4476 - acc: 0.85 - ETA: 10s - loss: 0.4445 - acc: 0.85 - ETA: 10s - loss: 0.4413 - acc: 0.86 - ETA: 10s - loss: 0.4400 - acc: 0.86 - ETA: 10s - loss: 0.4375 - acc: 0.86 - ETA: 10s - loss: 0.4337 - acc: 0.86 - ETA: 10s - loss: 0.4314 - acc: 0.86 - ETA: 9s - loss: 0.4291 - acc: 0.8643 - ETA: 9s - loss: 0.4264 - acc: 0.865 - ETA: 9s - loss: 0.4240 - acc: 0.865 - ETA: 9s - loss: 0.4214 - acc: 0.866 - ETA: 9s - loss: 0.4189 - acc: 0.867 - ETA: 9s - loss: 0.4163 - acc: 0.868 - ETA: 9s - loss: 0.4142 - acc: 0.868 - ETA: 9s - loss: 0.4117 - acc: 0.869 - ETA: 9s - loss: 0.4104 - acc: 0.870 - ETA: 9s - loss: 0.4081 - acc: 0.871 - ETA: 9s - loss: 0.4060 - acc: 0.871 - ETA: 8s - loss: 0.4047 - acc: 0.872 - ETA: 8s - loss: 0.4011 - acc: 0.873 - ETA: 8s - loss: 0.3991 - acc: 0.874 - ETA: 8s - loss: 0.3972 - acc: 0.874 - ETA: 8s - loss: 0.3948 - acc: 0.875 - ETA: 8s - loss: 0.3926 - acc: 0.876 - ETA: 8s - loss: 0.3902 - acc: 0.877 - ETA: 8s - loss: 0.3883 - acc: 0.877 - ETA: 8s - loss: 0.3859 - acc: 0.878 - ETA: 8s - loss: 0.3842 - acc: 0.878 - ETA: 7s - loss: 0.3824 - acc: 0.879 - ETA: 7s - loss: 0.3812 - acc: 0.879 - ETA: 7s - loss: 0.3790 - acc: 0.880 - ETA: 7s - loss: 0.3770 - acc: 0.881 - ETA: 7s - loss: 0.3750 - acc: 0.881 - ETA: 7s - loss: 0.3728 - acc: 0.882 - ETA: 7s - loss: 0.3713 - acc: 0.883 - ETA: 7s - loss: 0.3695 - acc: 0.883 - ETA: 7s - loss: 0.3674 - acc: 0.884 - ETA: 7s - loss: 0.3661 - acc: 0.884 - ETA: 7s - loss: 0.3646 - acc: 0.885 - ETA: 7s - loss: 0.3629 - acc: 0.885 - ETA: 7s - loss: 0.3615 - acc: 0.886 - ETA: 6s - loss: 0.3600 - acc: 0.886 - ETA: 6s - loss: 0.3584 - acc: 0.887 - ETA: 6s - loss: 0.3565 - acc: 0.887 - ETA: 6s - loss: 0.3552 - acc: 0.888 - ETA: 6s - loss: 0.3536 - acc: 0.888 - ETA: 6s - loss: 0.3522 - acc: 0.889 - ETA: 6s - loss: 0.3504 - acc: 0.889 - ETA: 6s - loss: 0.3489 - acc: 0.890 - ETA: 6s - loss: 0.3481 - acc: 0.890 - ETA: 6s - loss: 0.3466 - acc: 0.890 - ETA: 6s - loss: 0.3450 - acc: 0.891 - ETA: 6s - loss: 0.3442 - acc: 0.891 - ETA: 5s - loss: 0.3428 - acc: 0.892 - ETA: 5s - loss: 0.3411 - acc: 0.892 - ETA: 5s - loss: 0.3397 - acc: 0.893 - ETA: 5s - loss: 0.3380 - acc: 0.893 - ETA: 5s - loss: 0.3369 - acc: 0.894 - ETA: 5s - loss: 0.3358 - acc: 0.894 - ETA: 5s - loss: 0.3345 - acc: 0.895 - ETA: 5s - loss: 0.3330 - acc: 0.895 - ETA: 5s - loss: 0.3316 - acc: 0.895 - ETA: 5s - loss: 0.3303 - acc: 0.896 - ETA: 5s - loss: 0.3283 - acc: 0.897 - ETA: 5s - loss: 0.3267 - acc: 0.897 - ETA: 5s - loss: 0.3256 - acc: 0.897 - ETA: 4s - loss: 0.3245 - acc: 0.898 - ETA: 4s - loss: 0.3233 - acc: 0.898 - ETA: 4s - loss: 0.3221 - acc: 0.899 - ETA: 4s - loss: 0.3212 - acc: 0.899 - ETA: 4s - loss: 0.3201 - acc: 0.899 - ETA: 4s - loss: 0.3190 - acc: 0.900 - ETA: 4s - loss: 0.3178 - acc: 0.900 - ETA: 4s - loss: 0.3163 - acc: 0.901 - ETA: 4s - loss: 0.3159 - acc: 0.901 - ETA: 4s - loss: 0.3152 - acc: 0.901 - ETA: 4s - loss: 0.3139 - acc: 0.902 - ETA: 4s - loss: 0.3128 - acc: 0.902 - ETA: 4s - loss: 0.3117 - acc: 0.902 - ETA: 3s - loss: 0.3107 - acc: 0.903 - ETA: 3s - loss: 0.3098 - acc: 0.903 - ETA: 3s - loss: 0.3081 - acc: 0.903 - ETA: 3s - loss: 0.3069 - acc: 0.904 - ETA: 3s - loss: 0.3059 - acc: 0.904 - ETA: 3s - loss: 0.3051 - acc: 0.904 - ETA: 3s - loss: 0.3039 - acc: 0.905 - ETA: 3s - loss: 0.3031 - acc: 0.905 - ETA: 3s - loss: 0.3019 - acc: 0.906 - ETA: 3s - loss: 0.3010 - acc: 0.906 - ETA: 3s - loss: 0.3002 - acc: 0.906 - ETA: 3s - loss: 0.2994 - acc: 0.906 - ETA: 3s - loss: 0.2985 - acc: 0.907 - ETA: 2s - loss: 0.2977 - acc: 0.907 - ETA: 2s - loss: 0.2969 - acc: 0.907 - ETA: 2s - loss: 0.2959 - acc: 0.908 - ETA: 2s - loss: 0.2946 - acc: 0.908 - ETA: 2s - loss: 0.2933 - acc: 0.909 - ETA: 2s - loss: 0.2922 - acc: 0.909 - ETA: 2s - loss: 0.2911 - acc: 0.909 - ETA: 2s - loss: 0.2899 - acc: 0.909 - ETA: 2s - loss: 0.2891 - acc: 0.910 - ETA: 2s - loss: 0.2882 - acc: 0.910 - ETA: 2s - loss: 0.2872 - acc: 0.910 - ETA: 2s - loss: 0.2866 - acc: 0.910 - ETA: 2s - loss: 0.2860 - acc: 0.911 - ETA: 1s - loss: 0.2850 - acc: 0.911 - ETA: 1s - loss: 0.2843 - acc: 0.911 - ETA: 1s - loss: 0.2834 - acc: 0.911 - ETA: 1s - loss: 0.2824 - acc: 0.912 - ETA: 1s - loss: 0.2816 - acc: 0.912 - ETA: 1s - loss: 0.2807 - acc: 0.912 - ETA: 1s - loss: 0.2798 - acc: 0.913 - ETA: 1s - loss: 0.2788 - acc: 0.913 - ETA: 1s - loss: 0.2783 - acc: 0.913 - ETA: 1s - loss: 0.2778 - acc: 0.913 - ETA: 1s - loss: 0.2766 - acc: 0.914 - ETA: 1s - loss: 0.2761 - acc: 0.914 - ETA: 1s - loss: 0.2754 - acc: 0.914 - ETA: 1s - loss: 0.2747 - acc: 0.914 - ETA: 0s - loss: 0.2741 - acc: 0.915 - ETA: 0s - loss: 0.2735 - acc: 0.915\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-65d86327aeec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m12\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\guillaume\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m   1637\u001b[0m           \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1638\u001b[0m           \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1639\u001b[1;33m           validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1640\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1641\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32mc:\\users\\guillaume\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    219\u001b[0m           \u001b[0mbatch_logs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 221\u001b[1;33m         \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_batch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    222\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m           \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\guillaume\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m    247\u001b[0m     \u001b[0mt_before_callbacks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    248\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 249\u001b[1;33m       \u001b[0mcallback\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_batch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    250\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_delta_ts_batch_end\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mt_before_callbacks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    251\u001b[0m     \u001b[0mdelta_t_median\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmedian\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_delta_ts_batch_end\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\guillaume\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m    467\u001b[0m     \u001b[1;31m# will be handled by on_epoch_end.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    468\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseen\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 469\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_values\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    470\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    471\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\guillaume\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\generic_utils.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, current, values)\u001b[0m\n\u001b[0;32m    366\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dynamic_display\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    367\u001b[0m         \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\b'\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mprev_total_width\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 368\u001b[1;33m         \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\r'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    369\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    370\u001b[0m         \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\guillaume\\anaconda3\\lib\\site-packages\\colorama\\ansitowin32.py\u001b[0m in \u001b[0;36mwrite\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__convertor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\guillaume\\anaconda3\\lib\\site-packages\\colorama\\ansitowin32.py\u001b[0m in \u001b[0;36mwrite\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    139\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite_and_convert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrapped\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\guillaume\\anaconda3\\lib\\site-packages\\colorama\\ansitowin32.py\u001b[0m in \u001b[0;36mwrite_and_convert\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    167\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_ansi\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mmatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m             \u001b[0mcursor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 169\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite_plain_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcursor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    170\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\guillaume\\anaconda3\\lib\\site-packages\\colorama\\ansitowin32.py\u001b[0m in \u001b[0;36mwrite_plain_text\u001b[1;34m(self, text, start, end)\u001b[0m\n\u001b[0;32m    173\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mstart\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrapped\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 175\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrapped\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    176\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\guillaume\\anaconda3\\lib\\site-packages\\ipykernel\\iostream.py\u001b[0m in \u001b[0;36mflush\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    340\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpub_thread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mschedule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m                 \u001b[1;31m# and give a timeout to avoid\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 342\u001b[1;33m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mevt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflush_timeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    343\u001b[0m                     \u001b[1;31m# write directly to __stderr__ instead of warning because\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m                     \u001b[1;31m# if this is happening sys.stderr may be the problem.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\guillaume\\anaconda3\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    549\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    550\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 551\u001b[1;33m                 \u001b[0msignaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    552\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    553\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\guillaume\\anaconda3\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    297\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    298\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 299\u001b[1;33m                     \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    300\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Try to use the functional API\n",
    "#tf.reset_default_graph()\n",
    "def net2(inputs_shapes, emb_size=4):\n",
    "    images_shape = inputs_shapes\n",
    "    input_image = tf.keras.Input(images_shape,name='image_input')\n",
    "    x = tf.keras.layers.Conv2D(32,(3, 3), activation='relu')(input_image)\n",
    "    x = tf.keras.layers.Conv2D(64,(3, 3), activation='relu')(x)\n",
    "    x = tf.keras.layers.MaxPooling2D((2, 2))(x)\n",
    "    x = tf.keras.layers.Dropout(0.25)(x)\n",
    "    \n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dropout(0.5)(x)\n",
    "    out = tf.keras.layers.Dense(10,activation='softmax', name='out')(x)\n",
    "    return tf.keras.Model(inputs=input_image, outputs=out)\n",
    "\n",
    "w, h = SIZE\n",
    "inputs_shapes = (w, h, 1)\n",
    "model = net2(inputs_shapes)\n",
    "model.compile(tf.keras.optimizers.Adadelta(),loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "print(x_train.shape)\n",
    "model.fit(x_train,y_train,batch_size=128,epochs=12,validation_data=(x_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "image_input (InputLayer)     (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 11, 11, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 5, 5, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 3, 3, 64)          18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 1, 1, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 32,874\n",
      "Trainable params: 32,874\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train inputs:\n",
      "[[[[-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   ...\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]]\n",
      "\n",
      "  [[-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   ...\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]]\n",
      "\n",
      "  [[-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   ...\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   ...\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]]\n",
      "\n",
      "  [[-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   ...\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]]\n",
      "\n",
      "  [[-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   ...\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]]]\n",
      "\n",
      "\n",
      " [[[-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   ...\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]]\n",
      "\n",
      "  [[-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   ...\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]]\n",
      "\n",
      "  [[-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   ...\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   ...\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]]\n",
      "\n",
      "  [[-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   ...\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]]\n",
      "\n",
      "  [[-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   ...\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]]]\n",
      "\n",
      "\n",
      " [[[-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   ...\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]]\n",
      "\n",
      "  [[-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   ...\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]]\n",
      "\n",
      "  [[-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   ...\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   ...\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]]\n",
      "\n",
      "  [[-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   ...\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]]\n",
      "\n",
      "  [[-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   ...\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   ...\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]]\n",
      "\n",
      "  [[-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   ...\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]]\n",
      "\n",
      "  [[-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   ...\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   ...\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]]\n",
      "\n",
      "  [[-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   ...\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]]\n",
      "\n",
      "  [[-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   ...\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]]]\n",
      "\n",
      "\n",
      " [[[-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   ...\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]]\n",
      "\n",
      "  [[-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   ...\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]]\n",
      "\n",
      "  [[-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   ...\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   ...\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]]\n",
      "\n",
      "  [[-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   ...\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]]\n",
      "\n",
      "  [[-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   ...\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]]]\n",
      "\n",
      "\n",
      " [[[-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   ...\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]]\n",
      "\n",
      "  [[-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   ...\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]]\n",
      "\n",
      "  [[-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   ...\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   ...\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]]\n",
      "\n",
      "  [[-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   ...\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]]\n",
      "\n",
      "  [[-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   ...\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]]]]\n",
      "[5 0 4 ... 5 6 8]\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "Train outputs\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "Valid inputs\n",
      "[[[[-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   ...\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]]\n",
      "\n",
      "  [[-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   ...\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]]\n",
      "\n",
      "  [[-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   ...\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   ...\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]]\n",
      "\n",
      "  [[-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   ...\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]]\n",
      "\n",
      "  [[-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   ...\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]]]\n",
      "\n",
      "\n",
      " [[[-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   ...\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]]\n",
      "\n",
      "  [[-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   ...\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]]\n",
      "\n",
      "  [[-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   ...\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   ...\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]]\n",
      "\n",
      "  [[-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   ...\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]]\n",
      "\n",
      "  [[-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   ...\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]]]\n",
      "\n",
      "\n",
      " [[[-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   ...\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]]\n",
      "\n",
      "  [[-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   ...\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]]\n",
      "\n",
      "  [[-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   ...\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   ...\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]]\n",
      "\n",
      "  [[-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   ...\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]]\n",
      "\n",
      "  [[-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   ...\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   ...\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]]\n",
      "\n",
      "  [[-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   ...\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]]\n",
      "\n",
      "  [[-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   ...\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   ...\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]]\n",
      "\n",
      "  [[-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   ...\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]]\n",
      "\n",
      "  [[-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   ...\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]]]\n",
      "\n",
      "\n",
      " [[[-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   ...\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]]\n",
      "\n",
      "  [[-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   ...\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]]\n",
      "\n",
      "  [[-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   ...\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   ...\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]]\n",
      "\n",
      "  [[-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   ...\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]]\n",
      "\n",
      "  [[-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   ...\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]]]\n",
      "\n",
      "\n",
      " [[[-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   ...\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]]\n",
      "\n",
      "  [[-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   ...\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]]\n",
      "\n",
      "  [[-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   ...\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   ...\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]]\n",
      "\n",
      "  [[-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   ...\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]]\n",
      "\n",
      "  [[-0.99609375]\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n",
      "   ...\n",
      "   [-0.99609375]\n",
      "   [-0.99609375]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   [-0.99609375]]]]\n",
      "[7 2 1 ... 4 5 6]\n",
      "[0 0 1 ... 0 1 1]\n",
      "Valid outputs\n",
      "[[0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "[0 0 1 ... 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "# Check the inputs\n",
    "print(\"Train inputs:\")\n",
    "print(images_train)\n",
    "print(labels_train)\n",
    "print(issame_train_in)\n",
    "\n",
    "print(\"Train outputs\")\n",
    "print(labels_train_out)\n",
    "print(issame_train_in)\n",
    "\n",
    "print(\"Valid inputs\")\n",
    "print(pairs)\n",
    "print(labels_valid)\n",
    "print(issame_in)\n",
    "\n",
    "print(\"Valid outputs\")\n",
    "print(labels_valid_out)\n",
    "print(issame_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "can only convert an array of size 1 to a Python scalar",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-625f447efafb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;31m#validation_data = ([images_valid,issame_in], labels_valid)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;31m#validation_data=([images_valid,labels_valid,issame_in],[labels_valid,issame_out])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensorBoard\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlog_dir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'../output/logs/'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m )\n",
      "\u001b[1;32mc:\\users\\guillaume\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m   1637\u001b[0m           \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1638\u001b[0m           \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1639\u001b[1;33m           validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1640\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1641\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32mc:\\users\\guillaume\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    219\u001b[0m           \u001b[0mbatch_logs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 221\u001b[1;33m         \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_batch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    222\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m           \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\guillaume\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m    247\u001b[0m     \u001b[0mt_before_callbacks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    248\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 249\u001b[1;33m       \u001b[0mcallback\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_batch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    250\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_delta_ts_batch_end\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mt_before_callbacks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    251\u001b[0m     \u001b[0mdelta_t_median\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmedian\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_delta_ts_batch_end\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\guillaume\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m   1076\u001b[0m                   \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1077\u001b[0m                   if k not in ['batch', 'size', 'num_steps']}\n\u001b[1;32m-> 1078\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_write_custom_summaries\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_total_batches_seen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1079\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_total_batches_seen\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1080\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\guillaume\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_write_custom_summaries\u001b[1;34m(self, step, logs)\u001b[0m\n\u001b[0;32m   1048\u001b[0m         \u001b[0msummary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_summary\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1049\u001b[0m         \u001b[0msummary_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msummary\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1050\u001b[1;33m         \u001b[0msummary_value\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msimple_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1051\u001b[0m         \u001b[0msummary_value\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtag\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1052\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_summary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: can only convert an array of size 1 to a Python scalar"
     ]
    }
   ],
   "source": [
    "model.fit(\n",
    "    #[images_train,issame_train_in],\n",
    "    [images_train,labels_train,issame_train_in],\n",
    "    #[labels_train,issame_train_out],\n",
    "    #[labels_train,issame_train_out],\n",
    "    [labels_train_out,issame_train_in],\n",
    "    epochs=20,\n",
    "    batch_size=1024,\n",
    "    validation_data = ([pairs,labels_valid,issame_in], [labels_valid_out,issame_in]),\n",
    "    #validation_data = ([images_valid,issame_in], labels_valid)\n",
    "    #validation_data=([images_valid,labels_valid,issame_in],[labels_valid,issame_out])\n",
    "    callbacks=[tf.keras.callbacks.TensorBoard(log_dir='../output/logs/')]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "image_input (InputLayer)        (None, 100, 100, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 98, 98, 10)   280         image_input[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 49, 49, 10)   0           conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 47, 47, 20)   1820        max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 23, 23, 20)   0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 21, 21, 40)   7240        max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 10, 10, 40)   0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d (Globa (None, 40)           0           max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 32)           1312        global_average_pooling2d[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "input_labels (InputLayer)       (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "issame_input (InputLayer)       (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "arcface (Arcface)               (None, 1301)         41632       dense[0][0]                      \n",
      "                                                                 input_labels[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "validation (Validation)         (None, 1)            0           dense[0][0]                      \n",
      "                                                                 issame_input[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 52,284\n",
      "Trainable params: 52,284\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test it on the training/validation dataset to stop the worst examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_predict,_=model.predict([images_train[0:10],labels_train[0:10],issame_train_in[0:10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\guillaume\\anaconda3\\lib\\site-packages\\skimage\\io\\_plugins\\matplotlib_plugin.py:80: UserWarning: Float image out of standard range; displaying image with stretched contrast.\n",
      "  warn(\"Float image out of standard range; displaying \"\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2000f3e5d30>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import skimage as sk\n",
    "sk.io.imshow(images_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 0., ..., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_predict[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.6868594, 0.7891156]], dtype=float32)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict = model.predict([images_test[0:1],labels_test[0:1],issame_in[0:1]])\n",
    "predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate it on the test dataset: one shot learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
