{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DogFaceNet version 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import skimage as sk\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = '../data/dogfacenet/'\n",
    "PATH_IMAGES = PATH + 'images/'\n",
    "PATH_RESIZED = PATH + 'resized/'\n",
    "\n",
    "# Size of the input image into the network\n",
    "SIZE = (100,100,3)\n",
    "\n",
    "TEST_SPLIT = 0.05\n",
    "VALID_SPLIT = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset preprocessing\n",
    "- Get the dataset from folders\n",
    "- Associate the corresponding classes\n",
    "- Resized the dataset\n",
    "- Shuffle the dataset?\n",
    "- Divide the dataset into validation, training and testing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def square_crop(image):\n",
    "    \"\"\"\n",
    "    Takes the largest between height and width of the image and crops it into a square.\n",
    "    This square is located in the middle of the image.\n",
    "    \"\"\"\n",
    "    \n",
    "    h,w,c = image.shape\n",
    "    \n",
    "    if w > h:\n",
    "        margin = w - h\n",
    "        margin = margin // 2\n",
    "        image = image[:,margin:margin+h,:]\n",
    "    elif w < h:\n",
    "        margin = h - w\n",
    "        margin = margin // 2\n",
    "        image = image[margin:margin+w,:,:]\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resize pictures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6491e4baa6e74a5f88a1cbf88edfa807",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=18), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\guillaume\\anaconda3\\lib\\site-packages\\skimage\\transform\\_warps.py:105: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n",
      "c:\\users\\guillaume\\anaconda3\\lib\\site-packages\\skimage\\transform\\_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.\n",
      "  warn(\"Anti-aliasing will be enabled by default in skimage 0.15 to \"\n",
      "c:\\users\\guillaume\\anaconda3\\lib\\site-packages\\skimage\\util\\dtype.py:141: UserWarning: Possible precision loss when converting from float64 to uint8\n",
      "  .format(dtypeobj_in, dtypeobj_out))\n"
     ]
    }
   ],
   "source": [
    "w, h, c = SIZE\n",
    "\n",
    "label = 0\n",
    "\n",
    "filenames = os.listdir(PATH_IMAGES)\n",
    "\n",
    "# Just save the pictures after resizing them\n",
    "for i in tqdm_notebook(range(598+869,len(filenames))):\n",
    "    for file in os.listdir(PATH_IMAGES + filenames[i]):\n",
    "        label += 1\n",
    "        \n",
    "        # Read and resized image\n",
    "        image = sk.io.imread(PATH_IMAGES + filenames[i] + '/' + file)\n",
    "        if len(image.shape) == 3:\n",
    "            image_cropped = square_crop(image)\n",
    "            image_resized = sk.transform.resize(image_cropped,SIZE)\n",
    "\n",
    "            # Save image\n",
    "            ## Check if the good folder exists\n",
    "            if filenames[i] not in os.listdir(PATH_RESIZED):\n",
    "                os.mkdir(PATH_RESIZED + filenames[i])\n",
    "            sk.io.imsave(PATH_RESIZED + filenames[i] + '/' + str(label) + '.jpg', image_resized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dataset into memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images: 5568\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "994f4a7c5ade4f27883cc3d6e211ee98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1477), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "filenames = os.listdir(PATH_RESIZED)\n",
    "\n",
    "i = 0\n",
    "\n",
    "# Remove unique examples\n",
    "while i<len(filenames):\n",
    "    files = os.listdir(PATH_RESIZED + filenames[i])\n",
    "    if len(files)<=1:\n",
    "        filenames = filenames[:i] + filenames[i+1:]\n",
    "    else:\n",
    "        i += 1\n",
    "\n",
    "# Compute the number of images\n",
    "nbof_images = 0\n",
    "for i in range(0,len(filenames)):\n",
    "    files = os.listdir(PATH_RESIZED + filenames[i])\n",
    "    nbof_images += len(files)\n",
    "\n",
    "print(\"Number of images: \" + str(nbof_images))\n",
    "    \n",
    "w, h, c = SIZE\n",
    "\n",
    "images = np.empty((nbof_images,w,h,c))\n",
    "labels = np.empty(nbof_images, dtype=int)\n",
    "\n",
    "label = 0\n",
    "\n",
    "index = 0\n",
    "\n",
    "# Load images into numpy arrays\n",
    "for i in tqdm_notebook(range(len(filenames))):\n",
    "    files = os.listdir(PATH_RESIZED + filenames[i])\n",
    "    for file in files:\n",
    "        labels[index] = label\n",
    "        # Read image\n",
    "        image = sk.io.imread(PATH_RESIZED + filenames[i] + '/' + file)\n",
    "\n",
    "        # Add the image to the table\n",
    "        images[index] = image\n",
    "        \n",
    "        index += 1\n",
    "    label += 1\n",
    "    \n",
    "assert len(labels)==len(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Old method!\n",
    "\n",
    "Divide the dataset into train, valid and test:\n",
    "\n",
    "To create the validation dataset we use pictures of dogs were there is more than 3 pictures and took one of this picture.\n",
    "\n",
    "For the testing dataset we simply split the classes. We will then use the network as a one shot learner on this new dataset. With one picture the network will produce one embedding vector. For each embedding vector compute the L2 distance with each face to compute the most propable one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of images: 5568\n",
      "Number of test images: 278\n",
      "Number of validation images: 543\n",
      "Number of training images: 4747\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "only size-1 arrays can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-77-95fc612d2572>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m         \u001b[0mcount_train\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m \u001b[1;31m# print(labels)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;31m# print(labels_test)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: only size-1 arrays can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "w, h, c = SIZE\n",
    "\n",
    "nbof_test = int(len(images)*TEST_SPLIT)\n",
    "\n",
    "images_test = images[:nbof_test]\n",
    "labels_test = labels[:nbof_test]\n",
    "\n",
    "\n",
    "# Count valid images:\n",
    "state = -1\n",
    "count_valid = 0\n",
    "count_train = 0\n",
    "count_image_class = 0\n",
    "\n",
    "for i in range(nbof_test,len(labels)):\n",
    "    if state != labels[i]:\n",
    "        state = labels[i]\n",
    "        count_image_class = 0\n",
    "    else:\n",
    "        count_image_class += 1\n",
    "    \n",
    "    if count_image_class == 3:\n",
    "        count_valid += 1\n",
    "    else:\n",
    "        count_train += 1\n",
    "\n",
    "print(\"Total number of images: \" + str(len(labels)))\n",
    "print(\"Number of test images: \" + str(len(labels_test)))\n",
    "print(\"Number of validation images: \" + str(count_valid))\n",
    "print(\"Number of training images: \" + str(count_train))\n",
    "\n",
    "images_valid = np.empty((count_valid,w,h,c))\n",
    "labels_valid = np.empty(count_valid)\n",
    "\n",
    "images_train = np.empty((count_train,w,h,c))\n",
    "labels_train = np.empty(count_train)\n",
    "\n",
    "state = -1\n",
    "count_valid = 0\n",
    "count_train = 0\n",
    "count_image_class = 0\n",
    "\n",
    "for i in range(nbof_test,len(labels)):\n",
    "    if state != labels[i]:\n",
    "        state = labels[i]\n",
    "        count_image_class = 0\n",
    "    else:\n",
    "        count_image_class += 1\n",
    "    \n",
    "    if count_image_class == 3:\n",
    "        # Add the validation image in the validation array\n",
    "        images_valid[count_valid] = images[i]\n",
    "        labels_valid[count_valid] = labels[i]\n",
    "        \n",
    "        count_valid += 1\n",
    "    else:\n",
    "        images_train[count_train] = images[i]\n",
    "        labels_train[count_train] = labels[i]\n",
    "        \n",
    "        count_train += 1\n",
    "# print(labels)\n",
    "# print(labels_test)\n",
    "# print(labels_train)\n",
    "# print(labels_valid)\n",
    "print(\"Is the number of images coherent? \" + str(len(labels)==(len(labels_test)+len(labels_train)+len(labels_valid))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### New method\n",
    "- We divide the validation and test set from the training set with the classic division method: 85 percent training, 10 validating, 5 testing.\n",
    "- We then computes pairs of images in the validation set and testing set:\n",
    " - Some of these pairs are images of the same dog and some are picture of different dogs\n",
    " - We create a two lists:\n",
    "  - A list a images containing the pairs: two successive images are a pair of images. For example, image 0 and is a pair, image 2 and 3 is another pair, etc...\n",
    "  - A list of boolean called 'issame' indicating if a pair is a pair of images of the same dog or a pair of different dogs. For example, if image 0 and image 1 are showing the same dog value 0 and 1 in the list will be True. On the other hand if the image 2 and 3 represent two different dogs the value 2 and 3 in the list will be at False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of images: 5568\n",
      "Number of test images: 278\n",
      "Number of validation images: 556\n",
      "Number of training images: 4734\n",
      "Number of classes in the training set: 1300\n",
      "Number of pairs: 556\n",
      "Number of same images: 153\n",
      "Number of validation images: 556\n"
     ]
    }
   ],
   "source": [
    "w, h, c = SIZE\n",
    "\n",
    "nbof_test = int(len(images)*TEST_SPLIT)\n",
    "\n",
    "images_test = images[-nbof_test:]\n",
    "labels_test = labels[-nbof_test:]\n",
    "\n",
    "nbof_valid = int(len(images)*VALID_SPLIT)\n",
    "\n",
    "images_valid = images[-nbof_test-nbof_valid:-nbof_test]\n",
    "labels_valid = labels[-nbof_test-nbof_valid:-nbof_test]\n",
    "\n",
    "images_train = images[:-nbof_test-nbof_valid]\n",
    "labels_train = labels[:-nbof_test-nbof_valid]\n",
    "\n",
    "print(\"Total number of images: \" + str(len(labels)))\n",
    "print(\"Number of test images: \" + str(len(labels_test)))\n",
    "print(\"Number of validation images: \" + str(len(labels_valid)))\n",
    "print(\"Number of training images: \" + str(len(labels_train)))\n",
    "print(\"Number of classes in the training set: \" + str(labels_train[-1] - labels_train[0]))\n",
    "\n",
    "\n",
    "# Creates the pairs\n",
    "\n",
    "nbof_pairs = (len(images_valid)//2)*2 # it has to be multiple of 2\n",
    "\n",
    "print(\"Number of pairs: \" + str(nbof_pairs))\n",
    "\n",
    "pairs = np.empty((nbof_pairs,w,h,c))\n",
    "issame = np.empty((nbof_pairs//2,2))\n",
    "\n",
    "nbof_same = 0\n",
    "\n",
    "for i in range(0,nbof_pairs,2):\n",
    "    ## alea_issame will decide if the new pair will be a pair of same dog images or a pair of different\n",
    "    alea_issame = np.random.rand()\n",
    "\n",
    "    if alea_issame < 0.5: # Then it will be a pair of same dogs\n",
    "        # we randomly choose a dog\n",
    "        choice = np.random.randint(len(labels_valid))\n",
    "        \n",
    "        # we extract the images of this class\n",
    "        chosen_images = list(images_valid[np.equal(labels_valid,labels_valid[choice])])\n",
    "        \n",
    "        while len(labels_valid[np.equal(labels_valid,labels_valid[choice])]) < 2:\n",
    "            choice = np.random.randint(len(labels_valid))\n",
    "            chosen_images = list(images_valid[np.equal(labels_valid,labels_valid[choice])])\n",
    "            \n",
    "        # we then randomly choose two pictures of this class\n",
    "        choice1 = np.random.randint(len(chosen_images))\n",
    "        pairs[i] = chosen_images[choice1]\n",
    "        save = np.copy(chosen_images)\n",
    "        chosen_images = chosen_images[:choice1] + chosen_images[choice1+1:]\n",
    "        if len(chosen_images) == 0:\n",
    "            print(\"Bug!\")\n",
    "            print(save)\n",
    "        choice2 = np.random.randint(len(chosen_images))\n",
    "        pairs[i+1] = chosen_images[choice2]\n",
    "\n",
    "        issame[i//2] = [1,0]\n",
    "        \n",
    "        nbof_same += 1\n",
    "        \n",
    "    else: # Then it will be a pair of different dogs\n",
    "        # we randomly choose two dogs\n",
    "        choice1 = np.random.randint(len(labels_valid))\n",
    "        \n",
    "        # we extract the images of the class\n",
    "        chosen_images = list(images_valid[np.equal(labels_valid,labels_valid[choice1])])\n",
    "        \n",
    "        # we choose an image of this class\n",
    "        choice = np.random.randint(len(chosen_images))\n",
    "        #print(choice)\n",
    "        pairs[i] = images_valid[choice]\n",
    "        \n",
    "        choice2 = np.random.randint(len(labels_valid))\n",
    "        \n",
    "        # check if we have two different classes\n",
    "        while labels_valid[choice2] == labels_valid[choice1]:\n",
    "            choice2 = np.random.randint(len(labels_valid))\n",
    "        \n",
    "        chosen_images = list(images_valid[np.equal(labels_valid,labels_valid[choice2])])\n",
    "        \n",
    "        # we choose an image of this class\n",
    "        choice = np.random.randint(len(chosen_images))\n",
    "        \n",
    "        pairs[i+1] = images_valid[choice]\n",
    "        \n",
    "        issame[i//2] = [0,1]\n",
    "\n",
    "print(\"Number of same images: \" + str(nbof_same))\n",
    "print(\"Number of validation images: \" + str(len(labels_valid)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the model\n",
    "- Define the ArcFace layer\n",
    "- Define the dummy model first\n",
    "- Compile it with the softmax loss and and Adam optimizer\n",
    "- Then use transfer learning with a more complex model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the Arcface layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# My custom layer for arcface\n",
    "# It takes two inputs: one for the embedding, one for the label\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Layer\n",
    "import math\n",
    "\n",
    "# Arcface should only be used for training\n",
    "class Arcface(Layer):\n",
    "\n",
    "    def __init__(self, out_num, s = 64., m = 0.5, **kwargs):\n",
    "        self.out_num = out_num\n",
    "        self.s = s\n",
    "        self.m = m\n",
    "        super(Arcface, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape, initializer='uniform'):\n",
    "        assert isinstance(input_shape, list)\n",
    "        \n",
    "        shape = tf.TensorShape((input_shape[0][-1],self.out_num))\n",
    "        print(shape)\n",
    "        \n",
    "        # Create a trainable weight variable for this layer.\n",
    "        self.kernel = self.add_weight(name='kernel',\n",
    "                                                 shape=shape,\n",
    "                                                 initializer=initializer,\n",
    "                                                 dtype=tf.float32,\n",
    "                                                 trainable=True)\n",
    "        super(Arcface, self).build(input_shape)  # Be sure to call this at the end\n",
    "\n",
    "    def call(self, x):\n",
    "        assert isinstance(x, list)\n",
    "        embedding, labels = x\n",
    "        \n",
    "        cos_m = math.cos(self.m)\n",
    "        sin_m = math.sin(self.m)\n",
    "        mm = sin_m * self.m  # issue 1\n",
    "        threshold = math.cos(math.pi - self.m)\n",
    "        \n",
    "        # inputs and weights norm\n",
    "        embedding_norm = tf.norm(embedding, axis=1, keepdims=True)\n",
    "        embedding = tf.div(embedding, embedding_norm, name='norm_embedding')\n",
    "        \n",
    "        weights_norm = tf.norm(self.kernel, axis=0, keepdims=True)\n",
    "        weights = tf.div(self.kernel, weights_norm, name='norm_weights')\n",
    "        # cos(theta+m)\n",
    "        cos_t = tf.matmul(embedding, weights, name='cos_t')\n",
    "        cos_t2 = tf.square(cos_t, name='cos_2')\n",
    "        sin_t2 = tf.subtract(1., cos_t2, name='sin_2')\n",
    "        sin_t = tf.sqrt(sin_t2, name='sin_t')\n",
    "        cos_mt = self.s * tf.subtract(tf.multiply(cos_t, cos_m), tf.multiply(sin_t, sin_m), name='cos_mt')\n",
    "        \n",
    "        # this condition controls the theta+m should be in range [0, pi]\n",
    "        #      0<=theta+m<=pi\n",
    "        #     -m<=theta<=pi-m\n",
    "        cond_v = cos_t - threshold\n",
    "        cond = tf.cast(tf.nn.relu(cond_v, name='if_else'), dtype=tf.bool)\n",
    "\n",
    "        keep_val = self.s*(cos_t - mm)\n",
    "        cos_mt_temp = tf.where(cond, cos_mt, keep_val)\n",
    "\n",
    "        mask = tf.one_hot(labels, depth=self.out_num, name='one_hot_mask')\n",
    "        # mask = tf.squeeze(mask, 1)\n",
    "        inv_mask = tf.subtract(1., mask, name='inverse_mask')\n",
    "\n",
    "        s_cos_t = tf.multiply(self.s, cos_t, name='scalar_cos_t')\n",
    "\n",
    "        output = tf.add(tf.multiply(s_cos_t, inv_mask), tf.multiply(cos_mt_temp, mask), name='arcface_loss_output')\n",
    "        \n",
    "        return output\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        assert isinstance(input_shape, list)\n",
    "        shape_emb, shape_lab = input_shape\n",
    "        shape_emb[-1] = self.out_num\n",
    "        return tf.TensorShape(shape_emb)\n",
    "    \n",
    "#         shape = tf.TensorShape(input_shape).as_list()\n",
    "#         shape[-1] = self.num_classes\n",
    "#         return tf.TensorShape(shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Layer\n",
    "import math\n",
    "\n",
    "# Arcface should only be used for training\n",
    "class Arcface(Layer):\n",
    "\n",
    "    def __init__(self, out_num, s = 64., m = 0.5, **kwargs):\n",
    "        self.out_num = out_num\n",
    "        self.s = s\n",
    "        self.m = m\n",
    "        super(Arcface, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape, initializer='uniform'):\n",
    "        assert isinstance(input_shape, list)\n",
    "        \n",
    "        shape = tf.TensorShape((input_shape[0][-1],self.out_num))\n",
    "        print(shape)\n",
    "        \n",
    "        # Create a trainable weight variable for this layer.\n",
    "        self.kernel = self.add_weight(name='kernel',\n",
    "                                                 shape=shape,\n",
    "                                                 initializer=initializer,\n",
    "                                                 dtype=tf.float32,\n",
    "                                                 trainable=True)\n",
    "        super(Arcface, self).build(input_shape)  # Be sure to call this at the end\n",
    "\n",
    "    def call(self, x):\n",
    "        assert isinstance(x, list)\n",
    "        embedding, labels = x\n",
    "        \n",
    "        cos_m = math.cos(self.m)\n",
    "        sin_m = math.sin(self.m)\n",
    "        mm = sin_m * self.m  # issue 1\n",
    "        threshold = math.cos(math.pi - self.m)\n",
    "        \n",
    "        # inputs and weights norm\n",
    "        embedding_norm = tf.norm(embedding, axis=1, keepdims=True)\n",
    "        embedding = tf.div(embedding, embedding_norm, name='norm_embedding')\n",
    "        \n",
    "        weights_norm = tf.norm(self.kernel, axis=0, keepdims=True)\n",
    "        weights = tf.div(self.kernel, weights_norm, name='norm_weights')\n",
    "        # cos(theta+m)\n",
    "        cos_t = tf.matmul(embedding, weights, name='cos_t')\n",
    "        cos_t2 = tf.square(cos_t, name='cos_2')\n",
    "        sin_t2 = tf.subtract(1., cos_t2, name='sin_2')\n",
    "        sin_t = tf.sqrt(sin_t2, name='sin_t')\n",
    "        cos_mt = self.s * tf.subtract(tf.multiply(cos_t, cos_m), tf.multiply(sin_t, sin_m), name='cos_mt')\n",
    "        \n",
    "        # this condition controls the theta+m should be in range [0, pi]\n",
    "        #      0<=theta+m<=pi\n",
    "        #     -m<=theta<=pi-m\n",
    "        cond_v = cos_t - threshold\n",
    "        cond = tf.cast(tf.nn.relu(cond_v, name='if_else'), dtype=tf.bool)\n",
    "\n",
    "        keep_val = self.s*(cos_t - mm)\n",
    "        cos_mt_temp = tf.where(cond, cos_mt, keep_val)\n",
    "        \n",
    "        labels_int = tf.cast(labels,tf.int32, name='labels_int')\n",
    "        mask = tf.one_hot(labels_int, depth=self.out_num, name='one_hot_mask')\n",
    "        # mask = tf.squeeze(mask, 1)\n",
    "        inv_mask = tf.subtract(1., mask, name='inverse_mask')\n",
    "\n",
    "        s_cos_t = tf.multiply(self.s, cos_t, name='scalar_cos_t')\n",
    "        mul1 = tf.multiply(s_cos_t, inv_mask)\n",
    "        print(mul1.shape)\n",
    "        mul2 = tf.multiply(cos_mt_temp, mask)\n",
    "        print(mul2.shape)\n",
    "        #output = tf.add(tf.multiply(s_cos_t, inv_mask), tf.multiply(cos_mt_temp, mask), name='arcface_loss_output')\n",
    "        #print(output.shape)\n",
    "        return cos_mt_temp\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        assert isinstance(input_shape, list)\n",
    "        shape_emb, shape_lab = input_shape\n",
    "        shape_emb[-1] = self.out_num\n",
    "        return tf.TensorShape(shape_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MyLayer(Layer):\n",
    "\n",
    "    def __init__(self, output_dim, s = 64., m = 0.5, **kwargs):\n",
    "        self.out_num = output_dim\n",
    "        self.m = m\n",
    "        self.s = s\n",
    "        \n",
    "        super(MyLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Create a trainable weight variable for this layer.\n",
    "        shape = (input_shape[1].value, self.out_num)\n",
    "        print(shape)\n",
    "        self.kernel = self.add_weight(name='kernel', \n",
    "                                      shape=shape,\n",
    "                                      initializer='uniform',\n",
    "                                      trainable=True)\n",
    "        super(MyLayer, self).build(input_shape)  # Be sure to call this at the end\n",
    "\n",
    "    def call(self, x):\n",
    "        #assert isinstance(x, list)\n",
    "        embedding = x\n",
    "        \n",
    "        cos_m = math.cos(self.m)\n",
    "        sin_m = math.sin(self.m)\n",
    "        mm = sin_m * self.m  # issue 1\n",
    "        threshold = math.cos(math.pi - self.m)\n",
    "        \n",
    "        # inputs and weights norm\n",
    "        embedding_norm = tf.norm(embedding, axis=1, keepdims=True)\n",
    "        embedding = tf.div(embedding, embedding_norm, name='norm_embedding')\n",
    "        \n",
    "        weights_norm = tf.norm(self.kernel, axis=0, keepdims=True)\n",
    "        weights = tf.div(self.kernel, weights_norm, name='norm_weights')\n",
    "        #print(self.weights)\n",
    "        # cos(theta+m)\n",
    "        cos_t = tf.matmul(embedding, weights, name='cos_t')\n",
    "        cos_t2 = tf.square(cos_t, name='cos_2')\n",
    "        sin_t2 = tf.subtract(1., cos_t2, name='sin_2')\n",
    "        sin_t = tf.sqrt(sin_t2, name='sin_t')\n",
    "        cos_mt = self.s * tf.subtract(tf.multiply(cos_t, cos_m), tf.multiply(sin_t, sin_m), name='cos_mt')\n",
    "        \n",
    "        # this condition controls the theta+m should be in range [0, pi]\n",
    "        #      0<=theta+m<=pi\n",
    "        #     -m<=theta<=pi-m\n",
    "        cond_v = cos_t - threshold\n",
    "        cond = tf.cast(tf.nn.relu(cond_v, name='if_else'), dtype=tf.bool)\n",
    "\n",
    "        keep_val = self.s*(cos_t - mm)\n",
    "        cos_mt_temp = tf.where(cond, cos_mt, keep_val)\n",
    "\n",
    "        #mask = tf.one_hot(labels, depth=self.out_num, name='one_hot_mask')\n",
    "        # mask = tf.squeeze(mask, 1)\n",
    "        #inv_mask = tf.subtract(1., mask, name='inverse_mask')\n",
    "\n",
    "        \n",
    "        s_cos_t = tf.multiply(self.s, cos_t, name='scalar_cos_t')\n",
    "        \n",
    "        return s_cos_t\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0].value, self.output_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the validation layer:\n",
    "- The bigger the validation batch the better it is (no less than 64 pictures -> 32 pairs)\n",
    "- It computes the ROC curve\n",
    "- Finds the best threshold\n",
    "- Returns a list of 2D vectors [1,0] if the pair was the same dog, [0,1] if it was a different dog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.258671522140503\n",
      "[[ True  True False]\n",
      " [ True  True False]\n",
      " [ True  True False]\n",
      " [ True  True False]\n",
      " [ True  True False]\n",
      " [ True  True False]\n",
      " [ True  True False]\n",
      " [ True  True False]\n",
      " [ True  True False]\n",
      " [ True  True False]\n",
      " [ True  True False]\n",
      " [ True  True False]\n",
      " [ True  True False]\n",
      " [ True  True False]\n",
      " [ True  True False]\n",
      " [ True  True False]\n",
      " [ True  True False]\n",
      " [ True  True False]\n",
      " [ True  True False]\n",
      " [ True  True False]\n",
      " [ True  True False]\n",
      " [ True  True False]\n",
      " [ True  True False]\n",
      " [ True  True False]\n",
      " [ True  True False]\n",
      " [ True  True False]\n",
      " [ True  True False]\n",
      " [ True  True False]\n",
      " [ True  True False]\n",
      " [ True  True False]\n",
      " [ True  True False]\n",
      " [ True  True False]\n",
      " [ True  True False]\n",
      " [ True  True False]\n",
      " [ True  True False]\n",
      " [ True  True False]\n",
      " [ True  True False]\n",
      " [ True  True False]\n",
      " [ True  True False]\n",
      " [ True  True False]\n",
      " [ True  True False]\n",
      " [ True  True False]\n",
      " [ True  True False]\n",
      " [ True  True False]\n",
      " [ True  True False]\n",
      " [ True  True False]\n",
      " [ True  True False]\n",
      " [ True  True False]\n",
      " [ True  True False]\n",
      " [ True  True False]\n",
      " [ True  True False]\n",
      " [ True  True False]\n",
      " [ True  True False]\n",
      " [ True  True False]\n",
      " [ True  True False]\n",
      " [ True  True False]\n",
      " [ True  True False]\n",
      " [ True  True False]\n",
      " [ True  True False]\n",
      " [ True  True False]\n",
      " [ True  True False]\n",
      " [ True  True False]\n",
      " [ True  True False]\n",
      " [ True  True False]\n",
      " [ True  True False]\n",
      " [ True  True False]\n",
      " [ True  True False]\n",
      " [ True  True False]\n",
      " [ True  True False]\n",
      " [ True  True False]\n",
      " [ True  True False]\n",
      " [ True  True False]\n",
      " [ True  True False]\n",
      " [ True  True False]\n",
      " [ True  True False]\n",
      " [ True  True False]\n",
      " [ True  True False]\n",
      " [ True  True False]\n",
      " [ True  True False]\n",
      " [ True  True False]\n",
      " [ True  True False]\n",
      " [ True  True False]\n",
      " [ True  True False]\n",
      " [ True  True False]\n",
      " [ True  True False]\n",
      " [ True  True False]\n",
      " [ True  True False]\n",
      " [ True  True False]\n",
      " [ True  True False]\n",
      " [ True  True False]\n",
      " [ True  True False]\n",
      " [ True  True False]\n",
      " [ True  True False]\n",
      " [ True  True False]\n",
      " [ True  True False]\n",
      " [ True  True False]\n",
      " [ True  True False]\n",
      " [ True  True False]\n",
      " [ True  True False]\n",
      " [ True  True False]]\n",
      "[[False False  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True False]\n",
      " [ True  True False]\n",
      " [ True  True False]\n",
      " [ True  True False]\n",
      " [ True  True False]\n",
      " [ True  True False]\n",
      " [ True  True False]\n",
      " [ True  True False]\n",
      " [ True  True False]]\n",
      "[[1 0]\n",
      " [1 0]\n",
      " [0 1]]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "# tests unitaires\n",
    "\n",
    "tf.reset_default_graph()\n",
    "emb_raw = tf.constant([[12.,2],[8,4],[3,8],[2,10],[50,10],[10,30]])\n",
    "emb_ = tf.math.l2_normalize(emb_raw,0)\n",
    "\n",
    "# Normalizes\n",
    "emb = tf.math.l2_normalize(emb_,0)\n",
    "\n",
    "# Separates the pairs\n",
    "emb1 = emb[0::2]\n",
    "emb2 = emb[1::2]\n",
    "\n",
    "# Computes distance between pairs\n",
    "diff = tf.squared_difference(emb1,emb2)\n",
    "dist = tf.reduce_sum(diff,1)\n",
    "\n",
    "\n",
    "\n",
    "# Creates different threshold in order to find the best one\n",
    "np_threshold = np.vstack([np.arange(0,1,0.01)]*3).T\n",
    "\n",
    "# Reshapes the distance\n",
    "dist2 = tf.stack([dist]*len(np_threshold))\n",
    "\n",
    "# Reshapes the true values\n",
    "actual_issame = tf.constant([[True,True,False]]*len(np_threshold),dtype=bool)\n",
    "\n",
    "threshold = tf.constant(np_threshold, dtype=tf.float32)\n",
    "\n",
    "# Uses the created thresholds to compute the predictions\n",
    "predict_issame = tf.less(dist2,threshold)\n",
    "\n",
    "# Computes the accuracy\n",
    "truth = tf.logical_not(tf.logical_xor(predict_issame, actual_issame))\n",
    "\n",
    "r = tf.reduce_sum(tf.cast(truth,tf.float32),1)\n",
    "\n",
    "# Finds the best accuracy with respect to the threshold\n",
    "m = tf.argmax(r)\n",
    "\n",
    "# best_threshold = threshold[m]\n",
    "# accuracy = r[m]/3\n",
    "\n",
    "# Ouputs the best output and reshapes the output in a softmax way\n",
    "bool_output = predict_issame[m]\n",
    "int_output = tf.cast(bool_output,tf.int32)\n",
    "output = tf.map_fn(lambda x : (1-x) * [0,1] + x * [1,0], int_output)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "with tf.Session() as sess:\n",
    "    t1 = time.time()\n",
    "    act,pred,a_ = sess.run([actual_issame,truth,output])\n",
    "    t2 = time.time()\n",
    "    print(t2-t1)\n",
    "    print(a_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Layer\n",
    "\n",
    "# Should only be used for validating\n",
    "class Validation(Layer):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Validation, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert isinstance(input_shape, list)\n",
    "        self.emb_shape = input_shape[0]\n",
    "        super(Validation, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        \"\"\"\n",
    "        Inputs: a tuple containing the embeddings and the issame list\n",
    "        - embeddings: shape=(batch_size, embedding_size), type=float\n",
    "        - issame: shape=(batch_size), type=bool\n",
    "        \n",
    "        Outputs: a tensor of shape=(batch_size,2), the ouput is either [1,0] (is same) or [0,1] (is different)\n",
    "        \"\"\"\n",
    "        assert isinstance(x, list)\n",
    "        \n",
    "        embeddings, issame = x\n",
    "        self.emb_shape = embeddings.shape\n",
    "\n",
    "        emb = tf.math.l2_normalize(embeddings,0)\n",
    "        # emb contains a list of pictures\n",
    "        # pictures with an even index are first pictures of the pairs\n",
    "        # pictures with an odd index are second pictures of the pairs\n",
    "        emb1 = emb[0::2]\n",
    "        emb2 = emb[1::2]\n",
    "        \n",
    "        # Compute the distance for each pair of vector\n",
    "        dist = tf.reduce_sum(tf.squared_difference(emb1,emb2),1)\n",
    "        \n",
    "        # Creates different threshold in order to find the best one\n",
    "        np_threshold = np.vstack([np.arange(0,1,0.01)]*int(self.emb_shape[0]//2)).T\n",
    "        \n",
    "        # Reshapes the distance\n",
    "        dist2 = tf.stack([dist]*len(np_threshold))\n",
    "    \n",
    "        actual_issame = tf.constant([issame[0::2]]*len(np_threshold),dtype=bool)\n",
    "        \n",
    "        threshold = tf.constant(np_threshold, dtype=tf.float32)\n",
    "\n",
    "        # Uses the created thresholds to compute the predictions\n",
    "        predict_issame = tf.less(dist2,threshold)\n",
    "\n",
    "        # Computes the accuracy\n",
    "        truth = tf.logical_not(tf.logical_xor(predict_issame, actual_issame))\n",
    "\n",
    "        r = tf.reduce_sum(tf.cast(truth,tf.float32),1)\n",
    "\n",
    "        # Finds the best accuracy with respect to the threshold\n",
    "        m = tf.argmax(r)\n",
    "\n",
    "        # best_threshold = threshold[m]\n",
    "        # accuracy = r[m]/3\n",
    "\n",
    "        # Ouputs the best output and reshapes the output in a softmax way\n",
    "        bool_output = predict_issame[m]\n",
    "        int_output = tf.cast(bool_output,tf.int32)\n",
    "        output = tf.map_fn(lambda x : (1-x) * [0,1] + x * [1,0], int_output)\n",
    "        \n",
    "        return output\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        assert isinstance(input_shape, list)\n",
    "        emb_shape, _ = input_shape\n",
    "        return (emb_shape[0]//2, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dummy(tf.keras.Model):\n",
    "    def __init__(self, out_num, emb_size = 32):\n",
    "        \"\"\"\n",
    "        -emb_size: size of the embedding\n",
    "        -out_num: number of identities in the \n",
    "        \"\"\"\n",
    "        super(Dummy, self).__init__(name='dummy')\n",
    "        self.conv1 = tf.keras.layers.Conv2D(10,(3, 3))\n",
    "        self.pool1 = tf.keras.layers.MaxPooling2D((2, 2))\n",
    "        self.conv2 = tf.keras.layers.Conv2D(20,(3, 3))\n",
    "        self.pool2 = tf.keras.layers.MaxPooling2D((2, 2))\n",
    "        self.conv3 = tf.keras.layers.Conv2D(40,(3, 3))\n",
    "        self.pool3 = tf.keras.layers.MaxPooling2D((2, 2))\n",
    "        self.conv4 = tf.keras.layers.Conv2D(80,(3, 3))\n",
    "        self.avg_pool = tf.keras.layers.GlobalAveragePooling2D()\n",
    "        self.dense = tf.layers.Dense(emb_size)\n",
    "        \n",
    "        self.arcface = Arcface(out_num)\n",
    "        self.validation = Validation()\n",
    "    \n",
    "    def call(self, input_tensor, training):\n",
    "        images, labels, issame = input_tensor\n",
    "        x = self.conv1(images)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.pool3(x)\n",
    "        x = self.avg_pool(x)\n",
    "        x = self.dense(x)\n",
    "        embeddings = tf.math.l2_normalize(x)\n",
    "        \n",
    "        if traning:\n",
    "            output = self.arcface((embeddings,labels))\n",
    "        else:\n",
    "            output = self.validation((embeddings,issame))\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoreDummy(tf.keras.Model):\n",
    "    def __init__(self, out_num, emb_size = 32):\n",
    "        \"\"\"\n",
    "        -emb_size: size of the embedding\n",
    "        -out_num: number of identities in the \n",
    "        \"\"\"\n",
    "        super(MoreDummy, self).__init__(name='more_dummy')\n",
    "        self.conv1 = tf.keras.layers.Conv2D(10,(3, 3))\n",
    "        self.pool1 = tf.keras.layers.MaxPooling2D((2, 2))\n",
    "        self.conv2 = tf.keras.layers.Conv2D(20,(3, 3))\n",
    "        self.pool2 = tf.keras.layers.MaxPooling2D((2, 2))\n",
    "        self.conv3 = tf.keras.layers.Conv2D(40,(3, 3))\n",
    "        self.pool3 = tf.keras.layers.MaxPooling2D((2, 2))\n",
    "        self.conv4 = tf.keras.layers.Conv2D(80,(3, 3))\n",
    "        self.avg_pool = tf.keras.layers.GlobalAveragePooling2D()\n",
    "        self.dense = tf.layers.Dense(emb_size)\n",
    "\n",
    "        self.arcface = Arcface(out_num)\n",
    "    \n",
    "    def call(self, input_tensor):\n",
    "        images, labels = input_tensor\n",
    "        x = self.conv1(images)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.pool3(x)\n",
    "        x = self.avg_pool(x)\n",
    "        x = self.dense(x)\n",
    "        embeddings = tf.math.l2_normalize(x)\n",
    "        \n",
    "        output = self.arcface([embeddings,labels])\n",
    "        \n",
    "        return output\n",
    "    \n",
    "#     def compute_output_shape(self, input_shape):\n",
    "#         # You need to override this function if you want to use the subclassed model\n",
    "#         # as part of a functional-style model.\n",
    "#         # Otherwise, this method is optional.\n",
    "#         shape = tf.TensorShape(self.out_num).as_list()\n",
    "#         shape[-1] = self.num_classes\n",
    "#         return tf.TensorShape(shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-139-ad7cd2cdf8bd>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-139-ad7cd2cdf8bd>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    def Model()\u001b[0m\n\u001b[1;37m               ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MoreDummy(1300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tests unitaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    " #tf.reset_default_graph()\n",
    "class EvenMoreDummy(tf.keras.Model):\n",
    "    def __init__(self, emb_size = 32):\n",
    "        \"\"\"\n",
    "        -emb_size: size of the embedding\n",
    "        -out_num: number of identities in the \n",
    "        \"\"\"\n",
    "        super(EvenMoreDummy, self).__init__(name='even_more_dummy')\n",
    "        self.conv1 = tf.keras.layers.Conv2D(10,(3, 3))\n",
    "        self.pool1 = tf.keras.layers.MaxPooling2D((2, 2))\n",
    "        self.conv2 = tf.keras.layers.Conv2D(20,(3, 3))\n",
    "        self.pool2 = tf.keras.layers.MaxPooling2D((2, 2))\n",
    "        self.conv3 = tf.keras.layers.Conv2D(40,(3, 3))\n",
    "        self.pool3 = tf.keras.layers.MaxPooling2D((2, 2))\n",
    "        self.conv4 = tf.keras.layers.Conv2D(80,(3, 3))\n",
    "        self.avg_pool = tf.keras.layers.GlobalAveragePooling2D()\n",
    "        self.dense = tf.layers.Dense(emb_size)\n",
    "        \n",
    "        self.arcface = Arcface(1300, name='output')\n",
    "        #self.mylayer = MyLayer(1300, name='output')\n",
    "        self.out = tf.layers.Dense(1)\n",
    "    \n",
    "    def call(self, input_tensor):\n",
    "        images,labels = input_tensor\n",
    "        x = self.conv1(images)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.pool3(x)\n",
    "        x = self.avg_pool(x)\n",
    "        x = self.dense(x)\n",
    "        emb = tf.math.l2_normalize(x)\n",
    "        output = self.arcface([emb,labels])\n",
    "        #output = self.mylayer(emb)\n",
    "        #output = self.out(emb)\n",
    "        return output\n",
    "    \n",
    "#     def compute_output_shape(self, input_shape):\n",
    "#         # You need to override this function if you want to use the subclassed model\n",
    "#         # as part of a functional-style model.\n",
    "#         # Otherwise, this method is optional.\n",
    "#         shape = tf.TensorShape(self.out_num).as_list()\n",
    "#         shape[-1] = self.num_classes\n",
    "#         return tf.TensorShape(shape)\n",
    "\n",
    "model = EvenMoreDummy(32)\n",
    "model.compile(optimizer=tf.train.AdamOptimizer(),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 1300)\n",
      "(?, 1300)\n",
      "(?, 1300)\n",
      "Epoch 1/1\n",
      "1028/4734 [=====>........................] - ETA: 29:01 - loss: 7360578.0000 - acc: 0.0000e+ - ETA: 3:45 - loss: 5612769.6250 - acc: 0.0000e+00 - ETA: 2:22 - loss: 5855589.3077 - acc: 0.0000e+0 - ETA: 1:45 - loss: 5986389.6250 - acc: 0.0000e+0 - ETA: 1:21 - loss: 6091341.8854 - acc: 0.0000e+0 - ETA: 1:07 - loss: 6271097.0917 - acc: 0.0000e+0 - ETA: 55s - loss: 6310649.1419 - acc: 0.0000e+0 - ETA: 48s - loss: 6257284.1875 - acc: 0.0000e+ - ETA: 42s - loss: 6285765.7696 - acc: 0.0000e+ - ETA: 38s - loss: 6242189.2759 - acc: 0.0000e+ - ETA: 34s - loss: 6262091.0462 - acc: 0.0000e+ - ETA: 31s - loss: 6281878.6597 - acc: 0.0000e+ - ETA: 29s - loss: 6232392.4620 - acc: 0.0000e+ - ETA: 27s - loss: 6182257.3663 - acc: 0.0000e+ - ETA: 26s - loss: 6135510.2717 - acc: 0.0000e+ - ETA: 25s - loss: 6151570.3010 - acc: 0.0000e+ - ETA: 24s - loss: 6134638.1262 - acc: 0.0000e+ - ETA: 22s - loss: 6119093.6496 - acc: 0.0000e+ - ETA: 21s - loss: 6119172.5231 - acc: 0.0000e+ - ETA: 21s - loss: 6104005.5760 - acc: 0.0000e+ - ETA: 20s - loss: 6105882.3846 - acc: 0.0000e+ - ETA: 19s - loss: 6097246.3723 - acc: 0.0000e+ - ETA: 19s - loss: 6070198.5140 - acc: 0.0000e+ - ETA: 18s - loss: 6118014.0600 - acc: 0.0000e+ - ETA: 18s - loss: 6121372.8121 - acc: 0.0000e+ - ETA: 17s - loss: 6095467.5061 - acc: 0.0000e+ - ETA: 17s - loss: 6120127.8529 - acc: 0.0000e+ - ETA: 16s - loss: 6127335.7843 - acc: 0.0000e+ - ETA: 16s - loss: 6145162.6146 - acc: 0.0000e+ - ETA: 16s - loss: 6137116.7742 - acc: 0.0000e+ - ETA: 16s - loss: 6128478.6911 - acc: 0.0000e+ - ETA: 15s - loss: 6089757.9873 - acc: 0.0000e+ - ETA: 15s - loss: 6065926.9803 - acc: 0.0000e+ - ETA: 15s - loss: 6071292.8881 - acc: 0.0000e+ - ETA: 14s - loss: 6066238.7037 - acc: 0.0000e+ - ETA: 14s - loss: 6046070.9215 - acc: 0.0000e+ - ETA: 14s - loss: 6043379.5906 - acc: 0.0000e+ - ETA: 13s - loss: 6058297.1218 - acc: 0.0000e+ - ETA: 13s - loss: 6037061.5422 - acc: 0.0000e+ - ETA: 13s - loss: 6035323.9110 - acc: 0.0000e+ - ETA: 13s - loss: 6043334.1722 - acc: 0.0000e+00"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-7a8f697ccbff>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mlabels_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m )\n",
      "\u001b[1;32mc:\\users\\guillaume\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m   1637\u001b[0m           \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1638\u001b[0m           \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1639\u001b[1;33m           validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1640\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1641\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32mc:\\users\\guillaume\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    213\u001b[0m           \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 215\u001b[1;33m         \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    216\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    217\u001b[0m           \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\guillaume\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2984\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2985\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[1;32m-> 2986\u001b[1;33m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[0;32m   2987\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2988\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\guillaume\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1439\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1440\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(\n",
    "    [images_train,labels_train],\n",
    "    labels_train,\n",
    "    epochs=1,\n",
    "    batch_size=4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test it on the training/validation dataset to stop the worst examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate it on the test dataset: one shot learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
